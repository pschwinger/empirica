# Architecture & Structure

**Understanding Empirica's system design and canonical organization**

[â† AI vs Agent](ai_vs_agent.md) | [Back to Home](index.md)

---

## System Architecture Overview

Empirica is built on a **three-layer architecture** designed for modularity, extensibility, and genuine epistemic reasoning:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      USER INTERACTION LAYER                      â”‚
â”‚  - CLI (command-line interface)                                  â”‚
â”‚  - MCP Server (IDE integration)                                  â”‚
â”‚  - Dashboard (real-time monitoring)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EPISTEMIC FRAMEWORK LAYER                     â”‚
â”‚  - Canonical Assessment (13 vectors, genuine LLM reasoning)      â”‚
â”‚  - CASCADE Workflow (7 phases with investigation loop)           â”‚
â”‚  - Profile System (context-aware constraints)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INVESTIGATION SYSTEM LAYER                   â”‚
â”‚  - Domain strategies (code, research, creative, etc.)            â”‚
â”‚  - Plugin system (user-provided investigation tools)             â”‚
â”‚  - Tool recommendations (profile-driven suggestions)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PERSISTENCE LAYER                          â”‚
â”‚  - SQLite database (queryable, relational)                       â”‚
â”‚  - JSON sessions (portable, exportable)                          â”‚
â”‚  - Reflex logs (temporal separation)                             â”‚
â”‚  - Git notes (97.5% token reduction)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Source:** `docs/reference/ARCHITECTURE_OVERVIEW.md`

---

## Core Design Principles

### 1. No Heuristics Principle

**Empirica does not use heuristics to simulate AI self-awareness.**

âŒ **What Empirica Avoids:**
```python
# WRONG - Keyword matching
if 'refactor' in task:
    domain = 'code_analysis'
    know = 0.7  # Fake confidence

# WRONG - Hardcoded confidence boosts
confidence += 0.15 * tools_used  # Not genuine learning

# WRONG - Simulated learning
know_after = know_before + (rounds * 0.05)  # Fake growth
```

âœ… **What Empirica Does:**
```python
# Genuine LLM self-assessment
assessment = await assessor.assess(
    task="Refactor authentication system",
    context={"cwd": "/project", "domain": "security"}
)

# LLM genuinely reasons:
# "I understand authentication patterns (know: 0.7)
#  but I'm uncertain about this specific codebase (uncertainty: 0.6)
#  and I don't know the current implementation (context: 0.4).
#  I need to investigate before proceeding."
```

**Why This Matters:**
- Real reasoning about knowledge state
- Handles novel situations
- Honest uncertainty acknowledgment
- Genuine learning measurement

---

### 2. Temporal Separation

**Reflex logs separate current reasoning from historical reasoning.**

**Problem:** Self-referential recursion
```python
# WRONG - Circular reference
assessment = assess(task, context={
    'previous_assessment': current_assessment  # âŒ Self-referential!
})
```

**Solution:** Temporal separation
```python
# RIGHT - Past reasoning only
assessment = assess(task, context={
    'reflex_logs': load_historical_logs()  # âœ… Historical context
})
```

**Benefits:**
- Prevents confabulation
- AI can reflect on past without loops
- Clear separation of current vs historical
- Enables genuine meta-reasoning

---

### 3. Context-Aware Constraints

**Investigation constraints adapt to AI capability and domain.**

```
High Reasoning Models (Claude Opus, GPT-4, o1):
  â†’ high_reasoning_collaborative profile
  â†’ Unlimited investigation rounds
  â†’ Maximum autonomy

Autonomous Agents (GPT-3.5, Claude Haiku):
  â†’ autonomous_agent profile
  â†’ Max 5 investigation rounds
  â†’ Structured guidance

Critical Domains (Medical, Legal, Financial):
  â†’ critical_domain profile
  â†’ Max 3 investigation rounds
  â†’ Strict compliance rules
```

**Why Adaptive:**
- Right constraints for right context
- No artificial limitations
- Appropriate guidance per capability
- Domain-specific safety

---

### 4. Genuine Calibration

**Track how well AI predictions match reality.**

```
PREFLIGHT: Initial assessment (baseline)
    â†“
Investigation: Gather information
    â†“
CHECK: Reassess after investigation
    â†“
ACT: Execute task
    â†“
POSTFLIGHT: Final assessment (compare with baseline)
    â†“
Calibration delta reveals over/under-confidence patterns
```

**Calibration Metrics:**
- Prediction accuracy
- Overconfidence detection
- Underconfidence detection
- Learning effectiveness
- Continuous improvement

---

## Canonical Directory Structure

Empirica follows a **canonical** (authoritative) directory structure for consistency and clarity:

### Root Structure

```
empirica/
â”œâ”€â”€ empirica/                          # Core Python package
â”œâ”€â”€ mcp_local/                         # MCP server implementations
â”œâ”€â”€ docs/                              # Documentation
â”œâ”€â”€ tests/                             # Test suite
â”œâ”€â”€ examples/                          # Working examples
â”œâ”€â”€ .empirica/                         # Runtime data (auto-created)
â”œâ”€â”€ .empirica_reflex_logs/             # Reflex logs (auto-created)
â””â”€â”€ pyproject.toml                     # Package configuration
```

---

### Core Package: `empirica/`

#### 1. **Canonical Epistemic Framework** (`empirica/core/canonical/`)

**Purpose:** Genuine epistemic self-assessment (no heuristics)

```
empirica/core/canonical/
â”œâ”€â”€ canonical_epistemic_assessment.py  # LLM-powered self-assessment
â”œâ”€â”€ reflex_frame.py                    # Data structures (13 vectors)
â”œâ”€â”€ reflex_logger.py                   # Phase-specific JSON logging
â””â”€â”€ git_enhanced_reflex_logger.py      # Git checkpoints (97.5% reduction)
```

**Key Files:**
- `canonical_epistemic_assessment.py` - Generates LLM prompts, parses responses
- `reflex_frame.py` - Defines `EpistemicAssessment`, `VectorState`, `Action` enum
- `git_enhanced_reflex_logger.py` - Compressed checkpoints in git notes

**Import Paths:**
```python
from empirica.core.canonical import (
    CanonicalEpistemicAssessor,
    EpistemicAssessment,
    VectorState,
    Action,
    CANONICAL_WEIGHTS,
    ENGAGEMENT_THRESHOLD
)
```

---

#### 2. **CASCADE Workflow** (`empirica/core/metacognitive_cascade/`)

**Purpose:** 7-phase metacognitive workflow

```
empirica/core/metacognitive_cascade/
â”œâ”€â”€ metacognitive_cascade.py           # Main CASCADE orchestrator
â”œâ”€â”€ investigation_plugin.py            # Plugin interface
â”œâ”€â”€ investigation_strategy.py          # Domain-aware investigation
â””â”€â”€ mcp_aware_investigation.py         # MCP tool execution
```

**Key Files:**
- `metacognitive_cascade.py` - Orchestrates PREFLIGHT â†’ POSTFLIGHT
- `investigation_plugin.py` - User-provided investigation tools
- `investigation_strategy.py` - Domain-specific tool recommendations

**Import Paths:**
```python
from empirica.core.metacognitive_cascade import CanonicalEpistemicCascade
```

---

#### 3. **Configuration System** (`empirica/config/`)

**Purpose:** Profile-based investigation configuration

```
empirica/config/
â”œâ”€â”€ investigation_profiles.yaml        # 5 profiles (high_reasoning, autonomous, etc.)
â”œâ”€â”€ profile_loader.py                  # Profile loading and selection
â””â”€â”€ modality_config.yaml               # Modality switcher (optional)
```

**5 Built-in Profiles:**
1. `high_reasoning_collaborative` - Max autonomy (Claude, GPT-4, o1)
2. `autonomous_agent` - Structured (GPT-3.5, Haiku)
3. `critical_domain` - Strict compliance (medical, legal)
4. `exploratory` - Max freedom (research, learning)
5. `balanced` - Default middle-ground

**Import Paths:**
```python
from empirica.config.profile_loader import select_profile, load_profile
```

**Note:** v2.0 introduces **MCO Architecture** (Meta-Agent Configuration Object) which replaces Investigation Profiles with dynamic YAML-based configuration. See MCO section below.

---

#### 3a. **MCO Architecture (v2.0)** (`empirica/config/mco/`)

**Purpose:** Dynamic configuration via YAML files

```
empirica/config/mco/
â”œâ”€â”€ personas.yaml                      # 6 AI personas (researcher, implementer, etc.)
â”œâ”€â”€ cascade_styles.yaml                # CASCADE workflow styles
â”œâ”€â”€ goal_scopes.yaml                   # Goal scope recommendations
â”œâ”€â”€ model_profiles.yaml                # Model-specific bias corrections
â”œâ”€â”€ protocols.yaml                     # Communication protocols
â”œâ”€â”€ goal_scope_loader.py               # Scope recommendation logic
â””â”€â”€ mco_loader.py                      # MCO loading logic
```

**6 MCO Personas** (in `personas.yaml`):
1. `researcher` - High uncertainty tolerance, deep investigation
2. `implementer` - Balanced, action-oriented
3. `reviewer` - High precision, quality focus
4. `coordinator` - Multi-agent orchestration
5. `learner` - Maximum exploration
6. `expert` - Minimal investigation, high confidence

**Key Features:**
- **Dynamic Thresholds**: Persona-specific confidence gates
- **Scope Recommendations**: AI-driven goal scoping based on epistemic state
- **Model Profiles**: Bias correction for different AI models (GPT-4, Claude, etc.)
- **CASCADE Styles**: Different workflow patterns per persona
- **Backward Compatible**: Falls back to Investigation Profiles if MCO not available

**Import Paths:**
```python
from empirica.config.mco.mco_loader import load_mco_persona
from empirica.config.goal_scope_loader import get_scope_recommendations
```

---

#### 4. **Data Management** (`empirica/data/`)

**Purpose:** Persistence, export/import, tracking

```
empirica/data/
â”œâ”€â”€ session_database.py                # SQLite database
â””â”€â”€ session_json_handler.py            # JSON session export/import
```

**Storage Locations:**
- SQLite: `.empirica/sessions/sessions.db`
- JSON: `.empirica/sessions/<session_id>.json`
- Reflex logs: `.empirica_reflex_logs/<ai_id>/<date>/`
- Git notes: `git notes refs/empirica/checkpoints/<session_id>`

**Import Paths:**
```python
from empirica.data.session_database import SessionDatabase
from empirica.data.session_json_handler import SessionJSONHandler
```

---

#### 5. **CLI** (`empirica/cli/`)

**Purpose:** Command-line interface

```
empirica/cli/
â”œâ”€â”€ cli_core.py                        # Main CLI logic
â”œâ”€â”€ command_handlers/                  # Command implementations
â”‚   â”œâ”€â”€ bootstrap_commands.py
â”‚   â”œâ”€â”€ assessment_commands.py
â”‚   â”œâ”€â”€ cascade_commands.py
â”‚   â”œâ”€â”€ session_commands.py
â”‚   â””â”€â”€ ... (15+ handlers)
â””â”€â”€ uvl_formatter.py                   # UVL formatting
```

**Entry Point:**
```bash
python -m empirica.cli
# or
empirica <command>
```

---

#### 6. **Components** (`empirica/components/`)

**Purpose:** Optional advanced components (11 enterprise components)

```
empirica/components/
â”œâ”€â”€ code_intelligence_analyzer/        # Code analysis
â”œâ”€â”€ context_validation/                # Context verification
â”œâ”€â”€ goal_management/                   # Goal orchestration
â”œâ”€â”€ security_monitoring/               # Security scanning
â”œâ”€â”€ tool_management/                   # Enhanced tool handling
â””â”€â”€ ... (6 more components)
```

**Note:** Most components are optional. Core functionality doesn't depend on them.

---

### MCP Server: `mcp_local/`

**Purpose:** IDE integration via Model Context Protocol

```
mcp_local/
â”œâ”€â”€ empirica_mcp_server.py             # Main MCP server (23 tools)
â”œâ”€â”€ start_empirica_mcp.sh              # Startup script
â””â”€â”€ archive/                           # Archived documentation
```

**23 MCP Tools:**
- Session Management (4 tools)
- Assessment Workflow (6 tools)
- Goals & Subtasks (5 tools)
- Continuity (5 tools)
- Help (3 tools)

---

### Documentation: `docs/`

**Purpose:** Comprehensive documentation

```
docs/
â”œâ”€â”€ 00_START_HERE.md                   # Entry point for humans
â”œâ”€â”€ 01_a_AI_AGENT_START.md             # Entry point for AI agents (CLI)
â”œâ”€â”€ 01_b_MCP_AI_START.md               # Entry point for AI agents (MCP)
â”œâ”€â”€ production/                        # 25 production docs
â”‚   â”œâ”€â”€ 00_COMPLETE_SUMMARY.md
â”‚   â”œâ”€â”€ 05_EPISTEMIC_VECTORS.md
â”‚   â”œâ”€â”€ 06_CASCADE_FLOW.md
â”‚   â””â”€â”€ ... (22 more docs)
â”œâ”€â”€ reference/                         # Reference documentation
â”‚   â”œâ”€â”€ CANONICAL_DIRECTORY_STRUCTURE.md
â”‚   â”œâ”€â”€ ARCHITECTURE_OVERVIEW.md
â”‚   â””â”€â”€ ...
â””â”€â”€ guides/                            # User guides
```

---

### Runtime Data: `.empirica/` (auto-created)

**Purpose:** Runtime data storage

```
.empirica/
â”œâ”€â”€ sessions/                          # Session storage
â”‚   â”œâ”€â”€ sessions.db                    # SQLite database
â”‚   â””â”€â”€ <session_id>.json              # JSON session exports
â””â”€â”€ config/                            # Runtime configuration
    â””â”€â”€ user_preferences.json          # User preferences
```

**Auto-Initialization:** Created automatically on first use.

---

### Reflex Logs: `.empirica_reflex_logs/` (auto-created)

**Purpose:** Temporal separation logs

```
.empirica_reflex_logs/
â””â”€â”€ <ai_id>/                           # Per-AI logs
    â””â”€â”€ <date>/                        # Per-date logs
        â”œâ”€â”€ preflight_<timestamp>.json
        â”œâ”€â”€ investigate_<timestamp>.json
        â”œâ”€â”€ check_<timestamp>.json
        â””â”€â”€ postflight_<timestamp>.json
```

**Purpose:** Prevents self-referential recursion.

---

## System Prompts: The Foundation

### Why System Prompts Matter

**System prompts** define AI behavior, role, and capabilities:

**AI System Prompt (High Reasoning):**
```
You are a collaborative AI partner working WITH the user.
You have high autonomy and reasoning capability.

Use full CASCADE workflow:
- PREFLIGHT: Assess your knowledge state honestly
- THINK: Analyze task requirements
- PLAN: Formulate strategy
- INVESTIGATE: Research when uncertain
- CHECK: Validate readiness
- ACT: Execute with confidence
- POSTFLIGHT: Measure learning

Ask clarifying questions when uncertain.
Plan architecture and make design decisions.
Create goals and delegate to agents when appropriate.
Track your epistemic growth and learning.
```

**Agent System Prompt (Action-Based):**
```
You are an execution agent focused on completing specific tasks.
You receive well-defined subtasks from lead AIs.

Use simplified CASCADE:
- ACT: Execute subtask efficiently
- COMPLETE: Report evidence clearly

Use full CASCADE for complex/uncertain tasks.
Ask for clarification if task is unclear.
Optimize for speed and efficiency.
```

### Future: Dynamic System Prompts

**Vision:** Cognitive Vault + Sentinel provides role-based prompts

```python
# AI requests prompt
prompt = get_system_prompt(
    ai_id="claude-dev",
    role="collaborative_ai",
    modality="coding",
    task_type="feature_design"
)
â†’ Returns: AI_COLLABORATIVE_PROMPT

# Agent requests prompt
prompt = get_system_prompt(
    ai_id="mini-agent",
    role="acting_agent",
    modality="testing",
    task_type="test_implementation"
)
â†’ Returns: AGENT_EXECUTION_PROMPT
```

**Benefits:**
- Right prompt for right role
- Consistent terminology
- Token-efficient
- Centrally managed
- Version controlled

---

## Integration Points

### 1. CLI Integration

**Entry Point:** `python -m empirica.cli`

**Key Commands:**
```bash
# Bootstrap with profile
empirica bootstrap --profile high_reasoning_collaborative

# Auto-select profile
empirica bootstrap --ai-model claude-sonnet --domain research

# Workflow commands
empirica preflight "task description"
empirica investigate
empirica check
empirica postflight

# Session management
empirica sessions-list
empirica sessions-resume --ai-id=your-id

# Goals & subtasks
empirica goals-create --objective="Your goal"
empirica goals-add-subtask --goal-id=<id> --description="..."
empirica goals-complete-subtask --task-id=<id>
```

---

### 2. MCP Server Integration

**23 MCP Tools Available:**

**Session Management:**
- `bootstrap_session(ai_id, session_type, profile)`
- `resume_previous_session(ai_id, count)`
- `get_session_summary(session_id)`
- `get_epistemic_state(session_id)`

**Assessment Workflow:**
- `execute_preflight(session_id, prompt)`
- `submit_preflight_assessment(session_id, vectors)`
- `execute_check(session_id, findings, unknowns, confidence)`
- `submit_check_assessment(session_id, vectors, decision)`
- `execute_postflight(session_id, task_summary)`
- `submit_postflight_assessment(session_id, vectors)`

**Goals & Subtasks:**
- `create_goal(session_id, objective, scope, success_criteria)`
- `add_subtask(goal_id, description, importance)`
- `complete_subtask(task_id, evidence)`
- `get_goal_progress(goal_id)`
- `list_goals(session_id)`

**Continuity:**
- `create_git_checkpoint(session_id, phase, round_num)`
- `load_git_checkpoint(session_id)`
- `create_handoff_report(session_id, task_summary, key_findings, ...)`
- `query_handoff_reports(ai_id, limit)`

**Help:**
- `get_empirica_introduction()`
- `get_workflow_guidance(phase)`
- `cli_help()`

---

### 3. Plugin Integration

**Investigation Plugins:**

```python
# User creates plugin
my_plugin = InvestigationPlugin(
    name='database_search',
    description='Search internal database',
    improves_vectors=['know', 'context'],
    confidence_gain=0.20,
    executor=my_db_search_function
)

# Register with CASCADE
cascade = CanonicalEpistemicCascade(
    investigation_plugins={'db': my_plugin}
)

# Plugin suggestions filtered by profile
# - light mode: AI can ignore
# - guided mode: AI should consider
# - prescribed mode: Must use if relevant
```

---

## Data Flow: Complete CASCADE

```
1. User Prompt
    â†“
2. [PREFLIGHT] Canonical Epistemic Assessment
    â”‚ - LLM genuinely reasons about 13 vectors
    â”‚ - Baseline established
    â”‚ - Auto-tracked: SQLite + JSON + Reflex log
    â†“
3. [THINK] Initial Reasoning
    â”‚ - LLM thinks about approach
    â”‚ - Domain classification
    â†“
4. [PLAN] Task Decomposition
    â”‚ - Break down into subtasks
    â”‚ - Goal orchestrator manages goals
    â†“
5. [INVESTIGATE] Investigation Loop
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ â”‚ A. Identify epistemic gaps  â”‚
    â”‚ â”‚ B. Load investigation profileâ”‚
    â”‚ â”‚ C. Strategy recommends toolsâ”‚
    â”‚ â”‚ D. Plugins add custom tools â”‚
    â”‚ â”‚ E. AI selects tools         â”‚
    â”‚ â”‚ F. MCP executes tools       â”‚
    â”‚ â”‚ G. Results integrated       â”‚
    â”‚ â”‚ [CHECK] Self-assess         â”‚
    â”‚ â”‚ H. Continue OR exit?        â”‚
    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
6. [ACT] Execute Task
    â”‚ - Perform actual work
    â”‚ - Generate output
    â†“
7. [POSTFLIGHT] Final Assessment
    â”‚ - Genuine reassessment
    â”‚ - Compare with PREFLIGHT
    â”‚ - Epistemic delta calculated
    â”‚ - Auto-tracked: SQLite + JSON + Reflex log
    â†“
8. Results + Session Data
```

---

## Performance Characteristics

### Time Complexity
- Epistemic assessment: O(1) LLM call
- CASCADE execution: O(n) where n = investigation rounds
- Profile loading: O(1) YAML parse (cached)
- Database queries: O(log n) with indexes

### Space Complexity
- Session storage: ~10KB per session (JSON)
- Database growth: ~50KB per 100 assessments
- Reflex logs: ~5KB per phase per session
- Memory usage: <100MB typical

### Token Efficiency
- **Baseline session loading:** 1,821 tokens
- **Git-enhanced loading:** 46 tokens
- **Reduction:** 97.5%
- **Handoff reports:** 238-400 tokens vs 20,000
- **Reduction:** 98%

---

## Best Practices

### For Developers

1. **Follow canonical structure** - Don't deviate from directory layout
2. **Use import paths correctly** - Full paths required
3. **Extend via plugins** - Don't modify core
4. **Test against profiles** - Verify behavior across profiles
5. **Document integration points** - Clear extension mechanisms

### For Users

1. **Choose right profile** - Match AI capability and domain
2. **Use MCP tools** - Leverage IDE integration
3. **Track sessions** - Use session management
4. **Generate handoffs** - Enable continuity
5. **Review calibration** - Improve over time

### For AIs

1. **Understand architecture** - Know where files are
2. **Use correct imports** - Follow canonical paths
3. **Respect profiles** - Honor constraints
4. **Track learning** - Measure epistemic growth
5. **Generate handoffs** - Enable collaboration

---

## Next Steps

**Learn More:**
- [Epistemics](epistemics.md) - 13-vector system deep dive
- [Collaboration](collaboration.md) - Sessions, goals, handoffs
- [AI vs Agent](ai_vs_agent.md) - High reasoning vs action-based
- [Production Docs](../docs/reference/ARCHITECTURE_OVERVIEW.md) - Complete architecture reference
- [Directory Structure](../docs/reference/CANONICAL_DIRECTORY_STRUCTURE.md) - Complete file reference

**Try It:**
```python
from empirica.core.metacognitive_cascade.metacognitive_cascade import CanonicalEpistemicCascade
from empirica.config.profile_loader import select_profile

# Auto-select profile
profile = select_profile(ai_model='claude-sonnet', domain='research')

# Create CASCADE with profile
cascade = CanonicalEpistemicCascade(profile_name=profile.name)

# Run CASCADE
result = await cascade.run_epistemic_cascade(
    task="Your task here",
    context={"domain": "your_domain"}
)
```

---

**Built with architectural clarity. Understand the system, extend the system.** ğŸ—ï¸
