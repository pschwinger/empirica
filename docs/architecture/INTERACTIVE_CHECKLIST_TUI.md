# Interactive Epistemic Checklist - TUI Implementation

## Vision: Provider-Agnostic Workflow Enforcement

**Problem:** AIs forget to log epistemic breadcrumbs regardless of provider (Claude, GPT, Qwen, etc.)

**Solution:** TUI dashboard that actively prompts and validates, not passive observation.

---

## Core Concept: "Epistemic Completeness Score"

Dashboard shows real-time completeness of current session across all breadcrumb types:

```
â”Œâ”€ EPISTEMIC COMPLETENESS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Session: abc123 | Duration: 00:45:32                  â”‚
â”‚ Overall Score: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 75% (GOOD)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… PREFLIGHT     Complete (0:00:45 ago)               â”‚
â”‚ âš ï¸  Findings      2 logged (last: 15m ago) [+]        â”‚
â”‚ âš ï¸  Unknowns      1 logged (last: 20m ago) [+]        â”‚
â”‚ âŒ Mistakes       0 logged                    [+]     â”‚
â”‚ âŒ Dead Ends      0 logged                    [+]     â”‚
â”‚ âš ï¸  Sources       1 logged (GitHub URL)       [+]     â”‚
â”‚ âŒ POSTFLIGHT    Not started                  [!]     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ðŸ’¡ SUGGESTIONS:                                        â”‚
â”‚ â€¢ 15+ min since last finding - log discoveries?       â”‚
â”‚ â€¢ No mistakes logged - unusual for 45m session        â”‚
â”‚ â€¢ POSTFLIGHT required before ending session           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Three-Phase Checklist Integration

### Phase 1: Pre-Work Validation (Session Start)

**When:** AI opens project or starts work
**Goal:** Ensure proper session initialization

```
â”Œâ”€ SESSION INITIALIZATION CHECKLIST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                        â”‚
â”‚ Required Steps:                                        â”‚
â”‚ [âœ“] 1. Active session exists                          â”‚
â”‚ [âœ“] 2. Linked to project: empirica                    â”‚
â”‚ [âœ—] 3. PREFLIGHT assessment submitted                 â”‚
â”‚ [ ] 4. Project context loaded (bootstrap)             â”‚
â”‚                                                        â”‚
â”‚ âš ï¸  Step 3 incomplete!                                 â”‚
â”‚                                                        â”‚
â”‚ Options:                                               â”‚
â”‚ [1] Run PREFLIGHT now (guided)                        â”‚
â”‚ [2] Skip (not recommended)                            â”‚
â”‚ [3] Load from previous session                        â”‚
â”‚                                                        â”‚
â”‚ Press [1-3] or [Esc] to dismiss                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**How it works:**
1. Dashboard detects new session or session without PREFLIGHT
2. Shows blocking modal (can't dismiss without action)
3. Guides AI through PREFLIGHT creation
4. Only allows work to proceed when checklist complete

**Implementation:**
```python
def validate_session_start(session_id):
    """Check session initialization completeness"""
    db = SessionDatabase()

    checks = {
        "active_session": db.has_active_session(),
        "linked_to_project": db.get_session_project(session_id) is not None,
        "preflight_done": db.has_preflight(session_id),
        "bootstrap_loaded": check_bootstrap_timestamp()
    }

    incomplete = [k for k, v in checks.items() if not v]

    if incomplete:
        return {
            "complete": False,
            "missing": incomplete,
            "suggestions": generate_fix_suggestions(incomplete)
        }

    return {"complete": True, "score": 1.0}
```

---

### Phase 2: During-Work Monitoring (Active Session)

**When:** Continuously while AI works
**Goal:** Prompt for breadcrumbs based on activity patterns

#### Activity-Based Prompts

**Pattern 1: Files Modified â†’ Suggest Findings**
```python
# Dashboard detects git diff
if files_modified_count >= 3 and time_since_last_finding > 10_minutes:
    show_prompt(
        type="finding",
        message="3 files modified in last 10min. Log discoveries?",
        suggestions=[
            "Modified authentication flow to use JWT tokens",
            "Refactored error handling in API endpoints",
            "Custom input..."
        ]
    )
```

**TUI Display:**
```
â”Œâ”€ ðŸ’¡ FINDING SUGGESTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Activity detected: 3 files modified (last 10 min)      â”‚
â”‚                                                         â”‚
â”‚ Quick log a finding?                                   â”‚
â”‚ [1] Modified auth flow to use JWT tokens              â”‚
â”‚ [2] Refactored error handling in API                  â”‚
â”‚ [3] Custom message...                                  â”‚
â”‚ [Esc] Remind me later                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pattern 2: Error Messages â†’ Suggest Mistakes**
```python
# Dashboard monitors command outputs for errors
if stderr_contains_error() or exit_code != 0:
    show_prompt(
        type="mistake",
        message="Command failed. Log as mistake for learning?",
        context={
            "command": last_command,
            "error": stderr_snippet,
            "cost": estimate_time_lost()
        }
    )
```

**TUI Display:**
```
â”Œâ”€ âš ï¸  MISTAKE DETECTED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Command failed: npm install                            â”‚
â”‚ Error: ENOENT package.json not found                   â”‚
â”‚ Time lost: ~5 minutes                                  â”‚
â”‚                                                         â”‚
â”‚ Log this mistake for future learning?                  â”‚
â”‚                                                         â”‚
â”‚ Root Cause:                                            â”‚
â”‚ [1] Ran command in wrong directory                    â”‚
â”‚ [2] Package.json was deleted accidentally             â”‚
â”‚ [3] Other (specify)...                                â”‚
â”‚                                                         â”‚
â”‚ Prevention:                                            â”‚
â”‚ [Auto] Always verify pwd before npm commands          â”‚
â”‚                                                         â”‚
â”‚ [L] Log Mistake  [S] Skip  [Esc] Dismiss              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pattern 3: Uncertainty Keywords â†’ Suggest Unknowns**
```python
# Dashboard watches for uncertainty signals in AI output
uncertainty_keywords = ["unclear", "uncertain", "don't know", "not sure", "might be", "possibly"]

if ai_output_contains(uncertainty_keywords):
    show_prompt(
        type="unknown",
        message="Detected uncertainty. Log as unknown?",
        extract=extracted_uncertainty_phrase
    )
```

**TUI Display:**
```
â”Œâ”€ â“ UNCERTAINTY DETECTED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Output: "Token refresh timing is unclear"          â”‚
â”‚                                                        â”‚
â”‚ Log as unknown for investigation?                     â”‚
â”‚ Unknown: "Token refresh timing unclear"                â”‚
â”‚                                                        â”‚
â”‚ [L] Log Now  [I] Investigate First  [Esc] Skip       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pattern 4: Investigated but Didn't Work â†’ Suggest Dead End**
```python
# Dashboard detects rollback patterns (git checkout, ctrl-z, file deletion)
if rollback_detected() or large_deletion():
    show_prompt(
        type="deadend",
        message="Approach rolled back. Log as dead end?",
        context={"approach": what_was_tried, "why_failed": infer_reason}
    )
```

**Pattern 5: External References â†’ Suggest Sources**
```python
# Dashboard watches for URLs or file paths in AI output
if url_detected(ai_output) or file_path_mentioned(ai_output):
    show_prompt(
        type="source",
        message="Reference detected. Log as epistemic source?",
        source_url=extracted_url,
        source_type="url" if is_url else "local"
    )
```

**TUI Display:**
```
â”Œâ”€ ðŸ“š SOURCE DETECTED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reference: https://oauth.net/2/token-refresh/          â”‚
â”‚                                                         â”‚
â”‚ Log as epistemic source?                               â”‚
â”‚                                                         â”‚
â”‚ Type: [1] Documentation  [2] Tutorial  [3] Example    â”‚
â”‚                                                         â”‚
â”‚ Relevance: [H]igh  [M]edium  [L]ow                    â”‚
â”‚                                                         â”‚
â”‚ [L] Log Source  [Esc] Skip                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Phase 3: Post-Work Validation (Session End)

**When:** AI signals work complete or session duration > threshold
**Goal:** Ensure CASCADE completeness and knowledge capture

```
â”Œâ”€ SESSION COMPLETION CHECKLIST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Session: abc123 | Duration: 02:15:32                  â”‚
â”‚                                                        â”‚
â”‚ Before ending session:                                 â”‚
â”‚ [âœ“] 1. PREFLIGHT completed                            â”‚
â”‚ [âœ“] 2. Work performed (15 commands)                   â”‚
â”‚ [âœ“] 3. Findings logged (5)                            â”‚
â”‚ [~] 4. Unknowns logged (2) - any resolved?            â”‚
â”‚ [!] 5. Mistakes logged (0) - unusual!                 â”‚
â”‚ [âœ—] 6. POSTFLIGHT assessment                          â”‚
â”‚                                                        â”‚
â”‚ âš ï¸  Completeness: 70% (MEDIUM)                         â”‚
â”‚                                                        â”‚
â”‚ Missing:                                               â”‚
â”‚ â€¢ POSTFLIGHT assessment (required)                    â”‚
â”‚ â€¢ No mistakes logged (2hr session - likely missed)    â”‚
â”‚ â€¢ 2 unknowns unresolved (mark resolved or carry over) â”‚
â”‚                                                        â”‚
â”‚ Actions:                                               â”‚
â”‚ [1] Complete POSTFLIGHT now (guided, 2 min)           â”‚
â”‚ [2] Review unknowns before ending                     â”‚
â”‚ [3] Force end (creates incomplete session marker)     â”‚
â”‚                                                        â”‚
â”‚ Press [1-3] to continue                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Guided POSTFLIGHT:**
If user selects [1], dashboard guides through:
```
â”Œâ”€ GUIDED POSTFLIGHT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1/3: Reassess Knowledge Vectors                   â”‚
â”‚                                                         â”‚
â”‚ How did your knowledge change from PREFLIGHT?          â”‚
â”‚                                                         â”‚
â”‚ KNOW (was: 0.65)    [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 0.80 â¬† +0.15         â”‚
â”‚ CONTEXT (was: 0.55) [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 0.75 â¬† +0.20         â”‚
â”‚ UNCERTAINTY (0.70)  [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘] 0.35 â¬‡ -0.35         â”‚
â”‚                                                         â”‚
â”‚ Auto-detected from activity:                           â”‚
â”‚ â€¢ KNOW +0.15 (5 findings logged)                       â”‚
â”‚ â€¢ CONTEXT +0.20 (bootstrap loaded, 15 files changed)   â”‚
â”‚ â€¢ UNCERTAINTY -0.35 (2 unknowns â†’ resolved)            â”‚
â”‚                                                         â”‚
â”‚ Accept auto-values? [Y]es [E]dit [C]ancel             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Completeness Scoring Algorithm

```python
def calculate_completeness_score(session_id):
    """
    Calculate 0-1 score for session epistemic completeness.

    Scoring:
    - PREFLIGHT exists: +20%
    - Findings (1+ per 15min): +20%
    - Unknowns tracked: +15%
    - Mistakes logged: +10%
    - Sources cited: +10%
    - Dead ends documented: +5%
    - POSTFLIGHT exists: +20%
    """
    db = SessionDatabase()
    session = db.get_session(session_id)
    duration_min = session.duration.total_seconds() / 60

    score = 0.0

    # PREFLIGHT (20%)
    if db.has_preflight(session_id):
        score += 0.20

    # Findings (20%)
    findings_count = db.count_findings(session_id)
    expected_findings = max(1, duration_min // 15)  # 1 per 15 min
    findings_score = min(findings_count / expected_findings, 1.0) * 0.20
    score += findings_score

    # Unknowns (15%)
    unknowns_count = db.count_unknowns(session_id)
    if unknowns_count > 0:
        score += 0.15  # Any unknowns tracked = good

    # Mistakes (10%)
    mistakes_count = db.count_mistakes(session_id)
    if mistakes_count > 0 or duration_min < 30:
        score += 0.10  # Either logged mistakes or short session (no mistakes expected)

    # Sources (10%)
    sources_count = db.count_epistemic_sources(session_id)
    if sources_count > 0:
        score += 0.10

    # Dead ends (5%)
    deadends_count = db.count_deadends(session_id)
    if deadends_count > 0:
        score += 0.05

    # POSTFLIGHT (20%)
    if db.has_postflight(session_id):
        score += 0.20

    return {
        "score": score,
        "grade": "EXCELLENT" if score >= 0.9 else "GOOD" if score >= 0.7 else "MEDIUM" if score >= 0.5 else "LOW",
        "breakdown": {
            "preflight": 0.20 if db.has_preflight(session_id) else 0,
            "findings": findings_score,
            "unknowns": 0.15 if unknowns_count > 0 else 0,
            "mistakes": 0.10 if mistakes_count > 0 else 0,
            "sources": 0.10 if sources_count > 0 else 0,
            "deadends": 0.05 if deadends_count > 0 else 0,
            "postflight": 0.20 if db.has_postflight(session_id) else 0
        }
    }
```

---

## Provider-Agnostic Activity Detection

**How does dashboard detect activity without Claude Code hooks?**

### Method 1: Database Polling (Universal)
```python
class ActivityMonitor:
    def __init__(self, session_id):
        self.last_check = datetime.now()
        self.session_id = session_id

    def poll(self):
        """Check for activity every 1s"""
        db = SessionDatabase()

        # Detect new entries since last check
        new_findings = db.count_findings_since(self.session_id, self.last_check)
        new_commands = db.count_commands_since(self.session_id, self.last_check)

        # Detect staleness (no activity for 10+ min)
        if (datetime.now() - self.last_check).seconds > 600:
            return {"alert": "stale_session", "message": "No activity for 10 min"}

        self.last_check = datetime.now()
```

### Method 2: Git Watching (Universal)
```python
import watchdog

class GitWatcher:
    def on_modified(self, event):
        """Triggered when files change"""
        if self.is_significant_change(event):
            suggest_finding(context={
                "files_changed": event.src_path,
                "change_type": "modified"
            })
```

### Method 3: Command Logging (Built-in)
Empirica already logs all commands to `command_usage` table:
```python
# Every empirica command writes to DB
db.log_command_usage(
    command_name="finding-log",
    execution_time_ms=45,
    success=True
)

# Dashboard queries this:
recent_commands = db.get_commands_since(last_check)
for cmd in recent_commands:
    if cmd.name == "finding-log":
        update_completeness_score(+5%)  # Positive reinforcement
```

---

## Smart Suggestions Based on Context

### Context: Project Type Detection
```python
def detect_project_type():
    """Infer project type from files"""
    if exists("package.json"):
        return "nodejs"
    elif exists("requirements.txt"):
        return "python"
    elif exists("go.mod"):
        return "golang"
    # ... etc
```

**Tailored Prompts:**
- **Python project + pytest found** â†’ "Run tests and log results as finding?"
- **Node project + package-lock changed** â†’ "Dependency updated. Log reason as finding?"
- **Any project + .env modified** â†’ "Config changed. Document as source?"

---

## Integration with Semantic Index (Qdrant)

**Store this checklist logic as searchable context:**

```python
# At session start
context_query = f"epistemic workflow automation for {project_type} project"
results = qdrant_search(context_query, top_k=3)

# Results include:
# - This checklist doc
# - Project-specific workflow patterns
# - Common mistakes for this project type

# Dashboard shows personalized suggestions
for result in results:
    show_tip(result.content)
```

**Example tip from semantic index:**
```
ðŸ’¡ TIP: This Python project has high test coverage (95%).
   Suggested workflow:
   1. Modify code
   2. Run pytest
   3. Log test results as finding
   4. If test fails â†’ log as mistake with root cause
```

---

## Implementation Phases

### Phase 1: Basic Completeness Tracking (Week 1)
- [ ] Add completeness score calculation to dashboard
- [ ] Show simple checklist (PREFLIGHT/POSTFLIGHT status)
- [ ] Display counts: findings, unknowns, mistakes

### Phase 2: Activity-Based Prompts (Week 2-3)
- [ ] Git file watcher integration
- [ ] Command output monitoring (errors â†’ mistakes)
- [ ] Uncertainty detection (text analysis)
- [ ] Smart suggestions based on activity

### Phase 3: Guided Workflows (Week 4)
- [ ] Interactive PREFLIGHT wizard
- [ ] Interactive POSTFLIGHT wizard
- [ ] Dead end detection (rollback patterns)
- [ ] Source citation prompts

### Phase 4: Semantic Integration (Future)
- [ ] Index this doc in Qdrant
- [ ] Project-type-specific suggestions
- [ ] Learning from past sessions
- [ ] Automated pattern detection

---

## Success Metrics

Track improvement in epistemic completeness:

```sql
-- Before checklist TUI
SELECT AVG(completeness_score) FROM sessions WHERE created < '2025-01-01';
-- Result: 0.45 (45% average)

-- After checklist TUI
SELECT AVG(completeness_score) FROM sessions WHERE created >= '2025-01-01';
-- Target: 0.85 (85% average)
```

**Goals:**
- 90%+ sessions have PREFLIGHT + POSTFLIGHT
- 80%+ sessions have 1+ finding per 15 min
- 50%+ sessions log at least 1 mistake
- 70%+ sessions cite external sources
- 30%+ sessions document dead ends

---

## Provider-Agnostic Design Principles

1. **No Claude-specific hooks** - Works with any AI via TUI
2. **Database-driven** - All detection via DB polling
3. **Git-native** - Uses git for file change detection
4. **Terminal-based** - TUI works over SSH, any environment
5. **MCP-compatible** - Can integrate via MCP server if available
6. **Standalone** - Dashboard runs independently, no IDE required

This makes Empirica truly provider-agnostic while still ensuring workflow compliance.
