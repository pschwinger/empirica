# Bootstrap & Goal Orchestrator Audit

**Date:** 2024-11-14  
**Issues Found:** 3  
**Status:** Requires fixes

---

## Issue 1: MCP Tool Name Mismatch ‚ö†Ô∏è

**Problem:** Tool is named `bootstrap_session` but error suggests client is calling `empirica-bootstrap_session`

**Current MCP Tool Name:**
```python
# mcp_local/empirica_mcp_server.py line 335
name="bootstrap_session"
```

**Error Reported:**
```
‚úó empirica-bootstrap_session
   Tool 'empirica-bootstrap_session' does not exist.
```

**Root Cause:** Client may be adding `empirica-` prefix automatically, or documentation uses wrong name.

**Fix Options:**
1. Change tool name to `empirica-bootstrap_session` (match what client expects)
2. Check client configuration (may be auto-prefixing)
3. Update documentation to use correct name

**Recommendation:** Check what other tools are named. If they're all plain names (no prefix), this is a client configuration issue.

---

## Issue 2: Goal Orchestrator Uses Placeholder (Heuristic Fallback) ‚ö†Ô∏è

**Problem:** Goal orchestrator is initialized with `use_placeholder=True`, which means it's NOT using LLM reasoning.

**Current Code:**
```python
# empirica/bootstraps/optimal_metacognitive_bootstrap.py line 181
self.components['canonical_goal_orchestrator'] = create_goal_orchestrator(use_placeholder=True)
```

**What This Means:**
- `use_placeholder=True` ‚Üí Uses `_placeholder_goal_generation()` method
- This method uses **simple logic based on epistemic gaps**, NOT genuine LLM reasoning
- While it's better than hardcoded templates (responds to actual assessment), it's still a heuristic

**Placeholder Logic (from canonical_goal_orchestrator.py):**
```python
def _placeholder_goal_generation(...):
    # Low CLARITY ‚Üí need clarification
    if epistemic_assessment.clarity.score < 0.60:
        goals.append(Goal(...))
    
    # Low KNOW ‚Üí need investigation  
    if epistemic_assessment.know.score < GOAL_CONFIDENCE_THRESHOLD:
        goals.append(Goal(...))
    
    # Overall confidence met ‚Üí proceed to action
    if epistemic_assessment.overall_confidence >= GOAL_CONFIDENCE_THRESHOLD:
        goals.append(Goal(...))
```

**This IS a heuristic!** It uses threshold comparisons to generate goals, not AI reasoning.

**User's Requirement:**
> "Goals need to be grabbed from context and evaluated by the AI to be used in the workflow"

**What Should Happen:**
1. User provides context (conversation, task description)
2. AI reads context and epistemic assessment
3. AI **reasons** about what goals make sense
4. AI generates goals with genuine reasoning (not template matching)

**Current Issue:**
- Bootstrap sets `use_placeholder=True` ‚Üí bypasses LLM reasoning
- Placeholder uses threshold checks (if clarity < 0.60, if know < 0.70)
- This is rule-based logic, not genuine AI reasoning

---

## Issue 3: Bootstrap Comments Reference "Heuristics" ‚ö†Ô∏è

**Problem:** Bootstrap has comments/warnings about heuristics, suggesting legacy code may still exist.

**Evidence:**
```python
# Line 173: "Load canonical goal orchestrator (LLM-POWERED, NO HEURISTICS)"
print("\n3Ô∏è‚É£ Loading canonical goal orchestrator...")

# Line 186: "Canonical goal orchestrator loaded (LLM-powered, no heuristics)"

# Line 190: "Falling back to legacy goal orchestrator (heuristic-based)"

# Line 202: "Legacy goal orchestrator loaded (context-aware, heuristic-based)"

# Line 359: "Not the simple TwelveVectorSelfAwarenessMonitor with hardcoded defaults"
```

**Analysis:**
- Comments claim "NO HEURISTICS" but code uses `use_placeholder=True`
- Placeholder IS a heuristic (threshold-based logic)
- Fallback to "legacy goal orchestrator" exists (even more heuristic)
- Comments are misleading or aspirational, not accurate

---

## Deep Analysis: Goal Orchestrator Architecture

### Current Architecture

```
User Request
    ‚Üì
CanonicalGoalOrchestrator(use_placeholder=True)
    ‚Üì
orchestrate_goals() ‚Üí _placeholder_goal_generation()
    ‚Üì
Threshold-based logic:
  - if clarity < 0.60 ‚Üí CLARIFY goal
  - if know < 0.70 ‚Üí INVESTIGATE goal
  - if confidence >= 0.70 ‚Üí ACT goal
    ‚Üì
Goals with reasoning (but reasoning is generated by template logic)
```

**This is NOT genuine AI reasoning.** It's sophisticated heuristics.

### Intended Architecture (Per Code Comments)

```
User Request
    ‚Üì
CanonicalGoalOrchestrator(llm_client=..., use_placeholder=False)
    ‚Üì
orchestrate_goals() ‚Üí Build meta-prompt
    ‚Üì
LLM Client: Generate goals based on:
  - Conversation context
  - Epistemic assessment (all 12 vectors)
  - Current state
  - NO templates, NO thresholds
    ‚Üì
Parse LLM response ‚Üí Goals with genuine reasoning
```

**This WOULD be genuine AI reasoning.**

### What's Missing

**To use real LLM reasoning:**
1. Need `llm_client` passed to `CanonicalGoalOrchestrator`
2. Set `use_placeholder=False`
3. LLM must actually reason about goals (not template matching)

**Current Bootstrap:**
```python
# Line 181 - WRONG
self.components['canonical_goal_orchestrator'] = create_goal_orchestrator(use_placeholder=True)

# Should be:
self.components['canonical_goal_orchestrator'] = create_goal_orchestrator(
    llm_client=some_llm_client,
    use_placeholder=False
)
```

---

## The Core Problem: Who Generates Goals?

### Current State (Placeholder = True)
**Computer generates goals** using threshold logic:
- Low clarity? ‚Üí Add "Clarify task" goal
- Low knowledge? ‚Üí Add "Investigate" goal  
- High confidence? ‚Üí Add "Execute" goal

**Pros:** Fast, deterministic, no API calls  
**Cons:** Not genuine reasoning, template-based, doesn't understand context deeply

### Intended State (LLM Reasoning)
**AI reasons about goals** based on understanding:
- Read conversation context
- Consider epistemic state holistically
- Generate goals that make sense for THIS situation
- Provide genuine reasoning (not template)

**Pros:** Genuine understanding, context-aware, true reasoning  
**Cons:** Requires LLM API, costs tokens, may be slower

### User's Expectation
Based on "goals need to be grabbed from context and evaluated by the AI":

The AI should:
1. Read the user's request in full context
2. Assess its own epistemic state
3. **Reason** about what goals make sense
4. Generate goals with actual understanding, not pattern matching

**This requires `use_placeholder=False` and an LLM client.**

---

## Recommendations

### Fix 1: MCP Tool Name (Quick Fix)
**Check what client expects:**
```bash
# List all MCP tools to see naming pattern
mcp list-tools
```

**If all tools lack prefix:** Client config issue, not code issue  
**If some have prefix:** Need to standardize naming

### Fix 2: Enable Real Goal Orchestration (Major Change)

**Option A: Make Placeholder Smarter (Interim)**
- Keep `use_placeholder=True` but improve logic
- Instead of threshold checks, have AI read context and generate goals
- Still not true LLM reasoning but better than current

**Option B: Enable LLM Goal Generation (Proper Fix)**
```python
# empirica/bootstraps/optimal_metacognitive_bootstrap.py

# Need to get LLM client (from where?)
from somewhere import get_llm_client

llm_client = get_llm_client()  # How to get this?

self.components['canonical_goal_orchestrator'] = create_goal_orchestrator(
    llm_client=llm_client,
    use_placeholder=False  # Use real LLM reasoning
)
```

**Challenge:** Where does `llm_client` come from?
- Does Empirica have an LLM client built-in?
- Should user provide it during bootstrap?
- Should it use the same LLM that's running Empirica?

**This is a design question:**
- If Empirica runs inside Claude/GPT, the goal orchestrator should use the SAME AI
- If Empirica is separate, need to configure LLM endpoint

### Fix 3: Clarify Bootstrap Comments

Update comments to reflect reality:
```python
# BEFORE (misleading):
print("   ‚úÖ Canonical goal orchestrator loaded (LLM-powered, no heuristics)")

# AFTER (accurate):
print("   ‚úÖ Canonical goal orchestrator loaded (placeholder mode - threshold-based)")
print("   üí° For LLM-powered goal generation, configure llm_client")
```

---

## Questions to Answer

### Q1: How should goals be generated?
**Current:** Threshold-based heuristics  
**Intended:** LLM reasoning  
**Question:** Which do you want?

If **LLM reasoning**, need to answer:
- What LLM should generate goals?
- How does Empirica access it?
- Is it the same LLM running the agent or separate?

### Q2: Where does llm_client come from?
**Options:**
1. **Self-referential:** Agent uses itself to generate goals
   - Claude generates goals for Claude
   - GPT generates goals for GPT
   
2. **Separate LLM:** Dedicated goal generation service
   - Bootstrap receives llm_client as parameter
   - User configures during setup

3. **No LLM (keep placeholder):** Improve placeholder logic
   - Make threshold logic smarter
   - Not true AI reasoning but better than now

### Q3: What about the "legacy goal orchestrator" fallback?
```python
# Line 189-202: Fallback to legacy heuristic-based orchestrator
except Exception as e:
    from empirica.components.goal_management.autonomous_goal_orchestrator import (...)
```

**Question:** Should this fallback exist?
- If goal generation fails, fall back to heuristics?
- Or fail fast and require fix?

---

## Proposed Fixes

### Immediate (This Session)

**Fix 1: Update Bootstrap Comments**
```python
# Be honest about what's happening
print("   ‚úÖ Goal orchestrator loaded (placeholder mode)")
print("   üí° Using threshold-based goal generation")
print("   üí° For LLM-powered goals, configure llm_client")
```

**Fix 2: Document Limitation**
Add to documentation:
- Goal orchestrator currently uses placeholder (threshold-based)
- For true LLM reasoning, need to configure llm_client
- Roadmap: Enable self-referential goal generation

**Fix 3: Fix MCP Tool Name**
Investigate client calling `empirica-bootstrap_session`

### Short-term (Next Session)

**Design Decision: How to Enable LLM Goals**

Option A: **Self-Referential** (Recommended)
```python
# Agent uses itself to generate goals
# When Claude runs Empirica, Claude generates its own goals

def bootstrap_with_self_reflection(ai_id, llm_callback):
    """
    ai_id: Agent identifier
    llm_callback: Function to call current LLM for reasoning
    """
    orchestrator = create_goal_orchestrator(
        llm_client=llm_callback,
        use_placeholder=False
    )
```

Option B: **External LLM**
```python
# Configure external LLM for goal generation
orchestrator = create_goal_orchestrator(
    llm_client=OpenAIClient(api_key=...),
    use_placeholder=False
)
```

### Long-term

**Remove Placeholder Entirely**
- Make LLM reasoning the default
- Remove threshold-based fallback
- Require llm_client configuration

---

## Current Status

**Goal Orchestrator:**
- ‚úÖ Architecture is sound (LLM-first design)
- ‚ö†Ô∏è Implementation uses placeholder (threshold-based)
- ‚ùå Not using genuine AI reasoning as intended

**Bootstrap:**
- ‚úÖ Comments indicate awareness of heuristic issue
- ‚ö†Ô∏è Still initializes with `use_placeholder=True`
- ‚ùå Misleading comments claim "no heuristics"

**MCP Tool:**
- ‚úÖ Bootstrap tool exists and works
- ‚ö†Ô∏è Name mismatch issue (`bootstrap_session` vs `empirica-bootstrap_session`)

---

## Decision Required

**You need to decide:**

1. **Keep placeholder for now?**
   - Pro: Works without LLM client
   - Con: Not genuine AI reasoning

2. **Enable LLM goal generation?**
   - Pro: True AI reasoning
   - Con: Need to design llm_client integration

3. **Which LLM should generate goals?**
   - Same AI running Empirica? (self-referential)
   - Separate LLM? (external service)

**Recommendation:**
Enable **self-referential goal generation** where the AI running Empirica generates its own goals. This is most elegant and requires minimal configuration.

---

**Status:** Audit complete, awaiting decision on fixes
