# Handoff to Test AI (Qwen/Gemini)

**Date:** 2025-11-08  
**Task:** Implement comprehensive test suite for Empirica Phase 0 MVP  
**Status:** Ready for implementation  
**Estimated Time:** 12-18 hours

---

## ðŸŽ¯ Your Mission

Implement a comprehensive test suite for Empirica to validate it's ready for production release. Focus on:

1. âœ… **NO HEURISTICS validation** - Ensure no static values or shortcuts
2. âœ… **Genuine self-assessment enforcement** - Verify real epistemic tracking
3. âœ… **Code quality** - Linting, formatting, type checking
4. âœ… **Component testing** - Unit and integration tests
5. âœ… **Framework integrity** - Core principle validation

---

## ðŸ“‹ What's Already Done

### âœ… Test Infrastructure Created:
- `pyproject.toml` - Full test configuration
- `Makefile` - Convenient test commands
- `tests/conftest.py` - Pytest fixtures and helpers
- `tests/integrity/test_no_heuristics.py` - Integrity test starter
- `docs/testing/COMPREHENSIVE_TEST_PLAN.md` - Detailed plan

### âœ… Cleanup Complete:
- Repository cleaned and organized
- 50+ old files archived to `_archive/`
- Root directory now has only ~15 essential files
- Clean structure for Phase 0 MVP

---

## ðŸš€ Quick Start

### 1. Install Dependencies
```bash
cd /path/to/empirica

# Install with all dev dependencies
pip install -e ".[dev,mcp]"

# Or just test dependencies
pip install -e ".[test]"
```

### 2. Verify Setup
```bash
# Check installation
python3 -c "import empirica; print('âœ… Empirica installed')"

# Run existing tests (if any)
pytest tests/ -v

# Check linting
make lint

# Check types
make typecheck
```

### 3. Review Documentation
Read these files in order:
1. `docs/testing/COMPREHENSIVE_TEST_PLAN.md` - Full test plan
2. `docs/phase_0/EMPIRICA_SINGLE_AI_FOCUS.md` - Phase 0 focus
3. `docs/production/README.md` - Production docs
4. `README.md` - Main readme

---

## ðŸ“ Implementation Plan

### Phase 1: Setup & Validation (1-2 hours)

**Goal:** Get environment working and validate current state

**Tasks:**
1. Install dependencies: `make install`
2. Run linting: `make lint`
3. Run type checking: `make typecheck`
4. Fix any immediate issues found
5. Run existing integrity test: `pytest tests/integrity/ -v`

**Expected Output:**
- Dependencies installed
- Linting shows violations (we'll fix in Phase 2)
- Type checking may show errors (we'll fix in Phase 3)
- Integrity test should PASS (validates no heuristics)

---

### Phase 2: Linting & Formatting (1-2 hours)

**Goal:** Clean up code style and ensure consistency

**Tasks:**
1. Format code: `make format`
2. Fix linting issues: `make lint-fix`
3. Review remaining violations
4. Manually fix complex violations
5. Verify clean: `make lint`

**Files to focus on:**
- `empirica/cli/` - CLI commands
- `empirica/core/` - Core components
- `mcp_local/` - MCP server

**Commands:**
```bash
# Auto-format everything
make format

# Auto-fix linting issues
make lint-fix

# Check remaining issues
make lint

# For specific files
ruff check --fix empirica/cli/command_handlers/cascade_commands.py
```

**Success Criteria:**
- âœ… Zero linting violations
- âœ… Consistent code style
- âœ… All imports sorted

---

### Phase 3: Type Checking (2-3 hours)

**Goal:** Add type hints and fix type errors

**Tasks:**
1. Run type checking: `make typecheck`
2. Add missing type hints to functions
3. Fix type errors
4. Re-run until clean: `make typecheck`

**Priority files:**
- `empirica/core/canonical/canonical_epistemic_assessment.py`
- `empirica/cli/command_handlers/*.py`
- `mcp_local/empirica_mcp_server.py`

**Common fixes:**
```python
# Before
def assess(self, prompt, context):
    ...

# After
def assess(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
    ...
```

**Success Criteria:**
- âœ… >90% functions have type hints
- âœ… Zero type errors from pyright
- âœ… Public API fully typed

---

### Phase 4: Unit Tests - Core Components (4-6 hours)

**Goal:** Test individual components

**Create these test files:**

#### 4.1 `tests/unit/test_canonical_assessor.py`
Test CanonicalEpistemicAssessor:
- âœ… Generates self-assessment prompt
- âœ… Returns assessment_id
- âœ… Does NOT return vector scores directly
- âœ… Parses LLM responses correctly
- âœ… Validates assessment structure

```python
import pytest
from empirica.core.canonical import CanonicalEpistemicAssessor

@pytest.mark.asyncio
async def test_assessor_generates_prompt():
    assessor = CanonicalEpistemicAssessor(agent_id="test")
    result = await assessor.assess("test task", {})
    
    assert isinstance(result, dict)
    assert "self_assessment_prompt" in result
    assert "assessment_id" in result
    assert "vectors" not in result  # Should NOT have pre-computed scores

# Add more tests following the plan...
```

#### 4.2 `tests/unit/test_reflex_logger.py`
Test ReflexLogger:
- âœ… Creates log files in correct location
- âœ… Writes valid JSON
- âœ… Includes all required fields
- âœ… Handles concurrent writes

#### 4.3 `tests/unit/test_session_database.py`
Test SessionDatabase:
- âœ… Creates sessions
- âœ… Creates cascades
- âœ… Stores metadata
- âœ… Retrieves session history

#### 4.4 `tests/unit/test_cli_commands.py`
Test CLI command handlers:
- âœ… MCP commands work
- âœ… Preflight requires assessment
- âœ… Postflight calculates delta
- âœ… Output formats work (JSON, compact, etc.)

**Success Criteria:**
- âœ… >80% code coverage
- âœ… All tests passing
- âœ… Core components validated

---

### Phase 5: Integration Tests (2-3 hours)

**Goal:** Test component interactions

**Create these test files:**

#### 5.1 `tests/integration/test_preflight_postflight_flow.py`
Test complete workflow:
- âœ… Preflight â†’ work â†’ postflight
- âœ… Delta calculation
- âœ… Calibration assessment
- âœ… Learning validation

#### 5.2 `tests/integration/test_mcp_cli_integration.py`
Test MCP + CLI:
- âœ… MCP server start/stop
- âœ… MCP tools listing
- âœ… CLI can manage MCP server

#### 5.3 `tests/integration/test_session_continuity.py`
Test session management:
- âœ… Session creation
- âœ… Session retrieval
- âœ… Multiple cascades per session
- âœ… History tracking

**Success Criteria:**
- âœ… All major workflows tested
- âœ… Integration points validated
- âœ… End-to-end scenarios working

---

### Phase 6: Integrity Tests (2-3 hours)

**Goal:** Validate framework principles

The starter file `tests/integrity/test_no_heuristics.py` already exists.

**Additional tests to add:**

#### 6.1 Expand `test_no_heuristics.py`
- âœ… Scan all Python files for static vectors
- âœ… Verify no keyword matching
- âœ… Check no confabulation patterns

#### 6.2 Create `tests/integrity/test_genuine_assessment.py`
Test genuine assessment enforcement:
- âœ… Assessment requires LLM response
- âœ… Parse_llm_response extracts real scores
- âœ… No simulation or fake data

#### 6.3 Create `tests/integrity/test_framework_principles.py`
Test core principles:
- âœ… Privacy-first (local storage only)
- âœ… Universal interface (no model lock-in)
- âœ… 13-vector system complete

**Success Criteria:**
- âœ… NO HEURISTICS validated
- âœ… Genuine assessment enforced
- âœ… Framework integrity confirmed

---

### Phase 7: Final Validation (1 hour)

**Goal:** Ensure everything is ready for release

**Tasks:**
1. Run full test suite: `make test-cov`
2. Generate coverage report
3. Run all checks: `make validate-full`
4. Document any known issues
5. Create test report

**Commands:**
```bash
# Run everything with coverage
make validate-full

# Check results
open htmlcov/index.html  # View coverage report

# Create test report
pytest tests/ --html=test_report.html --self-contained-html
```

**Success Criteria:**
- âœ… >80% code coverage
- âœ… All tests passing
- âœ… Zero linting violations
- âœ… Zero type errors
- âœ… All integrity tests passing

---

## ðŸ§ª Testing Commands Reference

### Quick Commands
```bash
make help                 # Show all commands
make test                 # Run all tests
make test-unit            # Unit tests only
make test-integration     # Integration tests only
make test-integrity       # Integrity tests only
make test-cov             # Tests with coverage
make lint                 # Check code quality
make lint-fix             # Auto-fix linting
make format               # Format code
make typecheck            # Check types
make validate             # Full validation
```

### Pytest Commands
```bash
# Run specific test file
pytest tests/unit/test_canonical_assessor.py -v

# Run specific test function
pytest tests/unit/test_canonical_assessor.py::test_assessor_generates_prompt -v

# Run with markers
pytest -m integrity        # Only integrity tests
pytest -m "not slow"       # Exclude slow tests

# Run with coverage
pytest tests/unit/ --cov=empirica --cov-report=html

# Run with verbose output
pytest tests/ -vv
```

---

## ðŸ“Š What to Report

After completing each phase, report:

### Progress Report Format:
```markdown
## Phase X: [Name] - [Status]

**Time Spent:** X hours
**Status:** âœ… Complete / â³ In Progress / âŒ Blocked

**Completed:**
- âœ… Task 1
- âœ… Task 2

**Issues Found:**
- âš ï¸ Issue description
- ðŸ“ How fixed / workaround

**Test Results:**
- Tests run: X
- Tests passed: X
- Coverage: X%

**Next Steps:**
- Move to Phase X+1
- Fix remaining issues
```

### Final Report Should Include:
1. **Summary** - Overall status
2. **Test Coverage** - Percentage and report
3. **Issues Found** - List of problems discovered
4. **Issues Fixed** - What was corrected
5. **Known Issues** - Remaining problems (if any)
6. **Recommendations** - Suggestions for improvement

---

## âš ï¸ Important Notes

### Critical: NO HEURISTICS
- If you find static baseline values, report immediately
- All vector scores MUST come from genuine LLM responses
- No shortcuts, no simulations, no fake data
- This is non-negotiable

### Testing Philosophy
- Write tests that validate behavior, not implementation
- Focus on public APIs and interfaces
- Mock external dependencies (APIs, file system when appropriate)
- Use fixtures from conftest.py

### Code Quality Standards
- Line length: 120 characters max
- Type hints on all public functions
- Docstrings on classes and public methods
- Consistent import ordering (handled by ruff)

---

## ðŸ†˜ Getting Help

### Documentation:
- `docs/testing/COMPREHENSIVE_TEST_PLAN.md` - Full details
- `docs/phase_0/EMPIRICA_SINGLE_AI_FOCUS.md` - What matters for Phase 0
- `docs/production/README.md` - How Empirica works

### Pytest Help:
```bash
pytest --help              # General help
pytest --markers           # Show available markers
pytest --fixtures          # Show available fixtures
```

### Debugging Tests:
```bash
# Run with print statements visible
pytest tests/unit/ -s

# Run with pdb on failure
pytest tests/unit/ --pdb

# Run single test with verbose output
pytest tests/unit/test_file.py::test_function -vv
```

---

## âœ… Success Criteria

### Minimum Release Requirements:
- âœ… All integrity tests passing (NO HEURISTICS validated)
- âœ… >80% unit test coverage
- âœ… All integration tests passing
- âœ… Zero linting violations
- âœ… Zero type errors
- âœ… All CLI commands tested
- âœ… MCP server tested

### Stretch Goals:
- âœ… >90% test coverage
- âœ… Performance benchmarks
- âœ… Load testing
- âœ… Security audit

---

## ðŸŽ‰ When Complete

1. Commit all test code
2. Generate final coverage report
3. Create test summary document
4. Tag version as "test-ready"
5. Report back to main development team

---

**Good luck! Remember: Focus on validating NO HEURISTICS and genuine self-assessment. Everything else is secondary.**

**Questions?** Review the comprehensive test plan or check existing test examples in `~/empirica-parent/pydantic-ai/tests/` for inspiration.

---

**Status:** âœ… Ready for AI tester handoff  
**Next:** Begin Phase 1 (Setup & Validation)
