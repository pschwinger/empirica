# Auto Issue Capture: Continuous Epistemic Learning System

## Overview

Auto Issue Capture is not just error tracking - it's a **continuous epistemic learning system** that enables AI agents to:
- Capture problems discovered during work without interrupting flow
- Learn patterns from previous work (bugs, performance issues, incomplete work)
- Make better decisions across sessions by understanding what's been tried
- Improve autonomously through semantic retrieval of similar issues

## Architecture: 5-Layer Knowledge System

Empirica's epistemic memory consists of five complementary layers, all storable in Qdrant for semantic retrieval:

```
PROJECT KNOWLEDGE GRAPH
â”œâ”€â”€ 1. FINDINGS (positive discoveries)
â”‚   â””â”€â”€ "Implemented OAuth2 with JWT tokens"
â”‚   â””â”€â”€ Semantic: "auth implementation patterns"
â”‚
â”œâ”€â”€ 2. UNKNOWNS (gaps identified)
â”‚   â””â”€â”€ "Token refresh mechanism unclear"
â”‚   â””â”€â”€ Semantic: "gaps in current knowledge"
â”‚
â”œâ”€â”€ 3. AUTO-CAPTURED ISSUES (problems encountered)
â”‚   â””â”€â”€ "CHECK-SUBMIT doesn't support stdin JSON"
â”‚   â””â”€â”€ Semantic: "CLI compatibility issues"
â”‚   â””â”€â”€ Status: resolved â†’ next AI knows it was fixed
â”‚
â”œâ”€â”€ 4. MISTAKES (things tried and failed)
â”‚   â””â”€â”€ "Tried connection pooling with HikariCP - caused deadlocks"
â”‚   â””â”€â”€ Semantic: "anti-patterns, what NOT to do"
â”‚
â”œâ”€â”€ 5. DEAD ENDS (paths explored and abandoned)
â”‚   â””â”€â”€ "Investigated WebSocket approach - too complex for use case"
â”‚   â””â”€â”€ Semantic: "architectural decisions, why NOT taken"
â”‚
â””â”€â”€ 6. EPISTEMIC ARTIFACTS (sources & validation)
    â””â”€â”€ Papers, docs, validated facts
    â””â”€â”€ Semantic: "ground truth for this domain"
```

## How Issues Fit Into Epistemic Learning

### Issue Lifecycle and Meaning

Each issue passes through phases with semantic significance:

```
NEW â†’ INVESTIGATING â†’ RESOLVED/WONTFIX/HANDOFF
                â†“
         Each state tells next AI something
```

**NEW**: "Problem discovered, no action taken yet"
- Next AI: "Should I work on this?"

**INVESTIGATING**: "AI actively working on it"
- Next AI: "This is being handled, don't duplicate"

**RESOLVED**: "AI X fixed this on [date]"
- Next AI: "This problem is solved. If you see it again, here's what worked"

**HANDOFF**: "AI X couldn't finish, marking for specialist"
- Next AI: "I'm the specialist - here's context from who started it"

**WONTFIX**: "Intentional decision not to fix"
- Next AI: "This is a known issue but we're accepting it - here's why"

### Semantic Retrieval Use Cases

Once all issues are stored in Qdrant with vectors:

**Case 1: Avoid Duplicate Work**
```
New AI encounters timeout in database queries
â†’ Semantic search: "database performance issues"
â†’ Finds: "Performance issue: Query user_profiles took 2500ms (expected 500ms)"
â†’ Finds resolution: "Implemented connection pooling"
â†’ AI: "I should implement connection pooling first"
```

**Case 2: Learn From Mistakes**
```
AI considers using approach X
â†’ Semantic search: "approach X patterns"
â†’ Finds mistake: "Tried WebSocket approach - too complex for this use case"
â†’ AI: "Someone already tried that and abandoned it. Why? Let me check..."
```

**Case 3: Pattern Recognition**
```
AI notices 3 similar CLI compatibility issues
â†’ Semantic search groups them
â†’ AI discovers pattern: "stdin JSON support missing across commands"
â†’ AI proposes systemic fix: "Add config file support to all commands"
```

**Case 4: Cross-AI Knowledge Transfer**
```
AI-1 encounters bug, creates issue, marks RESOLVED
AI-2 starts on related task
â†’ Project-bootstrap includes issue with status=resolved
â†’ AI-2 semantic search: "related CLI issues"
â†’ Finds: "Previous AI fixed CHECK-SUBMIT stdin support"
â†’ AI-2: "I should check if my work benefits from that fix"
```

## Issue Categories and Semantic Meaning

| Category | Meaning | Cross-Session | Semantic Value |
|----------|---------|----------------|-----------------|
| **BUG** | Code defect | âœ… YES | Anti-pattern, what to avoid |
| **ERROR** | Runtime failure | âš ï¸ CONTEXT | Was it transient or systemic? |
| **TODO** | Incomplete work | âœ… YES | Work queue, continuous improvement |
| **PERFORMANCE** | Degradation | âœ… YES | Optimization opportunities |
| **DEPRECATION** | Old patterns | âœ… YES | Migration path, what's superseded |
| **COMPATIBILITY** | Version/platform issues | âœ… YES | Environmental constraints |
| **DESIGN** | Architecture question | âœ… YES | Design decisions, tradeoffs |
| **WARNING** | Potential problem | âš ï¸ CONTEXT | Risk assessment |

## Integration Points

### 1. CASCADE Workflow

```
PREFLIGHT â†’ THINK â†’ PLAN â†’ INVESTIGATE â†’ CHECK â†’ ACT â†’ POSTFLIGHT
                                  â†“
                         Auto-capture issues
                         during investigation
                         
                          â†“
                         
                    Display in project-bootstrap
                    as epistemic context
```

### 2. Project-Bootstrap Output

```json
{
  "findings": [...],
  "unknowns": [...],
  "issues": {
    "active": [
      {"id": "...", "category": "bug", "message": "..."}
    ],
    "resolved": [
      {"id": "...", "message": "...", "resolution": "..."}
    ]
  }
}
```

### 3. Qdrant Semantic Index

All issues stored as vectors:
```
Issue â†’ Embedding â†’ Qdrant Vector Store
          â†“
    Semantic search for related issues
    Pattern detection across sessions
    Continuous learning
```

## Decision: What Gets Stored vs What Gets Shown

### Stored (All Sessions, All Projects)
- âœ… All issues with status
- âœ… Resolution notes
- âœ… Who fixed it and when
- âœ… Full semantic embedding

### Shown in project-bootstrap
- âœ… Active issues (new, investigating, handoff)
- âœ… Recently resolved (last 30 days)
- âœ… Critical bugs (severity=blocker)
- âš ï¸ Old resolved issues (summary count only)

### Removed (Never)
- âŒ Issues never deleted
- âœ… Status marks completion, not deletion
- âœ… Audit trail preserved
- âœ… Semantic learning maintained

## Continuous Learning Loop

```
Session N: AI-1 encounters issue â†’ captures it â†’ resolves it
                                    â†“
                            Stored in project DB
                            Vectorized in Qdrant
                                    â†“
Session N+1: AI-2 starts work
                â†“
        project-bootstrap shows issues
        AI-2 semantic search finds related issues
                â†“
        AI-2 learns: "Someone tried X before"
                â†“
        AI-2 makes better decision
                â†“
        Project knowledge improves
                â†“
Session N+2: AI-3 benefits from N+1's learning
                â†“
        Continuous improvement cycle
```

## Implementation Status

### Phase 1: Core Capture âœ… COMPLETE
- Issue capture service fully functional
- CLI commands (6 total) operational
- Database schema in place
- Manual testing verified

### Phase 2: CASCADE Integration ğŸ”„ IN PROGRESS
- [ ] Add issues to project-bootstrap output
- [ ] Display active issues in CHECK gate
- [ ] Filter resolved issues appropriately

### Phase 3: Qdrant Integration ğŸ“‹ PENDING
- [ ] Embed all issues with vectors
- [ ] Semantic search for similar issues
- [ ] Pattern detection across sessions
- [ ] Cross-AI knowledge transfer

### Phase 4: Learning Analytics ğŸ“‹ PENDING
- [ ] Track which resolved issues were actually helpful
- [ ] Measure pattern recognition effectiveness
- [ ] Improve AI decision quality over time
- [ ] Generate insights about project trends

## Example: Real-World Scenario

```
PROJECT: Empirica Core CLI

Session 1 (AI-1 - rovo-dev):
â”œâ”€ Discovers: "CHECK-SUBMIT doesn't support stdin JSON"
â”œâ”€ Auto-captures: BUG, severity=HIGH, status=NEW
â”œâ”€ Fixes it: Adds config file support
â”œâ”€ Marks: status=RESOLVED, resolution="Added stdin/config parsing"
â””â”€ Result: Issue stored + vectorized

Session 2 (AI-2 - qwen-optimizer):
â”œâ”€ Runs project-bootstrap
â”œâ”€ Sees: "Recently resolved: CHECK-SUBMIT stdin support"
â”œâ”€ Semantic search: "CLI JSON input issues"
â”œâ”€ Finds: "CHECK also lacked stdin support"
â”œâ”€ Applies same fix: "Config file support pattern"
â””â”€ Result: Systemic improvement, not one-off fix

Session 3 (AI-3 - analysis-bot):
â”œâ”€ Semantic analysis across all issues
â”œâ”€ Detects pattern: "stdin JSON missing in many commands"
â”œâ”€ Creates epic: "Standardize JSON input across CLI"
â”œâ”€ Proposes systemic solution
â””â”€ Result: Project-level improvement from accumulated learning
```

## Epistemic Value Proposition

Without auto-capture:
- Each AI rediscovers the same problems
- Context lost between sessions
- No continuous improvement
- Knowledge silos

With auto-capture + semantic retrieval:
- **Pattern Recognition**: Similar issues grouped by semantics
- **Decision Support**: "Here's what was tried before"
- **Continuous Learning**: Each session improves project knowledge
- **Audit Trail**: Full history of what was tried and why
- **Multi-AI Coordination**: Handoff without context loss

## Configuration

### To Enable/Disable Issue Capture
```bash
# In session initialization
empirica session-create --ai-id myai --auto-capture=true
```

### To Include in project-bootstrap
```bash
empirica project-bootstrap --project-id proj-uuid --include-issues=active
```

### To Perform Semantic Search
```bash
# Phase 3 implementation (pending)
empirica issues-search --project-id proj-uuid \
  --semantic-query "database performance problems"
```

## Next Steps

1. **Add issues to project-bootstrap** (next session)
2. **Implement Qdrant integration** (Phase 3)
3. **Build semantic search UI** (Phase 4)
4. **Measure learning effectiveness** (Phase 4)

---

**Key Insight**: Auto Issue Capture is the foundation for continuous epistemic learning across AI agents. By storing not just what was discovered, but also what was tried and the results, the system enables autonomous improvement over time.
