# Production Deployment Guide

**Empirica v4.0 - Deploying to Production**

**Storage Architecture:** See `docs/architecture/STORAGE_ARCHITECTURE_COMPLETE.md`  

**v4.0 Note:** Examples below may reference `bootstrap_level` parameter - this exists for backward compatibility but has no behavioral effect in v4.0. All sessions use unified storage and lazy component loading.

---

## Pre-Deployment Checklist

- [ ] Python 3.8+ installed
- [ ] All dependencies tested (`pip install -r requirements.txt`)
- [ ] Session creation tested
- [ ] Configuration validated
- [ ] Tests passing
- [ ] Documentation reviewed

---

## Configuration

### Environment Variables

```bash
# .env file
EMPIRICA_CONFIDENCE_THRESHOLD=0.75
EMPIRICA_MAX_INVESTIGATION_ROUNDS=3
EMPIRICA_DB_PATH=/var/lib/empirica/sessions.db
EMPIRICA_ENABLE_DASHBOARD=false
EMPIRICA_LOG_LEVEL=INFO

# Optional: LLM config
OLLAMA_URL=http://localhost:11434
OPENAI_API_KEY=your_key_here
```

### Load Configuration

```python
import os
from empirica.data.session_database import SessionDatabase
from empirica.assessment.canonical_assessor import CanonicalEpistemicAssessor

confidence = float(os.getenv('EMPIRICA_CONFIDENCE_THRESHOLD', '0.70'))
db_path = os.getenv('EMPIRICA_DB_PATH', '.empirica/sessions/sessions.db')

# Create session
db = SessionDatabase()
session_id = db.create_session(ai_id="production_ai")
db.close()

# Use canonical assessor
assessor = CanonicalEpistemicAssessor(session_id=session_id)
    action_confidence_threshold=confidence,
    enable_action_hooks=False,  # Disable tmux in production
    enable_session_db=True
)
```

---

## Deployment Options

### Option 1: Containerized (Docker)

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy Empirica
COPY empirica/ /app/empirica/

# Create data directory
RUN mkdir -p /app/.empirica/sessions

# Environment
ENV PYTHONPATH=/app
ENV EMPIRICA_BOOTSTRAP_LEVEL=2

# Run
CMD ["python3", "your_app.py"]
```

```bash
# Build
docker build -t empirica-app .

# Run
docker run -v empirica_data:/app/.empirica empirica-app
```

### Option 2: Kubernetes

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: empirica-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: empirica
  template:
    metadata:
      labels:
        app: empirica
    spec:
      containers:
      - name: empirica
        image: empirica-app:latest
        env:
        - name: EMPIRICA_BOOTSTRAP_LEVEL
          value: "2"
        - name: EMPIRICA_ENABLE_DASHBOARD
          value: "false"
        volumeMounts:
        - name: empirica-data
          mountPath: /app/.empirica
      volumes:
      - name: empirica-data
        persistentVolumeClaim:
          claimName: empirica-pvc
```

### Option 3: Serverless (AWS Lambda)

```python
# lambda_handler.py
import asyncio
from empirica.data.session_database import SessionDatabase
from empirica.core.metacognitive_cascade import CanonicalEpistemicCascade

# Initialize once (cold start)
db = SessionDatabase()
cascade = CanonicalEpistemicCascade(
    action_confidence_threshold=0.70,
    max_investigation_rounds=3
)

def lambda_handler(event, context):
    task = event['task']
    cascade_context = event.get('context', {})
    
    # Create session for this invocation
    session_id = db.create_session(ai_id="lambda", bootstrap_level=1)
    
    result = asyncio.run(
        cascade.run_epistemic_cascade(task, cascade_context)
    )
    
    return {
        'statusCode': 200,
        'body': result
    }
```

---

## Performance Optimization

### 1. Session Bootstrap Level Selection

```bash
# Create sessions with appropriate bootstrap levels

# Development: Level 2 (full metacognitive tracking)
empirica session-create --ai-id myai --bootstrap-level 2

# Production: Level 1 (recommended - standard tracking)
empirica session-create --ai-id myai --bootstrap-level 1

# Minimal: Level 0 (minimal logging, fastest)
empirica session-create --ai-id myai --bootstrap-level 0
```

```python
# Or via Python API
from empirica.data.session_database import SessionDatabase

db = SessionDatabase()

# Production (recommended)
session_id = db.create_session(ai_id="myai", bootstrap_level=1)

# Development
session_id = db.create_session(ai_id="myai", bootstrap_level=2)

# Minimal
session_id = db.create_session(ai_id="myai", bootstrap_level=0)
```

**Recommendation:** Use **Level 1** for production (standard tracking, good performance).

### 2. Investigation Limits

```python
cascade = CanonicalEpistemicCascade(
    max_investigation_rounds=2,  # Limit rounds (default: 3)
    action_confidence_threshold=0.70  # Adjust threshold
)
```

### 3. Database Optimization

```python
from empirica.data import SessionDatabase

# Use connection pooling
db = SessionDatabase(db_path='/fast/ssd/sessions.db')

# Batch operations
session_id = db.create_session(...)
cascade_ids = [db.create_cascade(...) for _ in range(10)]

# Close connections
db.close()
```

### 4. Disable Optional Features

```python
cascade = CanonicalEpistemicCascade(
    enable_action_hooks=False,      # No tmux in production
    enable_drift_monitor=False,     # Disable if not needed
    enable_bayesian=True            # Keep for precision-critical
)
```

---

## Monitoring & Observability

### Logging

```python
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/empirica/app.log'),
        logging.StreamHandler()
    ]
)

# Use in cascade
logger = logging.getLogger('empirica')
logger.info(f"Cascade started: {task}")
```

### Metrics

```python
import time
from prometheus_client import Counter, Histogram

# Define metrics
cascade_count = Counter('empirica_cascades_total', 'Total cascades')
cascade_duration = Histogram('empirica_cascade_duration_seconds', 'Cascade duration')
investigation_rounds = Histogram('empirica_investigation_rounds', 'Investigation rounds')

# Track metrics
@cascade_duration.time()
async def run_tracked_cascade(task, context):
    cascade_count.inc()
    result = await cascade.run_epistemic_cascade(task, context)
    investigation_rounds.observe(result['investigation_rounds'])
    return result
```

### Health Checks

```python
# health.py
async def health_check():
    try:
        # Test session creation
        db = SessionDatabase()
        session_id = db.create_session(ai_id="healthcheck", bootstrap_level=0)
        db.close()
        
        return {'status': 'healthy', 'session_id': session_id}
    except Exception as e:
        return {'status': 'unhealthy', 'error': str(e)}
```

---

## Security

### 1. Secure Database

```python
# Use encrypted database
db = SessionDatabase('/encrypted/volume/sessions.db')

# Set proper permissions
chmod 600 /encrypted/volume/sessions.db
```

### 2. API Keys

```python
# Never hardcode keys
import os

api_key = os.getenv('API_KEY')  # From environment
# OR
from secrets_manager import get_secret
api_key = get_secret('empirica/api_key')
```

### 3. Input Validation

```python
def validate_input(task: str, context: dict) -> bool:
    # Validate task
    if not task or len(task) > 10000:
        raise ValueError("Invalid task")
    
    # Validate context
    if not isinstance(context, dict):
        raise ValueError("Invalid context")
    
    return True

# Use before cascade
validate_input(task, context)
result = await cascade.run_epistemic_cascade(task, context)
```

---

## Scaling

### Horizontal Scaling

```python
# Stateless cascade instances
# Can scale horizontally without issues
# Database handles concurrency

# Use load balancer
# nginx, ALB, etc.
```

### Database Scaling

```python
# Option 1: Separate read/write
db_write = SessionDatabase('/primary/sessions.db')
db_read = SessionDatabase('/replica/sessions.db')

# Option 2: Partition by session
def get_db_for_session(session_id):
    shard = hash(session_id) % NUM_SHARDS
    return SessionDatabase(f'/data/shard_{shard}/sessions.db')
```

---

## Backup & Recovery

### Database Backup

```bash
# Daily backup
#!/bin/bash
DATE=$(date +%Y%m%d)
sqlite3 .empirica/sessions/sessions.db ".backup '/backups/sessions_${DATE}.db'"

# Retention (keep 30 days)
find /backups -name "sessions_*.db" -mtime +30 -delete
```

### Restore

```bash
# Restore from backup
cp /backups/sessions_20251028.db .empirica/sessions/sessions.db
```

---

## Troubleshooting Production

### High Memory Usage

```python
# Close connections
db.close()

# Limit investigation rounds
cascade = CanonicalEpistemicCascade(max_investigation_rounds=1)

# Use lower bootstrap level for faster startup
db = SessionDatabase()
session_id = db.create_session(ai_id="production", bootstrap_level=1)
```

### Slow Performance

```python
# Add timeout
import asyncio

try:
    result = await asyncio.wait_for(
        cascade.run_epistemic_cascade(task, context),
        timeout=30.0
    )
except asyncio.TimeoutError:
    # Handle timeout
    pass
```

### Database Lock Errors

```python
# Use connection pooling
# Ensure proper closing
try:
    db = SessionDatabase()
    # operations
finally:
    db.close()
```

---

## Production Checklist

### Before Deployment:
- [ ] Tests passing
- [ ] Bootstrap level = 2
- [ ] Dashboard disabled
- [ ] Database path configured
- [ ] Logging configured
- [ ] Health checks working
- [ ] Monitoring enabled

### After Deployment:
- [ ] Health check passes
- [ ] Metrics collecting
- [ ] Logs flowing
- [ ] Database accessible
- [ ] Performance acceptable
- [ ] Error rate acceptable

---

## Example Production Setup

```python
# production_app.py
import os
import asyncio
import logging
from empirica.core.metacognitive_cascade import CanonicalEpistemicCascade
from empirica.data.session_database import SessionDatabase

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('empirica')

# Load config
CONFIG = {
    'bootstrap_level': int(os.getenv('EMPIRICA_BOOTSTRAP_LEVEL', '1')),
    'confidence_threshold': float(os.getenv('EMPIRICA_CONFIDENCE_THRESHOLD', '0.70')),
    'max_rounds': int(os.getenv('EMPIRICA_MAX_ROUNDS', '3')),
    'db_path': os.getenv('EMPIRICA_DB_PATH', '.empirica/sessions/sessions.db')
}

# Initialize (once)
logger.info("Initializing Empirica...")

cascade = CanonicalEpistemicCascade(
    action_confidence_threshold=CONFIG['confidence_threshold'],
    max_investigation_rounds=CONFIG['max_rounds'],
    enable_action_hooks=False,
    enable_session_db=True
)

db = SessionDatabase(CONFIG['db_path'])

async def process_task(task: str, context: dict, ai_id: str = "production") -> dict:
    """Process task with Empirica"""
    logger.info(f"Processing task: {task[:50]}...")
    
    try:
        # Create session for this task
        session_id = db.create_session(
            ai_id=ai_id,
            bootstrap_level=CONFIG['bootstrap_level']
        )
        
        result = await cascade.run_epistemic_cascade(task, context)
        logger.info(f"Task completed: {result['action']}, confidence: {result['confidence']:.2f}")
        return result
    except Exception as e:
        logger.error(f"Task failed: {e}")
        raise

# Cleanup on shutdown
def shutdown():
    logger.info("Shutting down...")
    db.close()
```

---

## Next Steps

- **Monitoring:** Set up metrics and alerts
- **Scaling:** Add load balancing
- **Security:** Review and harden
- **Documentation:** Keep deployment docs updated

---

**Production-ready deployment with monitoring and security!** ðŸš€


---

**Note:** Empirica uses goals (with vectorial scope and subtasks) and git notes (checkpoints, goals, handoffs) for automatic session continuity and cross-AI coordination. See [Storage Architecture](../architecture/STORAGE_ARCHITECTURE_COMPLETE.md) and [Cross-AI Coordination](26_CROSS_AI_COORDINATION.md).
