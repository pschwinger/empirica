1

arXiv:2311.05232v2 [cs.CL] 19 Nov 2024

A Survey on Hallucination in Large Language Models:
Principles, Taxonomy, Challenges, and Open Questions
LEI HUANG, Harbin Institute of Technology, China
WEIJIANG YU, Huawei Inc., China
WEITAO MA and WEIHONG ZHONG, Harbin Institute of Technology, China
ZHANGYIN FENG and HAOTIAN WANG, Harbin Institute of Technology, China
QIANGLONG CHEN and WEIHUA PENG, Huawei Inc., China
XIAOCHENG FENG∗ , BING QIN, and TING LIU, Harbin Institute of Technology, China
The emergence of large language models (LLMs) has marked a significant breakthrough in natural language
processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to
hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over
the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to
detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs,
LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence
highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in
LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of
LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough
overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative
methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced
by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR
systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination
in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.
CCS Concepts: • Computing methodologies → Natural language generation; • General and reference
→ Surveys and overviews.
Additional Key Words and Phrases: Large Language Models, Hallucination, Factuality, Faithfulness
ACM Reference Format:
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua
Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2024. A Survey on Hallucination in Large Language Models:
Principles, Taxonomy, Challenges, and Open Questions. ACM Transactions on Information Systems 1, 1, Article 1
(January 2024), 58 pages. https://doi.org/10.1145/3703155
∗ corresponding author

Authors’ addresses: Lei Huang, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China,
150001, lhuang@ir.hit.edu.cn; Weijiang Yu, Huawei Inc., Bantian Subdistrict, Shenzhen, Guangdong, China, 518129,
weijiangyu8@gmail.com; Weitao Ma, wtma@ir.hit.edu.cn; Weihong Zhong, whzhong@ir.hit.edu.cn, Harbin Institute of
Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001; Zhangyin Feng, zyfeng@ir.hit.edu.cn; Haotian
Wang, wanght1998@gmail.com, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001;
Qianglong Chen, chenqianglong.ai@gmail.com; Weihua Peng, pengwh.hit@gmail.com, Huawei Inc., Bantian Subdistrict,
Shenzhen, Guangdong, China, 518129; Xiaocheng Feng, xcfeng@ir.hit.edu.cn; Bing Qin, qinb@ir.hit.edu.cn; Ting Liu,
tliu@ir.hit.edu.cn, Harbin Institute of Technology, 800 Dongchuan Road, Harbin, Heilongjiang, China, 150001.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
1046-8188/2024/1-ART1 $15.00
https://doi.org/10.1145/3703155
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:2

1

Huang, et al.

INTRODUCTION

Recently, the emergence of large language models (LLMs) [383], exemplified by LLaMA [299, 300],
Claude [9], Gemini [7, 259] and GPT-4 [232], has ushered in a significant paradigm shift in natural
language processing (NLP), achieving unprecedented progress in language understanding [116, 124],
generation [373, 393] and reasoning [57, 151, 250, 326, 354]. Furthermore, the extensive factual
knowledge encoded within LLMs has demonstrated considerable advancements in leveraging
LLMs for information seeking [6, 246], potentially reshaping the landscape of information retrieval
systems [394]. Nevertheless, in tandem with these remarkable advancements, concerns have arisen
about the tendency of LLMs to generate hallucinations [15, 105], resulting in seemingly plausible yet
factually unsupported content. Further compounding this issue is the capability of LLMs to generate
highly convincing and human-like responses [265], which makes detecting these hallucinations
particularly challenging, thereby complicating the practical deployment of LLMs, especially realworld information retrieval (IR) systems that have integrated into our daily lives like chatbots
[8, 231], search engines [4, 214], and recommender systems [97, 171]. Given that the information
provided by these systems can directly influence decision-making, any misleading information has
the potential to spread false beliefs, or even cause harm.
Notably, hallucinations in conventional natural language generation (NLG) tasks have been
extensively studied [125, 136], with hallucinations defined as generated content that is either
nonsensical or unfaithful to the provided source content. These hallucinations are categorized into
two types: intrinsic hallucination, where the generated output contradicts the source content, and
extrinsic hallucination, where the generated output cannot be verified from the source. However,
given their remarkable versatility across tasks [15, 30], understanding hallucinations in LLMs
presents a unique challenge compared to models tailored for specific tasks. Besides, as LLMs
typically function as open-ended systems, the scope of hallucination encompasses a broader concept,
predominantly manifesting factual errors. This shift necessitates a reevaluation and adjustment
of the existing taxonomy of hallucinations, aiming to enhance its adaptability in the evolving
landscape of LLMs.
In this survey, we propose a redefined taxonomy of hallucination tailored specifically for applications involving LLMs. We categorize hallucination into two primary types: factuality hallucination
and faithfulness hallucination. Factuality hallucination emphasizes the discrepancy between generated content and verifiable real-world facts, typically manifesting as factual inconsistencies.
Conversely, faithfulness hallucination captures the divergence of generated content from user input
or the lack of self-consistency within the generated content. This category is further subdivided
into instruction inconsistency, where the content deviates from the user’s original instruction;
context inconsistency, highlighting discrepancies from the provided context; and logical inconsistency, pointing out internal contradictions within the content. Such categorization refines our
understanding of hallucinations in LLMs, aligning it closely with their contemporary usage.
Delving into the underlying causes of hallucinations in LLMs is essential not merely for enhancing the comprehension of these phenomena but also for informing strategies aimed at alleviating
them. Recognizing the multifaceted sources of LLM hallucinations, our survey identifies potential
contributors into three main aspects: data, training, and inference stages. This categorization allows
us to span a broad spectrum of factors, providing a holistic view of the origins and mechanisms by
which hallucinations may arise within LLM systems. Furthermore, we comprehensively outline a
variety of effective detection methods specifically devised for detecting hallucinations in LLMs, as
well as an exhaustive overview of benchmarks related to LLM hallucinations, serving as appropriate
testbeds to assess the extent of hallucinations generated by LLMs and the efficacy of detection
methods. Beyond evaluation, significant efforts have been undertaken to mitigate hallucinations of

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:3

LLMs. These initiatives are comprehensively surveyed in our study, in accordance with the corresponding causes, spanning from data-related, training-related, and inference-related approaches. In
addition, the effectiveness of retrieval-augmented generation (RAG) in mitigating hallucinations has
garnered tremendous attention within the field. Despite the considerable potential of RAG, current
systems inherently face limitations and even suffer from hallucinations. Accordingly, our survey
undertakes an in-depth analysis of these challenges, aiming to provide valuable insights aimed
at developing more robust RAG systems. We also highlight several promising avenues for future
research, such as hallucinations in large vision-language models and understanding of knowledge
boundaries in LLM hallucinations, paving the way for forthcoming research in the field.
Comparing with Existing Surveys. As hallucination stands out as a major challenge in generative AI, numerous research [136, 192, 258, 298, 312, 376] has been directed towards hallucinations.
While these contributions have explored LLM hallucination from various perspectives and provided
valuable insights, our survey seeks to delineate their distinct contributions and the comprehensive
scope they encompass. Ji et al. [136] primarily shed light on hallucinations in pre-trained models
for NLG tasks, leaving LLMs outside their discussion purview. Tonmoy et al. [298] mainly focused
on discussing the mitigation strategies combating LLM hallucinations. Besides, Liu et al. [192] took
a broader view of LLM trustworthiness without delving into specific hallucination phenomena,
whereas Wang et al. [312] provided an in-depth look at factuality in LLMs. However, our work narrows down to a critical subset of trustworthiness challenges, specifically addressing factuality and
extending the discussion to include faithfulness hallucinations. To the best of our knowledge, Zhang
et al. [376] presented research closely aligned with ours, detailing LLM hallucination taxonomies,
evaluation benchmarks, and mitigation strategies. However, our survey sets itself apart through
a unique taxonomy and organizational structure. We present a detailed, layered classification of
hallucinations and conduct a more comprehensive analysis of the causes of hallucinations. Crucially,
our proposed mitigation strategies are directly tied to these causes, offering a targeted and coherent
framework for addressing LLM hallucinations.
Organization of this Survey. In this survey, we present a comprehensive overview of the latest
developments in LLM hallucinations, as shown in Fig 1. We commence by constructing a taxonomy
of hallucinations in the realm of LLM (§2). Subsequently, we analyze factors contributing to LLM
hallucinations in depth (§3), followed by a review of various strategies and benchmarks employed
for the reliable detection of hallucinations in LLMs (§4). We then detail a spectrum of approaches
designed to mitigate these hallucinations (§5). Concluding, we delve into the challenges faced by
current RAG systems (§6) and delineate potential pathways for forthcoming research (§7).
2

DEFINITIONS

For the sake of a comprehensive understanding of hallucinations in LLMs, we commence with a
succinct introduction to LLMs (§2.1), delineating the scope of this survey. Subsequently, we delve
into the training stages of LLMs (§2.2), as a thorough understanding of the training mechanisms
contributes significantly to elucidating the origins of hallucinations. Lastly, we expound upon the
concept of hallucinations in LLMs (§2.3), further categorizing it into two distinct types.
2.1

Large Language Models

Before delving into the causes of hallucination, we first introduce the concept of LLMs. Typically,
LLMs refer to a series of general-purpose models that leverage the transformer-based language
model architecture and undergo extensive training on massive textual corpora with notable examples including GPT-3 [29], PaLM [54], LLaMA [300], GPT-4 [232] and Gemini [259]. By scaling
the amount of data and model capacity, LLMs raise amazing emergent abilities, typically including
in-context learning (ICL) [29], chain-of-thought prompting [326] and instruction following [244].
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:4

Huang, et al.

Hallucination from Data

Hallucinations in Large Language Models

Hallucination
Causes (§3)

Hallucination from
Training

Hallucination from
Inference

Misinformation and
Biases

e.g. Bender et al. [20], Lee et al. [159], Lin et al. [182]

Knowledge Boundary

e.g. Katz et al. [149], Onoe et al. [230], Singhal et al. [279]

Inferior Alignment Data

e.g. Gekhman et al. [98], Li et al. [168]

Hallucination from
Pre-training

e.g. Li et al. [180], Liu et al. [183], Wang and Sennrich [313]

Hallucination from SFT

e.g. Schulman [269], Yang et al. [341], Zhang et al. [362]

Hallucination from RLHF

e.g. Cotra [64], Perez et al. [245], Sharma et al. [274], Wei et al. [327]

Imperfect Decoding
Strategies

e.g. Holtzman et al. [118], Stahlberg and Byrne [283]

Over-confidence

e.g. Chen et al. [45, 46], Liu et al. [193], Miao et al. [212]

Softmax Bottleneck

e.g. Chang and McCallum [38], Miao et al. [212]

Reasoning Failure

e.g. Berglund et al. [22], Zheng et al. [386]

Factuality Hallucination
Detection

e.g. Dhuliawala et al. [74], Manakul et al. [205], Min et al. [216]

Faithfulness Hallucination
Detection

e.g. Fabbri et al. [80], Maynez et al. [208], Scialom et al. [271]

Hallucination Evaluation
Benchmarks

e.g. TruthfulQA [182], HalluQA [49], HaluEval-2.0 [168]

Hallucination Detection
Benchmarks

e.g. SelfCheckGPT-Wikibio [213], HaluEval [169], FELM [42]

Data Filtering

e.g. Abbas et al. [1], Gunasekar et al. [107], Touvron et al. [300]

Model Editing

e.g. Dai et al. [67], Huang et al. [127], Mitchell et al. [219]

Retrieval-Augmented
Generation

e.g. Gao et al. [94], Ram et al. [255], Yu et al. [358]

Mitigating Pre-trainingrelated Hallucination

e.g. Li et al. [180], Liu et al. [183, 189], Shi et al. [276]

Mitigating Misalignment
Hallucination

e.g. Rimsky [264], Sharma et al. [274], Wei et al. [327]

Factuality Enhanced
Decoding

e.g. Chuang et al. [59], Lee et al. [160], Li et al. [172]

Faithfulness Enhanced
Decoding

e.g. Chang et al. [36], Shi et al. [275], Wan et al. [309]

Hallucination Detection
Hallucination
Detection and
Benchmarks(§4)
Hallucination Benchmarks

Mitigating Data-related
Hallucinations

Hallucination
Mitigation (§5)

Mitigating Training-related
Hallucinations

Mitigating Inference-related
Hallucinations

Fig. 1. The main content flow and categorization of this survey.

2.2

Training Stages of Large Language Models

The attributes and behaviors of LLMs are deeply intertwined with their training processes. LLMs
undergo three primary training stages: pre-training, supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF). Analyzing these stages provides insight into hallucination
origins in LLMs, as each stage equips the model with specific capabilities.
2.2.1 Pre-training. Pre-training is widely acknowledged as a foundational stage for LLM to acquire
knowledge and capabilities [388]. During this phase, LLMs engage in autoregressive prediction of
subsequent tokens within sequences. Through self-supervised training on extensive textual corpora,
LLMs acquire knowledge of language syntax, world knowledge, and reasoning abilities, thereby
laying a solid groundwork for further fine-tuning. Besides, recent research [72, 291] suggests that
predicting subsequent words is akin to losslessly compressing significant information. The essence
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:5

of LLMs lies in predicting the probability distribution for upcoming words. Accurate predictions
indicate a profound grasp of knowledge, translating to a nuanced understanding of the world.
2.2.2 Supervised Fine-Tuning. While LLMs acquire substantial knowledge and capabilities during
the pre-training stage, it’s crucial to recognize that pre-training primarily optimizes for completion.
Consequently, pre-trained LLMs fundamentally serve as completion machines, which can lead to
a misalignment between the next-word prediction objective of LLMs and the user’s objective of
obtaining desired responses. To bridge this gap, SFT [370] has been introduced, which involves
further training LLMs using a meticulously annotated set of (instruction, response) pairs, resulting
in enhanced capabilities and improved controllability of LLMs. Furthermore, recent studies [60, 129]
have confirmed the effectiveness of supervised fine-tuning to achieve exceptional performance on
unseen tasks, showcasing their remarkable generalization abilities.
2.2.3 Reinforcement Learning from Human Feedback. While the SFT process successfully enables
LLMs to follow user instructions, there is still room for them to better align with human preferences.
Among various methods that utilize human feedback, RLHF stands out as an representative solution
for aligning with human preferences through reinforcement learning [55, 233, 285]. Typically, RLHF
employs a preference model [26] trained to predict preference rankings given a prompt alongside a
pair of human-labeled responses. To align with human preferences, RLHF optimizes the LLM to
generate outputs that maximize the reward provided by the trained preference model, typically
employing a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO) [270].
Such integration of human feedback into the training loop has proven effective in enhancing the
alignment of LLMs, guiding them toward producing high-quality and harmless responses.
2.3

Hallucinations in Large Language Models

The concept of hallucination traces its roots to the fields of pathology and psychology and is
defined as the perception of an entity or event that is absent in reality [202]. Within the realm
of NLP, hallucination is typically referred to as a phenomenon in which the generated content
appears nonsensical or unfaithful to the provided source content [89, 208]. This concept bears a
loose resemblance to the phenomenon of hallucination observed in human psychology. Generally,
hallucinations in natural language generation tasks can be categorized into two primary types:
intrinsic hallucination and extrinsic hallucination [126, 136, 174].
Specifically, intrinsic hallucinations pertain to the model outputs that directly conflict with the
provided source context. On the other hand, extrinsic hallucinations involve outputs that cannot be
verified using the provided source context or external knowledge bases. This means the generated
text is neither supported by nor directly contradicts the available information, rendering the output
unverifiable and potentially misleading.
However, in the era of LLMs, the versatile capabilities of these models have facilitated their
widespread use across diverse fields, highlighting limitations in existing task-specific categorization
paradigms. Considering that LLMs place a significant emphasis on user-centric interactions and prioritize alignment with user directives, coupled with the fact that their hallucinations predominantly
surface at factual levels, we introduce a more granular taxonomy building upon the foundational
work by Ji et al. [136]. This refined taxonomy seeks to encapsulate the distinct intricacies associated with LLM hallucinations. To provide a more intuitive illustration of our definition of LLM
hallucination, we present examples for each type of hallucination in Table 1, namely factuality
hallucination and faithfulness hallucination.
2.3.1 Factuality Hallucination. The emergence of LLMs marks a significant shift from traditional
task-specific toolkits to AI assistants that have a heightened focus on open-domain interactions.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:6

Huang, et al.

Table 1. Examples of each category of LLM hallucinations. Content marked in Red represents the hallucinatory
output, while content marked in Blue indicates user instruction or provided context that contradicts the LLM
hallucination.
Category

Type

Example Dialog

.

Explanation

Instruction: What are Thomas Edison’s main contributions The response is factually incorrect. In reality, Edison
to science and technology?
Factual
improved the light bulb, building on earlier desiContradiction Response: Thomas Edison developed the first practical tele- gns by others, and Alexander Graham Bell invented the telephone.
phone and invented the light bulb.
Factuality
Hallucination
Factual
Fabrication

The response erroneously states that the construction
Instruction: What are the major environmental impacts of of the Eiffel Tower led to the extinction of the ‘Parisian
the construction of the Eiffel Tower?
tiger’ and sparked the global green architecture moveResponse: The construction of the Eiffel Tower in 1889 led ment. In fact, there is no evidence of a ‘Parisian tigto the extinction of the Parisian tiger, a species that played a er’ ever existing, making this a fabricated claim.
crucial role in the region’s ecosystem. Additionally, it is wid- Moreover, attributing the origins of green architecture to the Eiffel Tower is an exaggeration, as this
ely recognized as the event that sparked the global green
movement has diverse roots and cannot be traced back
architecture movement.
to a single event.

Instruction: Translate the English question into Spanish:
Instruction "What is the capital of France?"
Inconsistency
Response: The capital of France is Paris.

The user explicitly provided an instruction to translate
the English question into Spanish. Instead of performing the translation as instructed, the LLM chose to
answer the question.

Retrieved context: The Nile is a major north-flowing river
in northeastern Africa. It’s traditionally considered the
Faithfulness
longest river in the world. Originating in the Great Lakes
The summary incorrectly states that the Nile originates
Hallucination
region of central Africa, it flows through several countries
Context
from the "mountain ranges of central Africa", while the
before emptying into the Mediterranean Sea.
Inconsistency
paragraph mentions that the Nile originates in the
Instruction: Summarize the retrieval document above.
Great Lakes region of central Africa.
Response: The Nile originates from the mountain ranges
of central Africa and flows into the Mediterranean.
Instruction: Solve the equation 2x + 3 = 11 step by step.
Logical
Response: Step 1: Subtract 3 from both sides to get: 2x = 8.
Inconsistency Step 2: Divide both sides by 2 to get: x = 3. So the result
of the equation is x=3.

The first step correctly isolates the term with x to get
2x = 8. However, the next step inaccurately divides
8 by 2 to yield a result of x = 3, which is inconsistent
with the earlier reasoning.

This shift is primarily attributed to their vast parametric factual knowledge. However, existing LLMs
occasionally exhibit tendencies to produce outputs that are either inconsistent with real-world
facts or unverifiable [168], posing challenges to the trustworthiness of artificial intelligence. In this
context, we categorize these factuality hallucinations into two primary types:
Factual Contradiction refers to situations where the LLM’s output contains facts that can be
grounded in real-world information, but present contradictions. This type of hallucination occurs
most frequently and arises from diverse sources, encompassing the LLM’s capture, storage, and
expression of factual knowledge. Depending on the error type of contradictions, it can be further
divided into two subcategories: entity-error hallucination and relation-error hallucination.
• Entity-error hallucination refers to the situations where the generated text of LLMs
contains erroneous entities. As shown in Table 1, when asked about "the inventor of the
telephone", the model erroneously states "Thomas Edison", conflicting with the real fact that it
was "Alexander Graham Bell".
• Relation-error hallucination refers to instances where the generated text of LLMs contains
wrong relations between entities. As shown in Table 1, when inquired about "the inventor of
the light bulb", the model incorrectly claims "Thomas Edison", despite the fact that he improved
upon existing designs and did not invent it.
Factual Fabrication refers to instances where the LLM’s output contains facts that are unverifiable against established real-world knowledge. This can be further divided into unverifiability
hallucination and overclaim hallucination.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:7

• Unverifiability hallucination pertains to statements that are entirely non-existent or
cannot be verified using available sources. As shown in Table 1, when asked about "the major
environmental impacts of the construction of the Eiffel Tower", the model incorrectly states that
"the construction led to the extinction of the Parisian tiger", a species that does not exist and
thus, this claim cannot be substantiated by any historical or biological record.
• Overclaim hallucination involves claims that lack universal validity due to subjective biases.
As shown in Table 1, the model claims that "the Eiffel Tower’s construction is widely recognized
as the event that sparked the global green architecture movement." This is an overclaim, as
there is no broad consensus or substantial evidence to support the statement.
2.3.2 Faithfulness Hallucination. LLMs are inherently trained to align with user instructions. As
the use of LLMs shifts towards more user-centric applications, ensuring their consistency with
user-provided instructions and contextual information becomes increasingly vital. Furthermore,
LLM’s faithfulness is also reflected in the logical consistency of its generated content. From this
perspective, we categorize three subtypes of faithfulness hallucinations:
Instruction inconsistency refers to the LLM’s outputs that deviate from a user’s directive.
While some deviations might serve safety guidelines, the inconsistencies here signify unintentional
misalignment with non-malicious user instructions. As described in Table 1, the user’s actual
intention is translation, However, the LLM erroneously deviated from the user’s instruction and
performed a question-answering task instead.
Context inconsistency points to instances where the LLM’s output is unfaithful with the user’s
provided contextual information. For example, as shown in Table 1, the user mentioned the Nile’s
source being in the Great Lakes region of central Africa, yet the LLM’s response contradicted the
context.
Logical inconsistency underscores when LLM outputs exhibit internal logical contradictions,
often observed in reasoning tasks. This manifests as inconsistency both among the reasoning steps
themselves and between the steps and the final answer. For example, as shown in Table 1, while
the reasoning step of dividing both sides of the equation by 2 is correct, the final answer of x=4 is
inconsistent with the reasoning chain, leading to an incorrect result.
3

HALLUCINATION CAUSES

LLM hallucinations have multifaceted origins, spanning the entire spectrum of LLMs’ capability
acquisition process. In this section, we delve into the root causes of hallucinations in LLMs, primarily
categorized into three key aspects: (1) Data (§3.1), (2) Training (§3.2), and (3) Inference (§3.3).
3.1

Hallucination from Data

Data for training LLMs are comprised of two primary components: (1) pre-training data, through
which LLMs acquire their general capabilities and factual knowledge [388], and (2) alignment
data, which teach LLMs to follow user instructions and align with human preferences [322].
Although these data constantly expand the capability boundaries of LLMs, they inadvertently
become the principal contributors to LLM hallucinations. This primarily manifests in three aspects:
the presence of misinformation and biases in the flawed pre-training data sources (§3.1.1), the
knowledge boundary inherently bounded by the scope of the pre-training data (§3.1.2), and the
hallucinations induced by inferior alignment data (§3.1.3).
3.1.1 Misinformation and biases. Neural networks possess an intrinsic tendency to memorize
training data [35], and this memorization tendency grows with model size [34, 54]. In general, the
inherent memorization capability is a double-edged sword in the fight against hallucinations. On
the one hand, the capacities of LLMs to memorize suggests their potential to capture profound
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:8

Huang, et al.

Table 2. Examples of Hallucinations from Misinformation and Biases. The table categorizes hallucinations
arising from flawed data sources into imitative falsehoods, duplication bias, and social biases. Each category
is accompanied by a premise outlining the data issue, user input, and the LLM’s hallucinatory output, and an
explanation for the occurrence, aiding comprehension of these complex phenomena.
Type

Premise

Imitative
Falsehood

Within the LLM’s
training data, a frequently referenced
but incorrect fact
appears
multiple
times: Thomas Edison
invented the light bulb.

Who is credited with the
invention of the light bulb?

In the model’s training data, certain
names may frequently
be associated with
specific nationalities.

Provide a summary of the
following news: Dr. Kim recently published a breakthrough paper on quantum
computing. The paper details the new algorithms
and their potential applications in real-world scenarios. It has been hailed as
revolutionary by experts.

Societal
Biases

Input

Model Ouput

Explanation

Thomas Edison is credited with the invention
of the light bulb.

While Edison improved the light
bulb’s design, he was
not its sole inventor.
The model’s answer
reflects a misconception
prevalent in its training
data, exemplifying an
imitative falsehood.

Dr. Kim from South Korea recently published
a paper on quantum
computing, detailing
new algorithms and
their applications.

The model erroneously added "South
Korea" based on the
name "Kim" revealing
its bias in linking certain names to specific
nationalities, which can
cause hallucinations in
the summary.

world knowledge. On the other hand, it becomes problematic in the context of misinformation and
biases present within pre-training data and may inadvertently be amplified, manifesting as imitative
falsehood [182] and the reinforcement of societal biases. For a more comprehensive understanding,
detailed examples are presented in Table 2.
Imitative Falsehood. Misinformation such as fake news and unfounded rumors has been widely
spread among social media platforms and gradually serves as a significant contributor to LLM
hallucinations. The increasing demand for large-scale corpora for pre-training necessitates the
employment of heuristic data collection methods. While facilitating the acquisition of extensive
data, challenges arise in maintaining consistent data quality, which inevitably introduces such
misinformation in pre-training data [20, 328]. In this situation, LLMs will increase the likelihood
of generating such false statements due to their remarkable memorization capabilities, leading to
imitative falsehoods. The issue is further exacerbated by the fact that LLMs have drastically lowered
the barriers to content creation, posing considerable risks to the trustworthiness of public discourse
and internet ecosystems.
Societal Biases. In addition to misinformation, biases are also deeply rooted in social media
platforms, showing up variously, like biased hiring, prejudiced news, and hate-spewing. As a purely
negative phenomenon, biases and hallucinations have obvious differences, mainly manifested
as stereotypes spreading and social inequalities. However, certain biases are intrinsically tied to
hallucinations, especially those related to gender [242] and nationality [155, 226]. For instance,
LLMs might associate the profession of nursing with females, even when gender isn’t explicitly
mentioned in the user-provided context, exemplifying context inconsistency as discussed in Section
(§2.3). Such biases can be inadvertently acquired from internet-based texts, which are rife with
diverse and biased viewpoints, and subsequently be propagated into the generated content [155].
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:9

Table 3. Example of Knowledge Boundary. Content marked in Red represents the hallucinatory output.
Type

Long-tail Knowledge

Up-to-date Knowledge

Copyright-sensitive
Knowledge

Input

Model Ouput

Explanation

What causes phenylketonuria?

Phenylketonuria is caused by
a mutation in the hemoglobin
gene, which results in an
unusual interaction with
hemoglobin, affecting its
ability to transport oxygen in
the bloodstream.

The correct cause of phenylketonuria is mutations in the PAH
gene. Such specialized medical
information is less frequently
encountered in the training
corpus, making LLMs struggle
to memorize.

Who will be the Democratic Party candidate
for the 2024 US presidential election?

Michelle Obama has announced her candidacy and
is leading in the polls, with
significant endorsements from
key Democratic figures such
as Barack Obama and Bernie
Sanders. She has also outlined
a policy platform focusing on
healthcare reform and climate
change initiatives.

After President Joe Biden announced his withdrawal from the
race on July 21, 2024, he endorsed
Kamala Harris as the Democratic
Party candidate. Such hallucinations arise because LLMs’ parametric knowledge cannot be
updated in real-time, leading
to potential fabrication regarding current events.

Describe the ending of
the novel "The Secrets
of the Alchemist" that
was published last year.

In the dramatic conclusion of
"The Secrets of the Alchemist",
the protagonist discovers the
true secret behind the philosopher’s stone, leading to a peaceful resolution with all characters sharing in the wisdom.

The novel "The Secrets of the
Alchemist" is under copyright
protection, and LLMs have not
been trained directly on such
copyrighted materials. Thus,
the model’s output fabricates details about the book’s ending.

3.1.2 Knowledge Boundary. While the vast pre-training corpora have empowered LLMs with
extensive factual knowledge, they inherently possess knowledge boundaries. These boundaries arise
primarily from two sources: (1) the inability of LLMs to memorize all factual knowledge encountered
during pre-training, especially the less frequent long-tail knowledge; and (2) the intrinsic boundary
of the pre-training data itself, which does not include rapidly evolving world knowledge or content
restricted by copyright laws. Consequently, when LLMs encounter information that falls outside
their limited knowledge boundaries, they are more susceptible to generating hallucinations. We
present detailed examples for clear illustration in Table 3.
Long-tail Knowledge. The distribution of knowledge within the pre-training corpora is inherently non-uniform, which results in LLMs demonstrating varying levels of proficiency across
different types of knowledge. Recent studies have highlighted a strong correlation between the
model’s accuracy on general domain questions and the volume of relevant documents [145] or
entity popularity [204] within the pre-training corpora. Furthermore, given that LLMs are predominantly trained on extensive general domain corpora [93, 243, 254], they may exhibit deficits
in domain-specific knowledge. This limitation becomes particularly evident when LLMs are confronted with tasks that require domain-specific expertise, such as medical [179, 279] and legal
[149, 353] questions, these models may exhibit pronounced hallucinations, often manifesting as
factual fabrication.
Up-to-date Knowledge. Beyond the shortfall in long-tail knowledge, another intrinsic limitation
concerning the knowledge boundaries within LLMs is their constrained capacity for up-to-date
knowledge. The factual knowledge embedded within LLMs exhibits clear temporal boundaries
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:10

Huang, et al.

and can become outdated over time [148, 166, 230]. Once these models are trained, their internal
knowledge is never updated. This poses a challenge given the dynamic and ever-evolving nature of
our world. When confronted with queries that transcend their temporal scope, LLMs often resort
to fabricating facts or providing answers that might have been correct in the past but are now
outdated.
Copyright-sensitive Knowledge. Due to licensing restrictions [262], existing LLMs are legally
constrained to training on corpora that are publicly licensed [63, 93] or otherwise available for use
without infringing copyright laws [10, 115]. This limitation significantly impacts the breadth and
diversity of knowledge that LLMs can legally acquire. A significant portion of valuable knowledge,
encapsulated in copyrighted materials such as recent scientific research, proprietary data, and
copyrighted literary works, remains inaccessible to LLMs. This exclusion creates a knowledge gap,
leading to potential hallucinations when LLMs attempt to generate information in domains where
their training data is inaccessible [215].
3.1.3 Inferior Alignment Data. After the pre-training stage, LLMs have embedded substantial
factual knowledge within their parameters, thereby establishing obvious knowledge boundaries.
During the supervised fine-tuning (SFT) stage, LLMs are typically trained on instruction pairs
labeled by human annotators, potentially introducing new factual knowledge that extends beyond
the knowledge boundary established during pre-training. Gekhman et al. [98] analyzed the training
dynamics of incorporating new factual knowledge during the SFT process and found that LLMs
struggle to acquire such new knowledge effectively. Most importantly, they discovered a correlation
between the acquisition of new knowledge through SFT and increased hallucinations, suggesting
that introducing new factual knowledge encourages LLMs to hallucinate. Additionally, Li et al. [168]
conducted extensive analysis on the effect of instructions in producing hallucinations. Findings
indicated that task-specific instructions which primarily focus on task format learning, tend to yield
a higher proportion of hallucinatory responses. Moreover, overly complex and diverse instructions
also lead to increased hallucinations.
3.2

Hallucination from Training

As detailed in Section 2.2, the distinct stages of training impart various capabilities to LLMs,
with pre-training focusing on acquiring general-purpose representations and world knowledge,
and alignment enables LLMs to better align with user instructions and preferences. While these
stages are critical for equipping LLMs with remarkable capabilities, shortfalls in either stage can
inadvertently pave the way for hallucinations.
3.2.1 Hallucination from Pre-training. Pre-training constitutes the foundational stage for LLMs,
predominantly utilizing a transformer-based architecture following the paradigm established by
GPT [29, 251, 252], and further developed by OPT[372], Falcon [243], and Llama-2 [300]. This
stage employs a causal language modeling objective, where models learn to predict subsequent
tokens solely based on preceding ones in a unidirectional, left-to-right manner. While facilitating
efficient training, it inherently limits the ability to capture intricate contextual dependencies,
potentially increasing risks for the emergence of hallucination [180]. Moreover, recent research
has exposed that LLMs can occasionally exhibit unpredictable reasoning hallucinations spanning
both long-range and short-range dependencies, which potentially arise from the limitations of
soft attention [52, 111], where attention becomes diluted across positions as sequence length
increases. Notably, the phenomenon of exposure bias [21, 256] has been a longstanding and serious
contribution to hallucinations, resulting from the disparity between training and inference in the
auto-regressive generative model. Such inconsistency can result in hallucinations [313], especially
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:11

when an erroneous token generated by the model cascades errors throughout the subsequent
sequence, akin to a snowball effect [368].
3.2.2 Hallucination from Supervised Fine-tuning. LLMs have inherent capability boundaries established during pre-training. SFT seeks to utilize instruction data and corresponding responses
to unlock these pre-acquired abilities. However, challenges arise when the demands of annotated
instructions exceed the model’s pre-defined capability boundaries. In such cases, LLMs are trained
to fit responses beyond their actual knowledge boundaries. As discussed in §3.1.3, over-fitting
on new factual knowledge encourages LLMs prone to fabricating content, amplifying the risk of
hallucinations [98, 269]. Moreover, another significant reason lies in the models’ inability to reject.
Traditional SFT methods typically force models to complete each response, without allowing them
to accurately express uncertainty [341, 362]. Consequently, when faced with queries that exceed
their knowledge boundaries, these models are more likely to fabricate content rather than reject it.
This misalignment of knowledge boundaries, coupled with the inability to express uncertainty, are
critical factors that contribute to the occurrence of hallucinations during the SFT stage.
3.2.3 Hallucination from RLHF. Several studies [13, 31] have demonstrated that LLM’s activations
encapsulate an internal belief related to the truthfulness of its generated statements. Nevertheless,
misalignment can occasionally arise between these internal beliefs and the generated outputs. Even
when LLMs are refined with human feedback [233], they can sometimes produce outputs that
diverge from their internal beliefs. Such behaviors, termed as sycophancy [64], underscore the
model’s inclination to appease human evaluators, often at the cost of truthfulness. Recent studies
indicate that models trained via RLHF exhibit pronounced behaviors of pandering to user opinions.
Such sycophantic behaviors are not restricted to ambiguous questions without definitive answers
[245], like political stances, but can also arise when the model chooses a clearly incorrect answer,
despite being aware of its inaccuracy [327]. Delving into this phenomenon, Sharma et al. [274]
suggested that the root of sycophancy may lie in the training process of RLHF models. By further
exploring the role of human preferences in this behavior, the research indicates that the tendency
for sycophancy is likely driven by both humans and preference models showing a bias towards
sycophantic responses over truthful ones.
3.3

Hallucination from Inference

Decoding plays an important role in manifesting the capabilities of LLMs after pretraining and
alignment. However, certain shortcomings in decoding strategies can lead to LLM hallucinations.
3.3.1 Imperfect Decoding Strategies. LLMs have demonstrated a remarkable aptitude for generating
highly creative and diverse content, a proficiency that is critically dependent on the pivotal role of
randomness in their decoding strategies. Stochastic sampling [84, 118] is currently the prevailing
decoding strategy employed by these LLMs. The rationale for incorporating randomness into
decoding strategies stems from the realization that high likelihood sequences often result in
surprisingly low-quality text, which is called likelihood trap [118, 209, 283, 363]. The diversity
introduced by the randomness in decoding strategies comes at a cost, as it is positively correlated
with an increased risk of hallucinations [59, 78]. An elevation in the sampling temperature results
in a more uniform token probability distribution, increasing the likelihood of sampling tokens
with lower frequencies from the tail of the distribution. Consequently, this heightened tendency to
sample infrequently occurring tokens exacerbates the risk of hallucinations [5].
3.3.2 Over-confidence. Prior studies in conditional text generation [45, 212] have highlighted the
issue of over-confidence which stems from an excessive focus on the partially generated content,
often prioritizing fluency at the expense of faithfully adhering to the source context. While LLMs,
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:12

Huang, et al.

primarily adopting the causal language model architecture, have gained widespread usage, the
over-confidence phenomenon continues to persist. During the generation process, the prediction
of the next word is conditioned on both the language model context and the partially generated
text. However, as demonstrated in prior studies [19, 189, 307], language models often exhibit a
localized focus within their attention mechanisms, giving priority to nearby words and resulting in
a notable deficit in context attention [275]. Furthermore, this concern is further amplified in LLMs
that exhibit a proclivity for generating lengthy and comprehensive responses. In such cases, there
is even a heightened susceptibility to the risk of instruction forgetting [46, 193]. This insufficient
attention can directly contribute to faithfulness hallucinations, wherein the model outputs content
that deviates from the original context.
3.3.3 Softmax Bottleneck. The majority of language models utilize a softmax layer that operates on
the final layer’s representation within the language model, in conjunction with a word embedding,
to compute the ultimate probability associated with word prediction. Nevertheless, the efficacy
of Softmax-based language models is impeded by a recognized limitation known as the Softmax
bottleneck [342], wherein the employment of softmax in tandem with distributed word embeddings
constrains the expressivity of the output probability distributions given the context which prevents
LMs from outputting the desired distribution. Additionally, Chang and McCallum [38] discovered
that when the desired distribution within the output word embedding space exhibits multiple
modes, language models face challenges in accurately prioritizing words from all the modes as the
top next words, which also introduces the risk of hallucination.
3.3.4 Reasoning Failure. Beyond the challenges with long-tail knowledge, effective utilization of
knowledge is inextricably linked with reasoning capabilities. For instance, in multi-hop questionanswering scenarios, even if the LLM possesses the necessary knowledge, it may struggle to produce
accurate results if multiple associations exist between questions, due to its limitations in reasoning
[386]. Furthermore, Berglund et al. [22] unveiled a specific reasoning failure in LLMs termed the
Reversal Curse. Specifically, while the model can correctly answer when the question is formulated
as "A is B", it exhibits a failed logical deduction when asked the converse "B is A". This discrepancy
in reasoning extends beyond simple deductions.
4

HALLUCINATION DETECTION AND BENCHMARKS

The issue of hallucinations within LLMs has garnered considerable attention, raising concerns
about the reliability of LLMs and their deployment in practical applications. As LLMs become
increasingly adept at generating human-like text, accurately distinguishing between hallucinated
versus factual content becomes increasingly vital. Moreover, effectively measuring the level of
hallucination in LLM is crucial for improving their reliability. Thus, in this section, we delve into
hallucination detection approaches (§4.1) and benchmarks for assessing LLM hallucinations (§4.2).
4.1

Hallucination Detection

Existing strategies for detecting hallucinations in LLMs can be categorized based on the type of
hallucination: (1) factuality hallucination detection, which aims to identify factual inaccuracies in
the model’s outputs, and (2) faithfulness hallucination detection, which focuses on evaluating the
faithfulness of model’s outputs to the contextual information provided.
4.1.1 Factuality Hallucination Detection. Factuality hallucination detection involves assessing
whether the output of LLMs aligns with real-world facts. Typical methods generally fall into
two categories: fact-checking, which involves verifying the factuality of the generated response
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:13

against trusted knowledge sources, and uncertainty estimation, which focuses on detecting factual
inconsistency via internal uncertainty signals.
Fact-checking. Given that the output of LLMs is typically comprehensive and consists of
multiple factual statements, the fact-checking approach is generally divided into two primary steps:
(1) fact extraction, which involves extracting independent factual statements within the model’s
outputs (2) fact verification, which aims at verifying the correctness of these factual statements
against trusted knowledge sources. Depending on the type of knowledge sources employed for
verification, fact-checking methodologies can be broadly categorized into two distinct parts: external
retrieval and internal checking.
• External retrieval: The most intuitive strategy for fact verification is external retrieval. Min et al.
[216] developed FACTSCORE, a fine-grained factual metric tailored for evaluating long-form
text generation. It first decomposes the generation content into atomic facts and subsequently
computes the percentage supported by reliable knowledge sources. Expanding on this concept,
Chern et al. [50] proposed a unified framework that equips LLMs with the capability to identify
factual inaccuracies by utilizing a collection of external tools dedicated to evidence gathering.
In addition to retrieving supporting evidence solely based on decomposited claims, Huo et al.
[128] improved the retrieval process through query expansion. By combining the original
question with the LLM-generated answer, they effectively addressed the issue of topic drift,
ensuring that the retrieved evidence aligns with both the question and the LLM’s response.
• Internal checking: Given the extensive factual knowledge encoded in their parameters, LLMs
have been explored as factual knowledge sources for fact-checking. Dhuliawala et al. [74]
introduced the Chain-of-Verification (CoVe), where an LLM first generates verification questions for a draft response and subsequently leverages its parametric knowledge to assess
the consistency of the answer against the original response, thereby detecting potential
inconsistencies.Kadavath et al. [143] and Zhang et al. [375] calculates the probability 𝑝 (𝑇 𝑟𝑢𝑒)
to assess the factuality of the response to a boolean question, relying exclusively on the
model’sinternal knowledge. Additionally, Li et al. [168] observed that most atomic statements
are interrelated, some may serve as contextual backgrounds for others, which potentially
leads to incorrect judgments. Thus, they instruct the LLM to directly predict hallucination
judgments considering all factual statements. However, as LLMs are not inherently reliable
factual databases [385], solely relying on LLMs’ parametric knowledge for fact-checking may
result in inaccurate assessments.
Uncertainty Estimation. While many approaches to hallucination detection rely on external
knowledge sources for fact-checking, several methods have been devised to address this issue in
zero-resource settings, thus eliminating the need for retrieval. The foundational premise behind
these strategies is that the origin of LLM hallucinations is inherently tied to the model’s uncertainty.
Therefore, by estimating the uncertainty of the factual content generated by the model, it becomes
feasible to detect hallucinations. The methodologies in uncertainty estimation can broadly be
categorized into two approaches: based on LLM internal states and LLM behavior, as shown in Fig. 2.
• LLM internal states: The internal states of LLMs can serve as informative indicators of their
uncertainty, often manifested through metrics like token probability or entropy. Varshney et al.
[306] determined the model’s uncertainty towards key concepts quantified by considering
the minimal token probability within those concepts. The underlying rationale is that a
low probability serves as a strong indicator of the model’s uncertainty, with less influence
from higher probability tokens present in the concept. Similarly, Luo et al. [198] employed
a self-evaluation-based approach for uncertainty estimation by grounding in the rationale
that a language model’s ability to adeptly reconstruct an original concept from its generated
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:14

Huang, et al.

Question: What is the highest peak in the world?
The highest peak in the world is Mount Fuji.

Large Language Model
low uncertainty

high uncertainty

(a) LLM Internal States

Mount Everest stands as the tallest
peak in the world.
As far as I know, the highest peak
in the world is Mount Fuji in Japan.
The highest peak is Mount Everest
located in the Himalayas.
(1) Self-Consistency

Consistency

The highest peak in the world
is Mount Fuji.
I must correct you. Mount Fuji is the highest
peak in Japan. The highest peak in the world
is Mount Everest in the Himalayas range.
I stand corrected, you are right.

(b) LLM Behavior

(2) Multi-Debate

Fig. 2. Taxonomy of Uncertainty Estimation Methods in Factual Hallucination Detection, featuring a) LLM Internal States and b) LLM Behavior, with LLM Behavior encompassing two main categories: Self-Consistency
and Multi-Debate.

explanation is indicative of its proficiency with that concept. By initially prompting the model
to generate an explanation for a given concept and then employing constrained decoding
to have the model recreate the original concept based on its generated explanation, the
probability score from the response sequence can serve as a familiarity score for the concept.
Furthermore, Yao et al. [345] interpreted hallucination through the lens of adversarial attacks.
Utilizing gradient-based token replacement, they devised prompts to induce hallucinations.
Notably, they observed that the first token generated from a raw prompt typically exhibits
low entropy, compared to those from adversarial attacks. Based on this observation, they
proposed setting an entropy threshold to define such hallucination attacks.
• LLM behavior: However, when systems are only accessible via API calls [100, 214, 231],
access to the output’s token-level probability distribution might be unavailable. Given this
constraint, several studies have shifted their focus to probing a model’s uncertainty, either
through natural language prompts [143, 335] or by examining its behavioral manifestations.
For instance, by sampling multiple responses from an LLM for the same prompt, Manakul et al.
[205] detected hallucinations via evaluating the consistency among the factual statements.
However, these methods predominantly rely on direct queries that explicitly solicit information or verification from the model. Agrawal et al. [3], inspired by investigative interviews,
advocated for the use of indirect queries. Unlike direct ones, these indirect counterparts often
pose open-ended questions to elicit specific information. By employing these indirect queries,
consistency across multiple model generations can be better evaluated. Beyond assessing
uncertainty from the self-consistency of a single LLM’s multiple generations, one can embrace
a multi-agent perspective by incorporating additional LLMs. Drawing inspiration from legal
cross-examination practices, Cohen et al. [62] introduced the LMvLM approach. This strategy
leverages an examiner LM to question an examinee LM, aiming to unveil inconsistencies of
claims during multi-turn interaction.
4.1.2 Faithfulness Hallucination Detection. Ensuring the faithfulness of LLMs to provide context or
user instructions is pivotal for their practical utility in IR applications, from conversational search
to interactive dialogue systems. We categorize existing hallucination detection metrics tailored
to faithfulness into the following groups, with an overview shown in Fig. 3: (1) Fact-based (2)
Classifier-based (3) QA-based (4) Uncertainty-based (5) LLM-based.
Fact-based Metrics. In the realm of assessing faithfulness, one of the most intuitive methods
involves measuring the overlap of pivotal facts between the generated content and the source
content. Given the diverse manifestations of facts, faithfulness can be measured based on n-gram,
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

Information
Extraction

User Query

Fact Overlap
Facts

low uncertainty

Large Language Model

(a) Fact-based Metric

User Query
NLI Model

high uncertainty
SOE

LLM Generation

Information
Extraction

Facts

1:15

Entailment

User Query

LLM Generation
(b) Classifier-based Metric

User Query

Question
Answering

Answers

Questions

LLM Generation

Question
Generation
Answer
Selection
(c) QA-based Metric

(d) Uncertainty Estimation

Answer
Overlap
Answers

User Query
LLM Generation

binary
judgement
k-point
Likert scale

(e) Prompting-based Metric

Fig. 3. The illustration of detection methods for faithfulness hallucinations: a) Fact-based Metrics, which
assesses faithfulness by measuring the overlap of facts between the generated content and the source
content; b) Classifier-based Metrics, utilizing trained classifiers to distinguish the level of entailment
between the generated content and the source content; c) QA-based Metrics, employing question-answering
systems to validate the consistency of information between the source content and the generated content; d)
Uncertainty Estimation, which assesses faithfulness by measuring the model’s confidence in its generated
outputs; e) Prompting-based Metrics, wherein LLMs are induced to serve as evaluators, assessing the
faithfulness of generated content through specific prompting strategies.

entities, and relation triples. Traditional n-gram-based metrics, such as BLEU [239], ROUGE [181]
and PARENT-T [324], typically fall short in differentiating the nuanced discrepancies between the
generated content and the source content [208]. Entity-based metrics [225] make a step further by
calculating the overlap of entities, as any omission or inaccurate generation of these key entities
could lead to an unfaithful response. Notably, even if entities match, the relations between them
might be erroneous. Thus, relation-based metrics [99] focus on the overlap of relation tuples and
introduce a metric that computes the overlap of relation tuples extracted using trained end-to-end
fact extraction models.
Classifier-based Metrics. Beyond computing fact overlap, another straightforward approach
to assessing the faithfulness of the model generation involves utilizing classifiers trained on data
from related tasks such as natural language inference (NLI) and fact-checking, or data comprised of
synthetically task-specific hallucinated and faithful content. A foundational principle for assessing
the faithfulness of generated text is anchored on the idea that genuinely faithful content should
inherently be entailed by its source content. In line with this, numerous studies [82, 208] have
trained classifiers on NLI datasets to identify factual inaccuracies, especially in the context of
abstract summarization. However, Mishra et al. [217] highlighted that the mismatch in input
granularity between conventional NLI datasets and inconsistency detection datasets limits their
applicability for effectively detecting inconsistencies. Building on this, more advanced studies have
proposed methods such as fine-tuning on adversarial datasets [17], decomposing the entailment
decisions at the dependency arc level [101], and segmenting documents into sentence units then
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:16

Huang, et al.

aggregating scores between sentence pairs [154]. While using data from related tasks to finetune the classifier has shown promise in evaluating faithfulness, it’s essential to recognize the
inherent gap between related tasks and the downstream task. The scarcity of annotated data further
constrains their applicability. In response to this challenge, a surge of research explores leveraging
data-augmentation methods to construct synthetical data for fine-tuning the classifier, either by
rule-based perturbation [79, 152, 266] or generation [389].
QA-based Metrics. In contrast to classifier-based metrics, QA-based metrics [77, 119, 271, 310]
have recently garnered attention for their enhanced ability to capture information overlap between
the model’s generation and its source. These metrics operate by initially selecting target answers
from the information units within the LLM’s output, and then questions are generated by the
question-generation module. The questions are subsequently used to generate source answers based
on the user context. Finally, the faithfulness of the LLM’s responses is calculated by comparing
the matching scores between the source and target answers. Although these methodologies share
a common thematic approach, they exhibit variability in aspects like answer selection, question
generation, and answer overlap, leading to diverse performance outcomes. Building on this foundational work, Fabbri et al. [80] conducted an in-depth evaluation of the components within QA-based
metrics, yielding further enhancements in faithfulness evaluation.
Uncertainty-based Metrics. Drawing parallels with the uncertainty-based approaches employed for detecting factuality hallucinations (§4.1.1), the application of uncertainty estimation
in assessing faithfulness has been widely explored, typically characterized by entropy and logprobability. For entropy-based uncertainty, Xiao and Wang [333] has revealed a positive correlation
between hallucination likelihood in data-to-text generation and predictive uncertainty, which is
estimated by deep ensembles [156]. In a related vein, Guerreiro et al. [106] leveraged the variance
in hypotheses yielded by Monte Carlo Dropout [92] as an uncertainty measure within neural
machine translation. More recently, van der Poel et al. [305] employed conditional entropy [337]
to assess model uncertainty in abstractive summarization. Regarding log-probability, it can be
applied at different levels of granularity, such as word or sentence level. Notably, several studies
[91, 106, 359] have adopted length-normalized sequence log-probability to measure model confidence. Furthermore, considering the hallucinated token can be assigned high probability when the
preceding context contains the same hallucinated information, Zhang et al. [374] focused on the
most informative and important keywords and introduced a penalty mechanism to counteract the
propagation of hallucinated content.
LLM-based Judgement. Recently, the remarkable instruction-following ability of LLMs has
underscored their potential for automatic evaluation [51, 190, 314]. Exploiting this capability,
researchers have ventured into novel paradigms for assessing the faithfulness of model-generated
content [2, 95, 133, 153, 199]. By providing LLMs with concrete evaluation guidelines and feeding
them both the model-generated and source content, they can effectively assess faithfulness. The
final evaluation output can either be a binary judgment on faithfulness [199] or a k-point Likert
scale indicating the degree of faithfulness [95]. For prompt selection, evaluation prompt can either
be direct prompting, chain-of-thought prompting [2], using in-context-learning [133] or allowing
the model to generate evaluation results accompanying with explanations [153].

4.2

Hallucination Benchmarks

In this section, we present a comprehensive overview of existing hallucination benchmarks, which
can be categorized into two primary domains: Hallucination Evaluation Benchmarks (§4.2.1), which
assess the extent of hallucinations generated by existing cutting-edge LLMs, and Hallucination
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:17

Detection Benchmarks (§4.2.2), designed specifically to evaluate the performance of existing hallucination detection methods. Collectively, these benchmarks establish a unified framework, enabling
a nuanced and thorough exploration of hallucinatory patterns in LLMs.
Table 4. An overview of existing hallucination benchmarks. For Attribute, Factuality and Faithfulness
represent whether the benchmark is used to evaluate LLM’s factuality or to detect faithfulness hallucination,
and Manual represents whether the inputs in the data are handwritten.
Attribute
Benchmark
TruthfulQA
[182]

Datasets

-

Data Size

817

Language

English

Factuality
✔

Faithfulness
✗

Task
Manual

Task Type

✔

Generative QA
Multi-Choice QA

Input

Label

Metric

Question

Answer

LLM-Judge &
Human

Question

Answer

Acc
EM & F1

REALTIMEQA
[148]

-

Dynamic

English

✔

✗

✔

Multi-Choice QA
Generative QA

SelfCheckGPT-Wikibio
[213]

-

1,908

English

✗

✔

✗

Detection

Paragraph &
Concept

Passage

AUROC

HaluEval
[169]

Task-specific
General

30,000
5,000

English
English

✗
✗

✔
✔

✗
✗

Detection
Detection

Query
Task Input

Response
Response

Acc
Acc

Med-HALT
[303]

-

4,916

Multilingual

✔

✗

✗

Multi-Choice QA

Question

Choice

Pointwise Score
& Acc

FACTOR
[223]

Wiki-FACTOR
News-FACTOR

2,994
1,036

English
English

✔
✔

✗
✗

✗
✗

Multi-Choice QA
Multi-Choice QA

Question
Question

Answer
Answer

likelihood
likelihood

BAMBOO
[76]

SenHallu
AbsHallu

200
200

English
English

✗
✗

✔
✔

✗
✗

Detection
Detection

Paper
Paper

Summary
Summary

P & R & F1
P & R & F1

ChineseFactEval
[311]

-

125

Chinese

✔

✗

✔

Generative QA

Question

-

Score

HaluQA
[49]

Misleading
Misleading-hard
Knowledge

175
69
206

Chinese
Chinese
Chinese

✔
✔
✔

✗
✗
✗

✔
✔
✔

Generative QA
Generative QA
Generative QA

Question
Question
Question

Answer
Answer
Answer

LLM-Judge
LLM-Judge
LLM-Judge

FreshQA
[308]

Never-changing
Slow-changing
Fast-changing
False-premise

150
150
150
150

English
English
English
English

✔
✔
✔
✔

✗
✗
✗
✗

✔
✔
✔
✔

Generative QA
Generative QA
Generative QA
Generative QA

Question
Question
Question
Question

Answer
Answer
Answer
Answer

Human
Human
Human
Human

FELM
[42]

-

3,948

English

✔

✔

✗

Detection

Question

Response

Balanced
Acc & F1

PHD
[340]

PHD-LOW
PHD-Meidum
PHD-High

100
100
100

English
English
English

✗
✗
✗

✔
✔
✔

✗
✗
✗

Detection
Detection
Detection

Entity
Entity
Entity

Response
Response
Response

P & R & F1
P & R & F1
P & R & F1

-

52

English

✗

✔

✗

Detection

Document

Summary

AUROC

COVID-QA
DROP
Open Assistant
TriviaQA

N/A
N/A
N/A
N/A

English
English
English
English

✗
✗
✗
✗

✔
✔
✔
✔

✗
✗
✗
✗

Detection
Detection
Detection
Detection

Question
Question
Question
Question

Answer
Answer
Answer
Answer

AUROC
AUROC
AUROC
AUROC

-

6,166

English

✗

✔

✗

Detection

Document

Summary

Balanced Acc

HotpotQA
NQ-Open

250
250

English
English

✗
✗

✔
✔

✗
✗

Detection
Detection

Question
Question

Answer
Answer

AUROC
AUROC

Biomedicine
Finance
Science
Education
Open domain

1,535
1,125
1,409
1,701
3,000

English
English
English
English
English

✔
✔
✔
✔
✔

✗
✗
✗
✗
✗

✗
✗
✗
✗
✗

Generative QA
Generative QA
Generative QA
Generative QA
Generative QA

Question
Question
Question
Question
Question

Answer
Answer
Answer
Answer
Answer

MiHR & MaHR
MiHR & MaHR
MiHR & MaHR
MiHR & MaHR
MiHR & MaHR

ScreenEval
[158]

RealHall
[90]
LSum
[85]
SAC3
[364]

HaluEval 2.0
[168]

4.2.1 Hallucination Evaluation Benchmarks. Hallucination evaluation benchmarks are devised
to quantify the tendency of LLMs to generate hallucinations, particularly emphasizing factual
inaccuracies and inconsistency from the given contexts. Given the adeptness of LLMs at memorizing
high-frequency count knowledge, the primary focus of current hallucination evaluation benchmarks
targets long-tailed knowledge and challenging questions that can easily elicit imitative falsehood.
As for evaluating, these benchmarks typically utilize multiple choice QA, where performance is
measured through accuracy metrics, or generative QA, evaluated either through human judgment
or scores given by proxy models.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:18

Huang, et al.

Long-tail Factual Knowledge. The selection criteria for gathering long-tail factual questionanswering samples typically include the frequency of appearance, recency, and specific domains.
Regarding the frequency of appearance, benchmarks such as PopQA [204] and Head-to-Tail [290]
are constructed based on entity popularity derived directly from Wikipedia. Considering that world
knowledge is constantly evolving, it becomes crucial to validate the LLM’s factuality concerning
the current world. Among benchmarks characterized by ever-changing, REALTIMEQA [148] and
FreshQA [308] stands out. REALTIMEQA offers real-time, open-domain multiple-choice questions
that are regularly updated to reflect the latest developments. These questions are derived from newly
published news articles, encompassing a broad spectrum of topics, including politics, business,
sports, and entertainment. Similarly, FreshQA challenges LLMs with questions designed to represent
varying degrees of temporal change—categorized into never-changing, slow-changing, and fastchanging world knowledge. This benchmark is further enriched by including questions based on
false premises, requiring debunking, thus comprising a total of 600 meticulously hand-crafted
questions. Moreover, long-tail knowledge often pertains to specific domains. For instance, MedHALT [303] is distinguished by its focus on the medical domain, challenging LLMs with multiplechoice questions derived from a variety of countries. Additionally, Malaviya et al. [203] collected
expert-curated questions across 32 fields of study, resulting in a high-quality long-form QA dataset
with 2,177 questions.
Imitative Falsehood Knowledge. Imitative falsehood knowledge is specifically designed to
challenge LLMs through adversarial prompting. This approach crafts questions in such a way
that they are prone to misleading LLMs due to false beliefs or misconceptions. The two most
representative benchmarks are TruthfulQA [182] and HalluQA [49]. TruthfulQA comprises 817
questions that span 38 diverse categories, such as health, law, finance, and politics. Crafted using an
adversarial methodology, it aims to elicit "imitative falsehoods"—misleading responses that models
might generate due to their frequent presence in training data. The benchmark is divided into two
parts, one of which contains manually curated questions that were further refined by filtering out
those correctly answered by GPT-3, resulting in 437 filtered questions. The other part includes
380 unfiltered non-adversarial questions. Drawing from the construction approach of TruthfulQA,
HalluQA is crafted to specifically assess hallucinations in Chinese LLMs, focusing on imitative
falsehoods and factual errors. The benchmark comprises 450 handcrafted adversarial questions
across 30 domains and is categorized into two parts. The misleading section captures questions that
successfully deceive GLM-130B, while the knowledge section retains questions that both ChatGPT
and Puyu consistently answer incorrectly. To comprehensively evaluate LLM hallucinations across
various domains, Li et al. [168] constructed an upgraded hallucination evaluation benchmark,
HaluEval 2.0, based on [169]. This benchmark includes 8,770 questions that LLMs are prone to
hallucination, across five domains: biomedicine, finance, science, education, and open domain.
4.2.2 Hallucination Detection Benchmarks. For hallucination detection benchmarks, most prior
studies have primarily concentrated on task-specific hallucinations, such as abstractive summarization [81, 102, 152, 208, 236, 310], data-to-text[240, 296], and machine translation [389]. However,
the content generated in these studies often originates from models with lesser capabilities, such
as BART [164] and PEGASUS [366]. As a result, they may not accurately reflect the effectiveness of
hallucination detection strategies, underlining the necessity for a significant shift toward developing
benchmarks that encapsulate more complex scenarios reflective of the era of LLMs.
For example, SelfCheckGPT-Wikibio [213] offers a sentence-level dataset created by generating
synthetic Wikipedia articles with GPT-3, manually annotated for factuality, highlighting the challenge of detecting hallucinations in the biography domain. Complementing this, HaluEval [169]
combines automated generation with human annotation to evaluate LLMs’ ability to recognize
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:19

hallucinations across 5,000 general user queries and 30,000 task-specific samples, leveraging a
"sampling-then-filtering" approach. Building upon existing research predominantly focused on
short documents, BAMBOO [76] and ScreenEval [158] extend the scope in long-form hallucination
detection. Further, FELM [42], distinguishes itself by assessing factuality across diverse domains
including world knowledge, science, and mathematics, producing 817 samples annotated for various
facets of factual accuracy, thereby addressing the need for cross-domain evaluation of factuality
in LLM-generated content. On a different note, PHD [340], shifts the focus towards passage-level
detection of non-factual content by analyzing entities from Wikipedia, thus offering a nuanced
view on the knowledge depth of LLMs. RealHall [90] and SAC3 [364] align closely with real-world
applications focusing on open-domain question-answering, whereas LSum [85] concentrating on
summarization tasks.
5

HALLUCINATION MITIGATION

In this section, we present a comprehensive review of contemporary methods aimed at mitigating
hallucinations in LLMs. Drawing from insights discussed in Hallucination Causes (§3), we systematically categorize these methods based on the underlying causes of hallucinations. Specifically, we
focus on approaches addressing Data-related Hallucinations (§5.1), Training-related Hallucinations
(§5.2) and Inference-related Hallucinations (§5.3), each offering tailored solutions to tackle specific
challenges inherent to their respective cause.
5.1

Mitigating Data-related Hallucinations

As analyzed in §3.1, data-related hallucinations generally emerge as a byproduct of misinformation,
biases, and knowledge gaps, which are fundamentally rooted in the pre-training data. Several
methods are proposed to mitigate such hallucinations, primarily categorized into three distinct
parts: (1) data filtering aiming at selecting high-quality data to avoid introducing misinformation and
biases, (2) model editing focusing on injecting up-to-date knowledge by editing model’s parameters,
and (3) retrieval-augmented generation leveraging external non-parametric database for knowledge
supplying.
5.1.1 Data Filtering. To reduce the presence of misinformation and biases, an intuitive approach
involves the careful selection of high-quality pre-training data from reliable sources. In this way, we
can ensure the factual correctness of data while also minimizing the introduction of social biases. As
early as the advent of GPT-2, Radford et al. [252] underscored the significance of exclusively scraping
web pages that had undergone rigorous curation and filtration by human experts. However, as
pre-training datasets continue to scale, manual curation becomes a challenge. Given that academic
or specialized domain data is typically factually accurate, gathering high-quality data emerges as a
primary strategy. Notable examples include the Pile [93] and “textbook-like” data sources [107, 177].
Additionally, up-sampling factual data during the pre-training phase has been proven effective in
enhancing the factual correctness of LLMs [300], thus alleviating hallucination.
In addition to strictly controlling the source of data, deduplication serves as a crucial procedure.
Existing practices typically fall into two categories: exact duplicates and near-duplicates. For exact
duplicates, the most straightforward method involves exact substring matching to identify identical
strings. However, given the vastness of pre-training data, this process can be computationally
intensive, a more efficient method utilizes the construction of a suffix array [206], enabling effective
computation of numerous substring queries in linear time. Regarding near-duplicates, the identification often involves approximate full-text matching, typically utilizing hash-based techniques to
identify document pairs with significant n-gram overlap. Furthermore, MinHash [28] stands out as
a prevalent algorithm for large-scale deduplication tasks [110]. Additionally, SemDeDup [1] makes
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:20

Huang, et al.

use of embeddings from pre-trained models to identify semantic duplicates, which refers to data
pairs with semantic similarities but not identical.
Discussion. Since data filtering works directly at the source of hallucinations, it effectively
mitigates hallucinations by ensuring the use of high-quality, factually accurate sources. Despite
its effectiveness, the efficiency and scalability of current data filtering methods pose significant
challenges as data volumes expand. Additionally, these methods often overlook the influence
of LLM-generated content, which can introduce new risks and inaccuracies. To advance, future
research must focus on developing more efficient, automated data filtering algorithms that can
keep pace with the rapid expansion of datasets and the complexities of LLM-generated content.
5.1.2 Model Editing. Model editing [280, 320, 369] has garnered rising attention from researchers,
which aims to rectify model behavior by incorporating additional knowledge. Current model editing
techniques can be categorized into two classes: locate-then-edit and meta-learning.
Locate-then-edit. Locate-then-edit methods [66, 210] consist of two stages, which first locate the
“buggy” part of the model parameters and then apply an update to them to alter the model’s behavior.
For example, ROME [210] located the edits-related layer by destroying and subsequently restoring
the activations and then updates the parameters of FFN in a direct manner to edit knowledge. MEMIT
[211] employed the same knowledge locating methods as ROME, enabling the concurrent updating
of multiple layers to facilitate the simultaneous integration of thousands of editing knowledge.
However, Yao et al. [347] found that these methods lack non-trivial generalization capabilities
and varying performance and applicability to different model architectures. The best-performing
methods ROME and MEMIT empirically only work well on decoder-only LLMs.
Meta-learning. Meta-learning methods [70, 218] train an external hyper-network to predict the
weight update of the original model. Nevertheless, meta-learning methods often require additional
training and memory cost, where MEND [218] utilized a low-rank decomposition with a specialized
design to reduce the size of hyper-networks. Notably, MEND would exhibit a cancellation effect,
where parameter shifts corresponding to different keys significantly counteract each other. MALMEN [292] further addressed this issue by framing the parameter shift aggregation as a least squares
problem rather than a simple summation, thereby greatly enhancing its capacity for extensive
editing. While these methods can fine-grainedly adjust the behavior of the model, modifications to
the parameters could have a potentially harmful impact on the inherent knowledge of the model.
Discussion. Model editing provides a precise way to mitigate hallucinations induced by specific
misinformation without extensive retraining. However, these methods struggle with large-scale
updates and can adversely affect the model’s overall performance, particularly when continuous
edits are applied. Consequently, future research should focus on improving model editing to handle
large-scale knowledge updates more efficiently and address hallucinations caused by social biases.
5.1.3 Retrieval-Augmented Generation. Typically, retrieval-augmented generation (RAG) [109, 165,
278] follows a retrieve-then-read pipeline, where relevant knowledge is firstly retrieved by a retriever
[146] from external sources, and then the final response is generated by a generator conditioning
on both user query and retrieved documents. By decoupling external knowledge from LLM, RAG
can effectively alleviate the hallucination caused by the knowledge gap without affecting the
performance of LLM. Common practices can be divided into three parts, as shown in Fig 4: one-time
retrieval, iterative retrieval, and post-hoc retrieval, depending on the timing of retrieval.
One-time Retrieval. One-time retrieval aims to directly prepend the external knowledge
obtained from a single retrieval to the LLMs’ prompt. Ram et al. [255] introduced In-context RALM,
which entails a straightforward yet effective strategy of prepending chosen documents to the
input text of LLMs. Beyond conventional knowledge repositories such as Wikipedia, ongoing
research endeavors have explored alternative avenues, specifically the utilization of knowledge
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

Query

Query

1:21

Query
Iteration

Retrieve

LLM

Large Language
Model
Output
Iteration

Generate

LLM

Large Language
Model

Revision

Answer

Revise

Output
...
Rertieve
Generate

...

Answer

Answer

(a) One-time Retrieval

(b) Iterative Retrieval

Revisior

(c) Post-hoc Retrieval

Fig. 4. The illustration of three distinct approaches for Retrieval-Augmented Generation: a) One-time
Retrieval, where relevant information is retrieved once before text generation; b) Iterative Retrieval,
involving multiple retrieval iterations during text generation for dynamic information integration; and c)
Post-hoc Retrieval, where the retrieval process happens after an answer is generated, aiming to refine and
fact-check the generated content.

graphs (KGs). These KGs serve as a pivotal tool for prompting LLMs, facilitating their interaction
with the most recent knowledge, and eliciting robust reasoning pathways [14, 249, 329]. Varshney
et al. [306] introduce the Parametric Knowledge Guiding (PKG) framework, enhancing LLMs with
domain-specific knowledge. PKG employs a trainable background knowledge module, aligning it
with task knowledge and generating relevant contextual information.
Iterative Retrieval. When confronted with intricate challenges like multi-step reasoning [344]
and long-form question answering [83, 284], traditional one-time retrieval may fall short. Addressing these demanding information needs, recent studies have proposed iterative retrieval, which
allows for continuously gathering knowledge throughout the generation process. Recognizing the
substantial advancements chain-of-thought prompting [326] has brought to LLMs in multi-step
reasoning, numerous studies [113, 301, 346] try to incorporate external knowledge at each reasoning
step and further guide retrieval process based on ongoing reasoning, reducing factual errors in
reasoning chains. Building upon chain-of-thought prompting, Press et al. [247] introduced self-ask.
Diverging from the conventional continuous, undelineated chain-of-thought prompting, self-ask
delineates the question it intends to address at each step, subsequently incorporating a search
action based on the follow-up question. Instead of solely depending on chain-of-thought prompting
for retrieval guidance, both Feng et al. [87] and Shao et al. [273] employed an iterative retrievalgeneration collaborative framework, where a model’s response serves as an insightful context to
procure more relevant knowledge, subsequently refining the response in the succeeding iteration.
Beyond multi-step reasoning tasks, Jiang et al. [140] shifted their emphasis to long-form generation.
They proposed an active retrieval augmented generation framework, which iteratively treats the
upcoming prediction as a query to retrieve relevant documents. If the prediction contains tokens
of low confidence, the sentence undergoes regeneration. In addition to using iterative retrieval
to improve intermediate generations, Zhang et al. [371] presented MixAlign, which iteratively
refines user questions using model-based guidance and seeking clarifications from users, ultimately
enhancing the alignment between questions and knowledge.
Post-hoc Retrieval. Beyond the traditional retrieve-then-read paradigm, a line of work has
delved into post-hoc retrieval, refining LLM outputs through subsequent retrieval-based revisions.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:22

Huang, et al.

To enhance the trustworthiness and attribution of LLMs, Gao et al. [94] adopted the researchthen-revise workflow, which initially research relevant evidence and subsequently revise the initial
generation based on detected discrepancies with the evidence. Similarly, Zhao et al. [381] introduced
the verify-and-edit framework to enhance the factual accuracy of reasoning chains by incorporating
external knowledge. For reasoning chains that show lower-than-average consistency, the framework
generates verifying questions and then refines the rationales based on retrieved knowledge, ensuring
a more factual response. Yu et al. [358] enhanced the post-hoc retrieval method through diverse
answer generation. Instead of generating just a single answer, they sample various potential
answers, allowing for a more comprehensive retrieval feedback. Additionally, by employing an
ensembling technique that considers the likelihood of the answer before and after retrieval, they
further mitigate the risk of misleading retrieval feedback.
Discussion. One crucial advantage of retrieval-augmented generation methodology is its effectiveness in mitigating hallucinations caused by knowledge gaps, and their generality, which
allows for application across any domain. This flexibility is further enhanced by the modularity of
the approach, treating external knowledge bases like plug-ins that can be swapped or modified as
needed. In terms of the drawbacks, it can be easily impacted by irrelevant retrievals, which may
decrease the overall performance by introducing noise or incorrect information into the response
generation process. Furthermore, the current paradigm exhibits shallow interactions between the
retriever and generator components, leading to suboptimal knowledge utilization. Hence, future
research should focus on developing a robust RAG system that minimizes the impact of irrelevant
retrieval, as well as integrating adaptive learning components that can dynamically adjust retrieval
strategies based on the context of the query and the performance of previous interactions.
5.2

Mitigating Training-related Hallucination

Training-related hallucinations typically arise from the intrinsic limitations of the architecture
and training strategies adopted by LLMs. In this context, we discuss various optimization methods
ranging from training stages (§5.2.1) and alignment stages (SFT & RLHF) (§5.2.2), aiming to mitigate
hallucinations within the training process.
5.2.1 Mitigating Pretraining-related Hallucination. One significant avenue of research in mitigating pretraining-related hallucination centers on the limitations inherent in model architectures,
especially unidirectional representation and attention glitches. In light of this, numerous studies have
delved into designing novel model architectures specifically tailored to address these flaws. To
address the limitations inherent in unidirectional representation, Li et al. [180] introduced BATGPT
which employs a bidirectional autoregressive approach. This design allows the model to predict
the next token based on all previously seen tokens, considering both past and future contexts, thus
capturing dependencies in both directions. Building on this idea, Liu et al. [189] highlighted the
potential of encoder-decoder models to make better use of their context windows, suggesting a
promising direction for future LLMs architecture design. Besides, recognizing the limitations of soft
attention within self-attention-based architecture, Liu et al. [183] proposed attention-sharpening
regularizers. This plug-and-play approach specifies self-attention architectures using differentiable
loss terms [365] to promote sparsity, leading to a significant reduction in reasoning hallucinations.
In the pre-training phase of LLMs, the choice of objective plays a pivotal role in determining the
model’s performance. However, conventional objectives can lead to fragmented representations and
inconsistencies in model outputs. Recent advancements have sought to address these challenges by
refining pre-training strategies, ensuring richer context comprehension, and circumventing biases.
Addressing the inherent limitations in training LLMs, where unstructured factual knowledge at a
document level often gets chunked due to GPU memory constraints and computational efficiency,
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:23

leading to fragmented information and incorrect entity associations, Lee et al. [160] introduced a
factuality-enhanced training method. By appending a TOPICPREFIX to each sentence in factual
documents, the approach transforms them into standalone facts, significantly reducing factual
errors and enhancing the model’s comprehension of factual associations. Similarly, considering that
randomly concatenating shorter documents during pre-training might introduce inconsistencies in
model outputs, Shi et al. [276] proposed In-Context Pretraining, an innovative approach in which
LLMs are trained on sequences of related documents. By altering the document order, this method
aims to maximize similarity within the context windows. It explicitly encourages LLMs to reason
across document boundaries, potentially bolstering the logical consistency between generations.
Discussion. Strategies designed to mitigate pretraining-related hallucinations typically are
fundamental, potentially yielding significant improvements. However, they typically involve modifications to pre-training architectures and objectives, which are computationally intensive. Moreover,
these integrations may lack broad applicability. Moving forward, the focus should be on developing
adaptable and efficient strategies that can be universally applied without extensive system overhaul.
5.2.2 Mitigating Misalignment Hallucination. Hallucinations induced during alignment often stem
from capability misalignment and belief misalignment. However, defining the knowledge boundary
of LLMs proves challenging, making it difficult to bridge the gap between LLMs’ inherent capabilities
and the knowledge presented in human-annotated data. While limited research addresses capability
misalignment, the focus mainly shifts toward belief misalignment.
Hallucinations stemming from belief misalignment often manifest as sycophancy, a tendency of
LLMs to seek human approval in undesirable ways. This sycophantic behavior can be attributed to
the fact that human preference judgments often favor sycophantic responses over more truthful
ones [274], paving the way for reward hacking [268]. To address this, a straightforward strategy is
to improve human preference judgments and, by extension, the preference model. Recent research
[25, 268] has investigated the use of LLMs to assist human labelers in identifying overlooked flaws.
Additionally, Sharma et al. [274] discovered that aggregating multiple human preferences enhances
feedback quality, thereby reducing sycophancy.
Besides, modifications to LLMs’ internal activations have also shown the potential to alter model
behavior. This can be achieved through methods like fine-tuning [327] or activation steering during
inference [69, 117, 289]. Specifically, Wei et al. [327] proposed a synthetic-data intervention, finetuning language models using synthetic data where the claim’s ground truth is independent of a
user’s opinion, aiming to reduce sycophantic tendencies.
Another avenue of research [263, 264] has been to mitigate sycophancy through activation
steering. This approach involves using pairs of sycophantic/non-sycophantic prompts to generate
the sycophancy steering vector, derived from averaging the differences in intermediate activations.
During inference, subtracting this vector can produce less sycophantic LLM outputs.
Discussion. Mitigating hallucinations through post-training methods represents a direct and
effective approach, bypassing the complexities associated with data sourcing and pre-training.
However, a notable gap in current research is the limited attention given to capability misalignment within LLMs. Future research should prioritize understanding the knowledge boundaries in
capability alignment to address hallucinations effectively.
5.3

Mitigating Inference-related Hallucination

Decoding strategies in LLMs play a pivotal role in determining the factuality and faithfulness of
the generated content. However, as analyzed in Section §3.3, imperfect decoding often results in
outputs that might lack factuality or stray from the original context. In this subsection, we explore
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:24

Huang, et al.

two advanced strategies aimed at refining the decoding strategy to enhance both the factuality and
faithfulness of the LLMs’ outputs.
5.3.1 Factuality Enhanced Decoding. Factuality Enhanced Decoding aims to improve the reliability
of outputs from LLMs by prioritizing the factuality of the information they generate. This line
of methods focuses on aligning model outputs closely with established real-world facts, thereby
minimizing the risk of disseminating false or misleading information.
Factuality Decoding. Considering the randomness in the sampling process can introduce
non-factual content into open-ended text generation, Lee et al. [160] introduced the factual-nucleus
sampling algorithm that dynamically adjusts the nucleus probability 𝑝 throughout sentence generation. By dynamically adjusting the nucleus probability based on decay factors and lower boundaries
and resetting the nucleus probability at the beginning of every new sentence, the decoding strategy
strikes a balance between generating factual content and preserving output diversity. Moreover,
some studies [31, 220] posit that the activation space of LLMs contains interpretable structures
related to factuality. Building on this idea, Li et al. [172] introduced Inference-Time Intervention
(ITI). This method first identifies a direction in the activation space associated with factually correct statements and then adjusts activations along the truth-correlated direction during inference.
By repeatedly applying such intervention, LLMs can be steered towards producing more factual
responses. Similarly, Chuang et al. [59] delved into enhancing the factuality of LLM’s decoding
process from a perspective of factual knowledge storage. They exploit the hierarchical encoding
of factual knowledge within transformer LLMs, noting that lower-level information is captured
in earlier layers and semantic information in the later ones. Drawing inspiration from [175], they
introduce DoLa, a strategy that dynamically selects and contrasts logits from different layers to
refine decoding factuality. By placing emphasis on knowledge from higher layers and downplaying
that from the lower layers, DoLa showcases its potential to make LLMs more factual, thus reducing
hallucinations.
Post-editing Decoding. Unlike methods that directly modify the probability distribution to
prevent hallucinations during the initial decoding, post-editing decoding seeks to harness the
self-correction capabilities of LLMs [237] to refine the originally generated content without relying
on an external knowledge base. Dhuliawala et al. [74] introduced the Chain-of-Verification (COVE),
which operates under the assumption that, when appropriately prompted, LLMs can self-correct
their mistakes and provide more accurate facts. Starting with an initial draft, it first formulates
verification questions and then systematically answers those questions in order to finally produce an
improved revised response. Similarly, Ji et al. [137] focused on the medical domain and introduced
an iterative self-reflection process. This process leverages the inherent ability of LLMs to first
generate factual knowledge and then refine the response until it aligns consistently with the
provided background knowledge.
Discussion. Factuality decoding methods, which typically assess the factuality at each decoding
step, can offer substantial improvements. Furthermore, due to their plug-and-play nature, they
allow for application without the need for computation-intensive training. Nevertheless, one of
the primary limitations of these methods lies in balancing factual accuracy with maintaining the
diversity and informativeness of the generated content, which can sometimes lead to compromises
in either aspect. On the other hand, post-editing decoding strategies, despite their effectiveness,
heavily rely on the self-correction capabilities of LLMs, which may be unreliable. Furthermore,
applying self-reflection can be time-consuming, limiting their practicality for real-time applications.
Hence, it is crucial to achieve an optimal balance between factuality and computational efficiency.
5.3.2 Faithfulness Enhanced Decoding. On the other hand, Faithfulness Enhanced Decoding prioritizes alignment with the provided context and also emphasizes enhancing the consistency within
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:25

the generated content. Thus, in this section, we summarize existing work into two categories,
including Context Consistency and Logical Consistency.
Context Consistency. In the era of LLMs, the issue of faithfulness hallucination typically lies in
insufficient attention to the given context, which inspired numerous research to design inferencetime strategies to enhance context consistency. Shi et al. [275] proposed context-aware decoding
(CAD), which modifies the model’s original output distribution in a contrastive formulation [175].
By amplifying the difference between output probabilities with and without context, CAD encourages the LLM to focus more on contextual information rather than over-rely on prior knowledge.
However, due to the inherent trade-off between diversity and context attribution [103, 363], overemphasizing contextual information can reduce diversity. To address this, Chang et al. [36] introduced
a dynamic decoding algorithm to bolster faithfulness while preserving diversity. Specifically, the
algorithm involves two parallel decoding steps, one with the context and one without. During the
decoding, the KL divergence between two token distributions serves as a guiding signal, indicating
the relevance of the source context. This signal is utilized to dynamically adjust the sampling
temperature to improve source attribution when the source is relevant. In a parallel line of work,
Choi et al. [53] introduced knowledge-constrained decoding (KCD), which employed a token-level
hallucination detection discriminator to identify contextual hallucinations and then guides the
faithful generation process by reweighing the token distribution. In addition to modifying output
distribution in place to enhance contextual attention, another line of work has explored a generic
post-edit approach to enhance faithfulness. Gao et al. [94] adopted a research-and-revise workflow,
where the research stage raises questions about various aspects of the model’s initial response and
gathers evidence for each query, while the revision stage detects and revises any disagreements
between the model’s response and the evidence. Similarly, Lei et al. [161] first detected contextual hallucinations at both the sentence and entity levels and then incorporated the judgments
to refine the generated response. Moreover, several studies have explored methods to overcome
the softmax bottleneck, which constrains the expression of diversity and faithful representations.
These approaches include employing a mixture of Softmax, which uses multiple hidden states to
compute softmax multiple times and merge the resulting distributions [343] and incorporating
pointer networks, which enables LLMs to copy the context words [37], thereby reducing context
hallucinations.
Logical Consistency. Inspired by the human thinking process, chain-of-thought [326] has been
introduced to encourage LLMs to decompose complex problems into explicitly intermediate steps,
thereby enhancing the reliability of the reasoning process [58]. Despite effective, recent research
[157, 302] demonstrated that the intermediate rationales generated by LLMs do not faithfully capture
their underlying behavior. A branch of research has been inspired to improve the consistency of
intermediate rationales generated by LLMs, particularly in multi-step reasoning [61] and logical
reasoning [18]. To enhance the self-consistency in chain-of-thought, Wang et al. [318] employed
a knowledge distillation framework. They first generate a consistent rationale using contrastive
decoding [175] and then fine-tune the student model with a counterfactual reasoning objective,
which effectively eliminates reasoning shortcuts [27] that derive answers without considering the
rationale. Furthermore, by employing contrastive decoding directly, LLMs can reduce surface-level
copying and prevent missed reasoning steps [229]. In addition, Li et al. [167] conducted a deep
analysis of the causal relevance among the context, CoT, and answer during unfaithful reasoning.
Analysis revealed that the unfaithfulness issue lies in the inconsistencies in the context information
obtained by the CoT and the answer. To address this, they proposed inferential bridging, which takes
the attribution method to recall contextual information as hints to enhance CoT reasoning and filter
out noisy CoTs that have low semantic consistency and attribution scores to the context. Paul et al.
[241] decomposed the reasoning process into two modules: an inference module, which employs
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:26

Huang, et al.

Direct Preference Optimization [253] to align the LLM towards preferring correct reasoning chains
over counterfactual chains, and a reasoning module, which encourages the LLM to reason faithfully
over the reasoning steps using a counterfactual and causal preference objective. Compared to
natural language reasoning, logical reasoning demands rigorous logical calculation, whereas plain
text often lacks precise logical structure, leading to unfaithful reasoning. To address this, Xu et al.
[338] introduced Symbolic CoT (SymbCoT), which incorporates symbolic expressions within CoT
to describe intermediate reasoning steps. Specifically, SymbCoT translates the natural language
context into a symbolic representation and then formulates a step-by-step plan to address the
logical reasoning problem, followed by a verifier to check the translation and reasoning chain,
thereby ensuring faithful logical reasoning.
Discussion. Faithfulness Enhanced Decoding significantly advances the alignment of LLM
outputs with provided contexts and enhances the internal consistency of the generated content.
However, strategies such as context-aware decoding often lack adaptive mechanisms, limiting their
effectiveness in scenarios that demand dynamic attention to context. Furthermore, many decoding
strategies require the integration of additional models that do not focus on context, introducing
significant computational overhead and reducing efficiency.
6

HALLUCINATIONS IN RETRIEVAL AUGMENTED GENERATION

Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucinations and improve the factuality of LLM outputs [131, 165, 255, 277]. By incorporating large-scale
external knowledge bases during inference, RAG equips LLMs with up-to-date knowledge, thus
reducing the potential risk of hallucination due to the inherent knowledge boundaries of LLMs
[260]. Despite being designed to mitigate LLM hallucinations, retrieval-augmented LLMs can still
produce hallucinations [16]. Hallucinations in RAG present considerable complexities, manifesting
as outputs that are either factually inaccurate or misleading. These hallucinations occur when
the content generated by the LLM does not align with real-world facts, fails to accurately reflect
the user’s query, or is not supported by the retrieved information. Such hallucinations can stem
from two primary factors: retrieval failure (§6.1) and generation bottleneck (§6.2). Through
a comprehensive analysis of the limitations present in current RAG systems, we aim to shed
light on potential improvements for retrieval-augmented LLMs, paving the way for more reliable
information retrieval systems.
6.1

Retrieval Failure

The retrieval process is a crucial initial step in the RAG framework, tasked with retrieving the most
relevant information for information-seeking queries. Consequently, failures in the retrieval stage
can have serious downstream effects on the RAG pipeline, leading to hallucinations. These failures
typically stem from three primary parts: the formulation of user queries, the reliability and scope
of retrieval sources, and the effectiveness of the retriever.
6.1.1 User Queries. User queries play a fundamental role in guiding the retrieval process with RAG
systems. The specificity and clarity of these queries critically influence the effectiveness of retrieval
outcomes. In this section, we discuss factors that may contribute to hallucinations from three
perspectives: blind retrieval, misinterpretation of ambiguous queries, and the challenges in accurate
retrieval of complex queries. Some examples are presented in Table 5 for a better understanding.
Retrieval Intent Decisions. Not all queries necessitate retrieval. Blind retrieval for queries
that do not require external knowledge can counterproductively lead to misleading responses. As
shown in Table 5, the query about "the boiling point of water at sea level" pertains to a basic scientific
fact that the model could address without external retrieval. However, the retrieval system was
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:27

Table 5. Examples of each category of User Queries that may lead to LLM hallucinations. Content marked in
Red represents the hallucinatory output.
Type

Retrieval
Intent

Example Dialog
Retrieved context: At high altitudes, water boils at lower
temperatures than at sea level, such as 95°C at elevations
above 2,000 meters. This altitude-induced reduction can affect
cooking times and methods.
Query: What is the boiling point of water at sea level?

Explanation
This query involves a basic scientific fact that should be
available within the model’s parametric knowledge.
However, the system inappropriately retrieves and uses
altitude-specific information about boiling temperatures,
leading to an incorrect response for sea-level conditions.

Response: The boiling point of water at sea level is 95°C.
Retrieved context: The Hennessey Venom GT famously
set a speed record by completing a mile on a racetrack in
32 seconds, highlighting its performance in motorsport events.

Ambiguous
Queries
Query: What’s the record for the fastest mile run on track?
Response: The fastest mile on a track was completed in
32 seconds by the Hennessey Venom GT.

Complex
Queries

Retrieved context: Alexander Fleming, known for his groundbreaking work in discovering penicillin, significantly influenced
modern antibiotics. Fleming’s discovery saved countless lives.
Query: Which university did the scientist who discovered
penicillin graduate from?

The query is ambiguous because it does not specify whether it refers to a human runner or a vehicle. The retrieval
system erroneously pulls information related to car racing
records instead of athletics.

The retrieval system only manages to fetch information
about Fleming’s professional achievements in the discovery
of penicillin. However, the document does not provide information about his educational background, thus the model generates a
hallucinatory answer.

Response: Alexander Fleming graduated from Oxford University.

inappropriately activated, blindly retrieving inaccurate information and consequently leading to
an undesirable response. Consequently, several studies [75, 204, 228, 378] have proposed to make a
shift from passive retrieval to adaptive retrieval. In general, these strategies can be divided into
two categories: heuristic-based and self-aware judgment. Heuristic-based methods employ heuristic
rules to determine the necessity of retrieval. For instance, Mallen et al. [204] observed a positive
correlation between LLMs’ memorization capabilities and entity popularity and suggested triggering
retrieval only when the entity popularity in the user query falls below a certain threshold. Similarly,
Jeong et al. [135] determined the timing of retrieval based on the query complexity, whereas Asai
et al. [11] considered whether the query is factual relevant. Self-aware judgment leverages the
models’ intrinsic judgment to decide the necessity for information retrieval. Feng et al. [86], Ren
et al. [261] and Wang et al. [321] directly prompted LLMs for retrieval decisions, recognizing
that LLMs possess a certain level of awareness regarding their knowledge boundaries [143, 350].
Moreover, Jiang et al. [140] introduced an active retrieval strategy that triggers retrieval only
when the LLM generates low-probability tokens. Similarly, Su et al. [288] not only considered the
uncertainty of each token but also its semantic contribution and impact on the subsequent context.
More recently, Cheng et al. [48] proposed four orthogonal criteria for determining the retrieval
timing, which include intent-aware, knowledge-aware, time-sensitive-aware, and self-aware.
Ambiguous Queries. Ambiguous user queries, containing omission, coreference, and ambiguity,
significantly complicate the retrieval system’s ability to fetch precisely relevant information, thereby
increasing the likelihood of generating undesirable responses. As shown in Table 5, due to the
ambiguity of the query about "the record for the fastest mile run on track", the retrieval system
erroneously retrieved information from automobile racing events, which led the model to generate
a response suited for vehicles instead of athletes. A prevalent mitigation strategy is query rewriting,
where queries are refined and decontextualized to better match relevant documents. Wang et al.
[317] and Jagerman et al. [132] have explored prompting approaches where the LLM is prompted
to generate a pseudo-document or rationale based on the original query, which is then used for
further retrieval. Additionally, Ma et al. [200] introduced a trainable rewriter which is trained using
the feedback from the LLM via reinforcement learning. Mao et al. [207] employed the feedback
signals from the reranker to train the rewrite model, thus eliminating the reliance on annotated data.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:28

Huang, et al.

However, the challenges deepen in conversational search, which encounters a more complex issue
of context-dependent query understanding with the lengthy conversational history. Addressing
this, Yoon et al. [351] proposed a similar framework for optimizing the LLM to generate retrieverpreferred query rewrites. This operated by generating a variety of queries and then using the
preference of the rank of retrieved passage to optimize the query rewriting model.
Complex Queries. Complex user queries, characterized by requiring intensive reasoning [286]
or encompassing multiple aspects [272, 319], pose significant challenges to the retrieval system.
Such queries require advanced understanding and decomposition capabilities, which may exceed
the current capabilities of the current retrieval methods based on keyword or semantic matching, often leading to partial or incorrect retrievals. For example, as shown in Table 5, due to the
multi-step nature of the query about "Which university did the scientist who discovered penicillin
graduate from?", direct retrieval often leads to incomplete results, thereby resulting in hallucinatory
responses. A common approach involves query decomposition, where the complex query is decomposed into sub-queries to facilitate more accurate information retrieval. For instance, Wang et al.
[319] implemented a sub-aspect explorer that utilizes the extensive world knowledge embedded
LLMs to identify potential sub-aspects of user queries, thereby providing explicit insights into
the user’s underlying intents. Similarly, Shao et al. [272] concentrated on the demanding task
of expository writing, aiming at retrieving comprehensive information to compose Wikipedialike articles from scratch on a specific topic. This approach involves decomposing the topic into
various perspectives and simulating multi-turn conversations with LLMs, each personified with
different perspectives for question asking. Additionally, Cao et al. [32] and Chu et al. [56] explored
knowledge-intensive complex reasoning and employed a divide-and-conquer strategy. This strategy
begins with decomposing complex questions into question trees, where at each node, the LLM
retrieves and aggregates answers from diverse knowledge sources.
6.1.2 Retrieval Sources. The reliability and scope of retrieval sources are crucial determinants of
the efficacy of RAG systems. Effective retrieval depends not only on the clarity of the user queries
but also on the quality and comprehensiveness of the sources from which information is retrieved.
When these sources contain factually incorrect or outdated information, the risk of retrieval failures
increases significantly, potentially leading to the generation of incorrect or misleading information.
As the landscape of content creation evolves with the rapid advancement of Artificial Intelligence
Generated Content (AIGC) [33], an increasing volume of LLM-generated content is permeating the
internet, subsequently becoming integrated into retrieval sources [39]. This integration is reshaping
the dynamics of information retrieval, as evidenced by recent empirical studies [68, 339] suggesting
that modern retrieval models tend to favor LLM-generated content over human-authored content.
Recent research [44] has explored the implications of progressively integrating LLM-generated
content into RAG systems. The findings indicate that, without appropriate intervention, humangenerated content may progressively lose its influence within RAG systems. Additionally, Tan
et al. [293] investigated the performance of RAG systems when incorporating LLM-generated into
retrieved contexts, revealing a significant bias favoring generated contexts. This bias stems from the
high similarity between generated context and questions, as well as the semantic incompleteness
of retrieved contexts. More seriously, the propensity of LLMs to produce factually inaccurate
hallucinations exacerbates the reliability issues of retrieval sources. As LLM-generated content
often contains factual errors, its integration into retrieval sources can mislead retrieval systems,
further diminishing the accuracy and reliability of the information retrieved.
To combat these biases, several approaches have been explored. Inspired by common practice in
pre-training data processing [23], Asai et al. [12] proposed a scenario that incorporates a quality
filter designed to ensure the high quality of the retrieval datastore. Additionally, Pan et al. [238]
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:29

proposed Credibility-aware Generation (CAG), which equips LLMs with the ability to discern and
handle information based on its credibility. This approach assigns different credibility levels to
information, considering its relevance, temporal context, and the trustworthiness of its source, thus
effectively reducing the impact of flawed information in RAG systems.
6.1.3 Retriever. When the user query is explicit and the retrieval source is reliable, the effectiveness
of the retrieval process depends crucially on the performance of the retriever. In such scenarios,
the retriever’s effectiveness is significantly compromised by improper chunking and embedding
practices.
Chunking. Given the extensive nature of retrieval sources, which often encompass lengthy
documents like web pages, it poses significant challenges for LLMs with limited context length. Thus,
chunking emerges as an indispensable step in RAG, which involves segmenting these voluminous
documents into smaller, more manageable chunks to provide precise and relevant evidence for LLMs.
According to actual needs, the chunking granularity ranges from documents to paragraphs, even
sentences. However, inappropriate retrieval granularity can compromise the semantic integrity
and affect the relevance of retrieved information [224], thereby affecting the performance of
LLMs. Fixed-size chunking, which typically breaks down the documents into chunks of a specified
length such as 100-word paragraphs, serves as the most crude and prevalent strategy of chunking,
which is widely used in RAG systems [24, 109, 165]. Considering fixed-size chunking falls short in
capture structure and dependency of lengthy documents, Sarthi et al. [267] proposed RAPTOR, an
indexing and retrieval system. By recursively embedding, clustering, and summarizing chunks of
text, RAPTOR constructs a tree to capture both high-level and low-level details. When retrieval,
RAPTOR enables LLMs to integrate information from different levels of abstraction, providing
a more comprehensive context for user queries. Instead of chunking text with a fixed chunk
size, semantic chunking adaptively identifies breakpoints between sentences through embedding
similarity, thereby preserving semantic continuity [144]. Furthermore, Chen et al. [43] pointed
out the limitations of the existing retrieval granularity. On the one hand, while a coarser retrieval
with a longer context can theoretically provide a more comprehensive context, it often includes
extraneous details that could potentially distract LLMs. On the other hand, a fine-grain level can
provide more precise and relevant information, it has limitations such as not being self-contained
and lacking necessary contextual information. To address these shortcomings, Chen et al. [43]
introduced a novel retrieval granularity, proposition, which is defined as atomic expressions within
the text, each encapsulating a distinct factoid and presented in a concise self-contained natural
language format.
Embedding. Once the retrieval text is chunked, text chunks are subsequently transformed into
vector representation via an embedding model. Such a representation scheme is supported by the
well-known data structure of vector database [142], which systematically organizes data as keyvalue pairs for efficient text retrieval. In this manner, the relevance score can be computed according
to the similarity function between the text representation and query representation. However, a
sub-optimal embedding model may compromise performance, which affects the similarity and
matching of chunks to user queries, potentially misleading LLMs. Typically, a standard embedding
model [96, 130, 147, 382] learns the query and text representations with encoder-based architecture
(e.g. BERT [73], RoBERTa [191]) via contrastive learning [304], where the loss is constructed by
contrasting a positive pair of query-document against a set of random negative pairs. However, these
embeddings showcase their limitations when applied to new domains, such as medical and financial
applications [222, 295]. In these cases, recent studies [71, 234, 277, 332] propose to fine-tune the
embedding models on domain-specific data to enhance retrieval relevance. For example, REPLUG
[277] utilizes language modeling scores of the answers as a proxy signal to train the dense retriever.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:30

Huang, et al.

More recently, Muennighoff et al. [221] have introduced generative representational instruction
tuning where a single LLM is trained to handle both generative and embedding tasks, which
largely reduces inference latency in RAG by caching representations. Despite these advancements,
the field faces challenges, particularly with the fine-tuning of high-performing yet inaccessible
embedding models, such as OpenAI’s text-embedding-ada-002. Addressing this gap, Zhang et al.
[367] introduced a novel approach for fine-tuning a black-box embedding model by augmenting it
with a trainable embedding model which significantly enhances the performance of the black-box
embeddings.
6.2

Generation Bottleneck

After the retrieval process, the generation stage emerges as a pivotal point, responsible for generating content that faithfully reflects the retrieved information. However, this stage can encounter
significant bottlenecks that may lead to hallucinations. We summarize two key capabilities of LLMs
that are closely related to these bottlenecks: contextual awareness and contextual alignment. Each
plays an important role in ensuring the reliability and credibility of the RAG system.
6.2.1 Contextual Awareness. Contextual awareness involves understanding and effectively utilizing
contextual information retrieved. This section discusses the key factors that impact the LLM’s
ability to maintain contextual awareness, which can be categorized into three main parts: (1) the
presence of noisy retrieval in context, (2) context conflicts, and (3) insufficient utilization of context
information.
Noisy Context. As emphasized in §6.1, the failure in the retrieval process may inevitably introduce irrelevant information, which will propagate into the generation stage. When the generator is
not robust enough to these irrelevant retrievals, it will mislead the generator and even introduce
hallucinations [65].
Yoran et al. [352] conducted a comprehensive analysis on the robustness of current retrievalaugmented LLMs, revealing a significant decrease in performance with random retrieval. While
using an NLI model to filter out irrelevant passages is effective, this method comes with the trade-off
of inadvertently discarding some relevant passages. A more effective solution is to train LLMs to
ignore irrelevant contexts by incorporating irrelevant contexts in training data. Similarly, Yu et al.
[357] introduced Chain-of-Note, which enables LLMs to first generate reading notes for retrieved
contexts and subsequently formulate the final answer. In this way, LLMs can not only filter irrelevant
retrieval to improve noise robustness but also respond with unknown when retrieval is insufficient
to answer user queries. In addition to improving LLM robustness by learning to ignore irrelevant
content in the context, several studies [139, 176, 323, 336] propose to compress the context to filter
out irrelevant information. Specifically, Li [176] and Jiang et al. [139] made use of small language
models to compute self-information and perplexity for prompt compression, finding the most
informative content. Similarly, Wang et al. [323] proposed to filter out irrelevant content and leave
precisely supporting content based on lexical and information-theoretic approaches. Besides, efforts
have been also made to employ summarization models as compressors. Xu et al. [336] presented
both extractive and abstractive compressors, which are trained to improve LLMs’ performance while
keeping the prompt concise. Liu et al. [188] involved summarization compression and semantic
compression, where the former achieves compression by summarizing while the latter removes
tokens with a lower impact on the semantic.
Context Conflict. Retrieval-augmented LLMs generate answers through the combined effect of
parametric knowledge and contextual knowledge. As discussed in §3.3.2, LLMs may sometimes
exhibit over-confidence, which can bring new challenges to the faithfulness of RAG systems when
facing knowledge conflicts. Knowledge conflicts in RAG are situations where contextual knowledge
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:31

contradicts LLMs’ parametric knowledge. Longpre et al. [194] first investigated knowledge conflicts
in open-domain question answering, where conflicts are automatically created by replacing all
spans of the gold answer in the retrieval context with a substituted entity. Findings demonstrate that
generative QA reader models (e.g. T5) tend to trust parametric memory over contextual information.
By further training the retriever to learn to trust the contextual evidence with augmented training
examples by entity substitution, the issue of over-reliance on parametric knowledge is mitigated.
Similar findings are also reported by Li et al. [166] who demonstrated that fine-tuning LLMs
on counterfactual contexts can effectively improve the controllability of LLMs when dealing
with contradicts contexts. Also building upon counterfactual data augmentation, Neeman et al.
[227] trained models to predict two disentangled answers, one based on contextual knowledge
and the other leveraging parametric knowledge to address knowledge conflicts. Besides, Zhou
et al. [390] introduced two effective prompting-based strategies, namely opinion-based prompts
and counterfactual demonstrations. Opinion-based prompts transform the context to narrators’
statements, soliciting the narrators’ opinions, whereas counterfactual demonstrations employ
counterfactual instances to improve faithfulness in situations of knowledge conflict. While Longpre
et al. [194] and Li et al. [166] concentrated their research on the context of a limited single evidence
setting, Chen et al. [40] further expanded this study to consider a more realistic scenario in which
models consider multiple evidence passages and find models rely almost exclusively on contextual
evidence.
Considering previous studies [166, 194] mostly focused on smaller models, Xie et al. [334]
raised doubts about the applicability of their conclusions in the era of LLMs. Such heuristic entitylevel substitution may lead to incoherent counter-memory, thereby making it trivial for LLMs
to overlook the construct knowledge conflicts. By directly eliciting LLMs to generate a coherent
counter-memory that factually conflicts with the parametric memory, LLMs exhibit their high
receptivity to external evidence.
Context Utilization. Despite successfully retrieving evidence relevant to factoid queries, LLMs
can encounter a significant performance degradation due to insufficient utilization of the context,
especially for information located in the middle of the long context window, a notable issue
known as the lost-in-the-middle phenomenon [189]. Beyond factoid QA, recent studies have further
demonstrated such a middle-curse also holds in abstractive summarization [257], long-form QA [41]
and passage ranking [294]. One potential explanation lies in the use of rotary positional embedding
(RoPE) [287], which is widely used in open-source LLMs, due to its excellent performance in length
extrapolation [380]. As a representative relative position embedding, RoPE features a long-term
decay property, which inherently biases the LLM to give precedence to current or proximate tokens,
thereby diminishing its attention on those that are more distant. Another contributing factor is
that the most salient information often resides at the beginning or the end of pre-training data,
a characteristic commonly observed in news reports [257]. Such an issue brings forth challenges
in retrieval-augmented LLMs, as retrieval-augmented LLMs are typically designed with extensive
lengths to accommodate more retrieval documents.
To mitigate this crucial issue, He et al. [114] introduced several tasks specially designed for
information seeking to enhance the capability of information utilization by explicitly repeating the
question and extracting the index of supporting documents before generating answers. Furthermore,
Zhang et al. [377] introduced Multi-scale Positional Encoding (Ms-PoE), which mitigates the longterm decay effect characteristic of RoPE by rescaling position indices. Ms-PoE provides a plugand-play solution to enhance the ability of LLMs to effectively capture information in the middle
of the context without the need for additional fine-tuning. Besides, Ravaut et al. [257] proposed
hierarchical and incremental summarization, which effectively preserves the salient information
and compresses the length of context to avoid the middle-curse.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:32

Huang, et al.

6.2.2 Contextual Alignment. Contextual alignment ensures that LLM outputs faithfully align with
relevant context. This section outlines the primary components of contextual alignment, which
include: (1) source attribution and (2) faithful decoding.
Source Attribution. Source attribution [121] in retrieval-augmented LLMs refers to the process
by which the model identifies and utilizes the origins of information within its generation process.
This component is crucial for ensuring that the outputs of RAG systems are not only relevant but
also verifiable and grounded in credible sources.
To achieve source attribution in RAG systems, recent studies have been explored, which can be
categorized into three lines based on the type of attribution. (1) Plan-then-Generate: Fierro et al.
[88] introduced the blueprint model for attribution, which conceptualizes text plans as a series
of questions that serve as blueprints for generation process, dictating both the content and the
sequence of the output. Compared with abstractive questions, Huang et al. [120] enabled the model
to first ground to extractive evidence spans, which guides the subsequent generation process.
Leveraging either abstract questions or extractive spans as planning facilitates a built-in attribution
mechanism, as they provide a natural link between retrieved information and the subsequent
generation. Similarly, Slobodkin et al. [282] broke down the conventional end-to-end generation
process into three intuitive stages: content selection, sentence planning, and sentence fusion. By
initially identifying relevant source segments and subsequently conditioning the generation process
on them, the selected segments naturally serve as attributions. (2) Generate-then-Reflect: Asai et al.
[11] proposed training the LLM to generate text with reflection tokens. These reflection tokens
empower the LLM to decide whether to retrieve, assess the relevance of the retrieved document,
and critique its own generation to ensure attributability. By critiquing its generation. Furthermore,
Ye et al. [348] introduced AGREE, designed to facilitate self-grounding in LLMs. AGREE trains
LLMs to generate well-grounded claims with citations and identify claims that lack verification.
An iterative retrieval process is then employed to actively seek additional information for these
unsupported statements. (3) Self-Attribution: In addition to leveraging external supervised signals
for attribution, Qi et al. [248] proposed a self-attribution mechanism that utilizes model-internal
signals. It operates by first identifying context-sensitive answer tokens, which are then paired with
retrieved documents that contributed to the model generation via saliency methods.
Faithful Decoding. Despite significant optimizations in the RAG pipeline that facilitate the
incorporation of highly relevant content into the model’s context, current LLMs still cannot
guarantee faithful generation. The unfaithful utilization of relevant context by LLMs undermines
the reliability of their outputs, even when the sources of information are verifiably accurate. Wu
et al. [331] analyzed the model’s knowledge preference when internal knowledge conflicts with
contextual information and observed the tug-of-war between the LLM’s internal prior and external
evidence. To tackle this issue, recent research [275, 330] has focused on faithful decoding within
RAG systems, aiming to improve the models’ ability to generate content that faithfully aligns
with contextual information. Shi et al. [275] presented context-aware decoding, which modifies
the model’s original output probability distribution into the pointwise mutual information (PMI)
formulation. The strategy operates by amplifying the difference between the output probabilities
when a model is used with and without context, thereby enhancing the faithfulness of LLMs to
the provided context. Li et al. [173] adopted a semi-parametric language modeling approach [150]
which facilitates the integration of contextual spans of arbitrary length into LM generations. The
generation is then verified via speculative decoding, further ensuring model faithfulness. More
recently, Wu et al. [330] proposed faithfulness-oriented decoding, which leverages a lightweight
faithfulness detector to monitor the beam-search process. The detector leverages fine-grained
decoding dynamics including sequence likelihood, uncertainty quantification, context influence, and
semantic alignment to synchronously detect unfaithful sentences. When an unfaithful generation
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:33

is detected, it triggers the backtrack operation and selects the beam with the more faithful score,
thus ensuring greater faithfulness to the retrieval sources.

7

FUTURE DISCUSSION

As the field of research on hallucinations in LLMs continues to evolve, our focus shifts towards
the next horizon of inquiry. We explore prospective areas of study, notably the phenomenon of
hallucinations in vision-language models (§7.1) and the challenge of delineating and understanding
knowledge boundaries within LLMs (§7.2).

7.1

Hallucination in Large Vision-Language Models

Enabling the visual perception ability, along with exceptional language understanding and generation capabilities, Large Vision-Language Models (LVLMs) have exhibited remarkable visionlanguage capabilities [47, 123, 186, 201, 355, 356, 360, 392]. Unlike previous pre-trained multi-modal
models that gain limited vision-language abilities from large-scale visual-language pre-training
datasets [170, 197, 325, 387], LVLMs exploit advanced LLMs to unleash the power of interacting
with humans and the environment. The consequent diverse applications of LVLMs also bring new
challenges to maintaining the reliability of such systems. Recent studies have revealed that current
LVLMs are suffering from multi-modal hallucinations, where models provide responses misaligned
with the corresponding visual information [104, 187, 297]. Such multi-modal hallucinations could
cause unexpected behaviors when applying LVLMs to real-world scenarios, which therefore had to
be further investigated and mitigated.
Li et al. [178] and Lovenia et al. [195] took the first step towards evaluating the object hallucinations in the LVLMs. Evaluations and experiments reveal that current LVLMs are prone to generate
inconsistent responses with respect to the associated image, including non-existent objects, wrong
object types, and attributes, incorrect semantic relationships, etc. [315, 361]. Furthermore, Liu et al.
[185], Zong et al. [395] and Liu et al. [184] show that LVLMs can be easily fooled and experience
a severe performance drop due to their over-reliance on the strong language prior, as well as its
inferior ability to defend against inappropriate user inputs [112, 134]. Jiang et al. [138], Wang et al.
[315] and Jing et al. [141] took a step forward to holistically evaluate multi-modal hallucination.
What’s more, when presented with multiple images, LVLMs sometimes mix or miss parts of the
visual context, as well as fail to understand temporal or logical connections between them, which
might hinder their usage in many scenarios, yet properly identifying the reason for such disorders
and tackling them still requires continued efforts. Despite the witnessed perception errors, LVLMs
can generate flawed logical reasoning results even when correctly recognizing all visual elements,
which remains further investigation.
Efforts have been made towards building a more robust large vision-language model. Gunjal et al.
[108], Lu et al. [196], Wang et al. [316], and Liu et al. [185] proposed to further finetune the model
for producing more truthful and helpful responses. Another line of work chooses to post-hoc rectify
the generated inconsistent content, such as [391], and [349], which introduced expert models. To
free from the external tools, Leng et al. [162], Huang et al. [122], and Zhao et al. [379] tried to fully
utilize the LVLM itself to alleviate hallucinations. Though proved to be effective, those methods
usually require additional data annotations, visual experts, training phases, and computational
costs, which prevent LVLMs from effectively scaling and generalizing to various fields. Thus, more
universal approaches are expected to build a more reliable system, such as faithful and large-scale
visual-text pre-training and alignment methods.
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:34

7.2

Huang, et al.

Understanding Knowledge Boundary in LLMs

Despite the impressive capacity to capture factual knowledge from extensive data, LLMs still face
challenges in recognizing their own knowledge boundaries. This shortfall leads to the occurrence
of hallucinations, where LLMs confidently produce falsehoods without an awareness of their own
knowledge limits [235, 261, 384]. Numerous studies delve into probing knowledge boundaries of
LLMs, utilizing strategies such as evaluating the probability of a correct response in a multiplechoice setting [143], or quantifying the model’s output uncertainty by evaluating the similarity
among sets of sentences with uncertain meanings.
Furthermore, a line of work [13, 31, 172, 220] has revealed that LLMs contain latent structures
within their activation space that relate to beliefs about truthfulness. Recent research [281] also
found substantial evidence for LLMs’ ability to encode the unanswerability of questions, despite
the fact that these models exhibit overconfidence and produce hallucinations when presented with
unanswerable questions. Nonetheless, Levinstein and Herrmann [163] have employed empirical
and conceptual tools to probe whether or not LLMs have beliefs. Their empirical results suggest that
current lie-detector methods for LLMs are not yet fully reliable, and the probing methods proposed
by Burns et al. [31] and Azaria and Mitchell [13] do not adequately generalize. Consequently,
whether we can effectively probe LLMs’ internal beliefs is ongoing, requiring further research.
8

CONCLUSION

In this comprehensive survey, we have undertaken an in-depth examination of hallucinations
within large language models, delving into the intricacies of their underlying causes, pioneering
detection methodologies as well as related benchmarks, and effective mitigation strategies. Although
significant strides have been taken, the conundrum of hallucination in LLMs remains a compelling
and ongoing concern that demands continuous investigation. Moreover, we envision this survey
as a guiding beacon for researchers dedicated to advancing robust information retrieval systems
and trustworthy artificial intelligence. By navigating the complex landscape of hallucinations, we
hope to empower these dedicated individuals with invaluable insights that drive the evolution of
AI technologies toward greater reliability and safety.
REFERENCES
[1] Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S Morcos. 2023. SemDeDup: Data-efficient
learning at web-scale through semantic deduplication. ArXiv preprint abs/2303.09540 (2023). https://arxiv.org/abs/
2303.09540
[2] Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2023. Evaluating
correctness and faithfulness of instruction-following models for question answering. ArXiv preprint abs/2307.16877
(2023). https://arxiv.org/abs/2307.16877
[3] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do Language Models Know When They’re Hallucinating References? ArXiv preprint abs/2305.18248 (2023). https://arxiv.org/abs/2305.18248
[4] Perplexity AI. 2023. Perplexity AI. https://www.perplexity.ai/
[5] Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yun-Hsuan Sung. 2023. Characterizing
Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models. ArXiv preprint abs/2302.05578
(2023). https://arxiv.org/abs/2302.05578
[6] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona T. Diab, and Marjan Ghazvininejad. 2022. A Review on
Language Models as Knowledge Bases. CoRR abs/2204.06031 (2022). https://doi.org/10.48550/ARXIV.2204.06031
arXiv:2204.06031
[7] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,
Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian
Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James
Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong
Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George
Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White,
ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:35

Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub
Sygnowski, and et al. 2023. Gemini: A Family of Highly Capable Multimodal Models. CoRR abs/2312.11805 (2023).
https://doi.org/10.48550/ARXIV.2312.11805 arXiv:2312.11805
[8] Anthropic. 2023. Claude. https://claude.ai/
[9] Antropic. 2024. Claude 3 haiku: our fastest model yet. 2024. https://www.anthropic.com/news/claude-3-haiku
[10] ArXiv. 2023. arxiv dataset. https://www.kaggle.com/datasets/Cornell-University/arxiv/versions/134
[11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-RAG: Learning to Retrieve,
Generate, and Critique through Self-Reflection. CoRR abs/2310.11511 (2023). https://doi.org/10.48550/ARXIV.2310.
11511 arXiv:2310.11511
[12] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau
Yih. 2024. Reliable, Adaptable, and Attributable Language Models with Retrieval. CoRR abs/2403.03187 (2024).
https://doi.org/10.48550/ARXIV.2403.03187 arXiv:2403.03187
[13] Amos Azaria and Tom M. Mitchell. 2023. The Internal State of an LLM Knows When its Lying. ArXiv preprint
abs/2304.13734 (2023). https://arxiv.org/abs/2304.13734
[14] Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-Augmented Language Model Prompting for ZeroShot Knowledge Graph Question Answering. ArXiv preprint abs/2306.04136 (2023). https://arxiv.org/abs/2306.04136
[15] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng
Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A Multitask, Multilingual, Multimodal Evaluation of
ChatGPT on Reasoning, Hallucination, and Interactivity. ArXiv preprint abs/2302.04023 (2023). https://arxiv.org/abs/
2302.04023
[16] Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven
Failure Points When Engineering a Retrieval Augmented Generation System. CoRR abs/2401.05856 (2024). https:
//doi.org/10.48550/ARXIV.2401.05856 arXiv:2401.05856
[17] Mario Barrantes, Benedikt Herudek, and Richard Wang. 2020. Adversarial nli for factual correctness in text summarisation models. ArXiv preprint abs/2005.11739 (2020). https://arxiv.org/abs/2005.11739
[18] Pierre Basso. 1993. Conditional Causal Logic: A Formal Theory of the Meaning Generating Processes in a Cognitive
System. In Proceedings of the 13th International Joint Conference on Artificial Intelligence. Chambéry, France, August 28 September 3, 1993, Ruzena Bajcsy (Ed.). Morgan Kaufmann, 845–851. http://ijcai.org/Proceedings/93-2/Papers/002.pdf
[19] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. ArXiv
preprint abs/2004.05150 (2020). https://arxiv.org/abs/2004.05150
[20] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of
Stochastic Parrots: Can Language Models Be Too Big?. In FAccT ’21: 2021 ACM Conference on Fairness, Accountability,
and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, Madeleine Clare Elish, William Isaac, and
Richard S. Zemel (Eds.). ACM, 610–623. https://doi.org/10.1145/3442188.3445922
[21] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled Sampling for Sequence Prediction
with Recurrent Neural Networks. In Advances in Neural Information Processing Systems 28: Annual Conference on
Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, Corinna Cortes, Neil D.
Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (Eds.). 1171–1179. https://proceedings.neurips.cc/
paper/2015/hash/e995f98d56967d946471af29d7bf99f1-Abstract.html
[22] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans.
2023. The Reversal Curse: LLMs trained on" A is B" fail to learn" B is A". ArXiv preprint abs/2309.12288 (2023).
https://arxiv.org/abs/2309.12288
[23] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy,
Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow,
Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. CoRR
abs/2204.06745 (2022). https://doi.org/10.48550/ARXIV.2204.06745 arXiv:2204.06745
[24] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den
Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick,
Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela
Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent
Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA (Proceedings of Machine Learning Research,
Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato (Eds.). PMLR,
2206–2240. https://proceedings.mlr.press/v162/borgeaud22a.html
[25] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosuite, Amanda
Askell, Andy Jones, Anna Chen, et al. 2022. Measuring progress on scalable oversight for large language models.
ArXiv preprint abs/2211.03540 (2022). https://arxiv.org/abs/2211.03540

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:36

Huang, et al.

[26] Ralph Allan Bradley and Milton E Terry. 1952. Rank analysis of incomplete block designs: I. The method of paired
comparisons. Biometrika 39, 3/4 (1952), 324–345. https://www.jstor.org/stable/2334029
[27] Ruben Branco, António Branco, João António Rodrigues, and João Ricardo Silva. 2021. Shortcutted Commonsense:
Data Spuriousness in Deep Learning of Commonsense Reasoning. In Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican
Republic, 1504–1521. https://doi.org/10.18653/v1/2021.emnlp-main.113
[28] Andrei Z Broder. 1997. On the resemblance and containment of documents. In Proceedings. Compression and Complexity
of SEQUENCES 1997 (Cat. No. 97TB100171). IEEE, 21–29.
[29] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html
[30] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. 2023.
Sparks of Artificial General Intelligence: Early experiments with GPT-4. ArXiv preprint abs/2303.12712 (2023).
https://arxiv.org/abs/2303.12712
[31] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models
without supervision. ArXiv preprint abs/2212.03827 (2022). https://arxiv.org/abs/2212.03827
[32] Shulin Cao, Jiajie Zhang, Jiaxin Shi, Xin Lv, Zijun Yao, Qi Tian, Lei Hou, and Juanzi Li. 2023. Probabilistic Treeof-thought Reasoning for Answering Knowledge-intensive Complex Questions. In Findings of the Association for
Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika
Bali (Eds.). Association for Computational Linguistics, 12541–12560. https://doi.org/10.18653/V1/2023.FINDINGSEMNLP.835
[33] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. 2023. A Comprehensive Survey
of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT. CoRR abs/2303.04226 (2023).
https://doi.org/10.48550/ARXIV.2303.04226 arXiv:2303.04226
[34] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022.
Quantifying memorization across neural language models. ArXiv preprint abs/2202.07646 (2022). https://arxiv.org/
abs/2202.07646
[35] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts,
Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th
USENIX Security Symposium (USENIX Security 21). 2633–2650.
[36] Chung-Ching Chang, David Reitter, Renat Aksitov, and Yun-Hsuan Sung. 2023. KL-Divergence Guided Temperature
Sampling. ArXiv preprint abs/2306.01286 (2023). https://arxiv.org/abs/2306.01286
[37] Haw-Shiuan Chang, Zonghai Yao, Alolika Gon, Hong Yu, and Andrew McCallum. 2023. Revisiting the Architectures
like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond. In
Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers,
Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 12707–12730. https:
//doi.org/10.18653/V1/2023.FINDINGS-ACL.805
[38] Haw-Shiuan Chang and Andrew McCallum. 2022. Softmax Bottleneck Makes Language Models Unable to Represent
Multi-mode Word Distributions. In Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Dublin, Ireland, 8048–8073. https:
//doi.org/10.18653/v1/2022.acl-long.554
[39] Canyu Chen and Kai Shu. 2023. Combating Misinformation in the Age of LLMs: Opportunities and Challenges. CoRR
abs/2311.05656 (2023). https://doi.org/10.48550/ARXIV.2311.05656 arXiv:2311.05656
[40] Hung-Ting Chen, Michael J. Q. Zhang, and Eunsol Choi. 2022. Rich Knowledge Sources Bring Complex Knowledge
Conflicts: Recalibrating Models to Reflect Conflicting Evidence. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav
Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 2292–2307. https:
//doi.org/10.18653/V1/2022.EMNLP-MAIN.146
[41] Hung-Ting Chen, Fangyuan Xu, Shane A Arora, and Eunsol Choi. 2023. Understanding Retrieval Augmentation for
Long-Form Question Answering. ArXiv preprint abs/2310.12150 (2023). https://arxiv.org/abs/2310.12150

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:37

[42] Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. 2023. FELM:
Benchmarking Factuality Evaluation of Large Language Models. ArXiv preprint abs/2310.00741. https://arxiv.org/
abs/2310.00741
[43] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu.
2023. Dense X Retrieval: What Retrieval Granularity Should We Use? CoRR abs/2312.06648 (2023). https://doi.org/10.
48550/ARXIV.2312.06648 arXiv:2312.06648
[44] Xiaoyang Chen, Ben He, Hongyu Lin, Xianpei Han, Tianshu Wang, Boxi Cao, Le Sun, and Yingfei Sun. 2024. Spiral of
Silence: How is Large Language Model Killing Information Retrieval? – A Case Study on Open Domain Question
Answering. arXiv:2404.10496 [cs.IR] https://arxiv.org/abs/2404.10496
[45] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. 2022. Towards Improving Faithfulness in Abstractive
Summarization. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/9b6d7202750e8e32cd5270eb7fc131f7Abstract-Conference.html
[46] Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. 2023. Improving Translation Faithfulness
of Large Language Models via Augmenting Instructions. ArXiv preprint abs/2308.12674 (2023). https://arxiv.org/abs/
2308.12674
[47] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2023. Measuring and Improving Chain-ofThought Reasoning in Vision-Language Models. ArXiv preprint abs/2309.04461 (2023). https://arxiv.org/abs/2309.04461
[48] Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan,
and Xipeng Qiu. 2024. Unified Active Retrieval for Retrieval Augmented Generation. CoRR abs/2406.12534 (2024).
https://doi.org/10.48550/ARXIV.2406.12534 arXiv:2406.12534
[49] Qinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu
Huang, Zhangyue Yin, Kai Chen, and Xipeng Qiu. 2023. Evaluating Hallucinations in Chinese Large Language Models.
arXiv:2310.03368 [cs.CL]
[50] I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei
Liu, et al. 2023. FacTool: Factuality Detection in Generative AI–A Tool Augmented Framework for Multi-Task and
Multi-Domain Scenarios. ArXiv preprint abs/2307.13528 (2023). https://arxiv.org/abs/2307.13528
[51] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations?
ArXiv preprint abs/2305.01937 (2023). https://arxiv.org/abs/2305.01937
[52] David Chiang and Peter Cholak. 2022. Overcoming a Theoretical Limitation of Self-Attention. In Proceedings of
the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for
Computational Linguistics, Dublin, Ireland, 7654–7664. https://doi.org/10.18653/v1/2022.acl-long.527
[53] Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. 2023. KCTS: Knowledge-Constrained Tree Search
Decoding with Token-Level Hallucination Detection. ArXiv preprint abs/2310.09044 (2023). https://arxiv.org/abs/
2310.09044
[54] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua
Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,
Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,
Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan
Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai,
Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,
Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling Language Modeling with Pathways. J. Mach. Learn.
Res. 24 (2023), 240:1–240:113. http://jmlr.org/papers/v24/22-1144.html
[55] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement
Learning from Human Preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 4299–4307.
https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html
[56] Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, and
Bing Qin. 2024. BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question
Answering. arXiv:2406.19820 [cs.CL] https://arxiv.org/abs/2406.19820
[57] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing
Qin, and Ting Liu. 2023. A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. ArXiv preprint
abs/2309.15402 (2023). https://arxiv.org/abs/2309.15402

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:38

Huang, et al.

[58] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin,
and Ting Liu. 2023. A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. CoRR abs/2309.15402
(2023). https://doi.org/10.48550/ARXIV.2309.15402 arXiv:2309.15402
[59] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding
by contrasting layers improves factuality in large language models. ArXiv preprint abs/2309.03883 (2023). https:
//arxiv.org/abs/2309.03883
[60] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. ArXiv preprint abs/2210.11416
(2022). https://arxiv.org/abs/2210.11416
[61] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry
Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve
Math Word Problems. CoRR abs/2110.14168 (2021). arXiv:2110.14168 https://arxiv.org/abs/2110.14168
[62] Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. LM vs LM: Detecting Factual Errors via Cross
Examination. ArXiv preprint abs/2305.13281 (2023). https://arxiv.org/abs/2305.13281
[63] Together Computer. 2023. RedPajama: an Open Dataset for Training Large Language Models. https://github.com/
togethercomputer/RedPajama-Data
[64] Ajeya Cotra. 2021. Why AI alignment could be hard with modern deep learning. https://www.cold-takes.com/whyai-alignment-could-be-hard-with-modern-deep-learning/ Cold Takes.
[65] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek,
Nicola Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. CoRR
abs/2401.14887 (2024). https://doi.org/10.48550/ARXIV.2401.14887 arXiv:2401.14887
[66] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge Neurons in Pretrained
Transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, Dublin, Ireland, 8493–8502. https://doi.org/10.18653/v1/2022.acllong.581
[67] Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, Qiaoqiao She, and Zhifang Sui. 2022. Neural knowledge bank
for pretrained transformers. ArXiv preprint abs/2208.00399 (2022). https://arxiv.org/abs/2208.00399
[68] Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, and Jun Xu. 2023. LLMs may
Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. CoRR abs/2310.20501
(2023). https://doi.org/10.48550/ARXIV.2310.20501 arXiv:2310.20501
[69] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne
Liu. 2020. Plug and Play Language Models: A Simple Approach to Controlled Text Generation. In 8th International
Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https:
//openreview.net/forum?id=H1edEyBKDS
[70] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
Online and Punta Cana, Dominican Republic, 6491–6506. https://doi.org/10.18653/v1/2021.emnlp-main.522
[71] Maria Angels de Luis Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estevão Filho, Todd
Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha,
Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra. 2024. RAG vs Fine-tuning: Pipelines,
Tradeoffs, and a Case Study on Agriculture. CoRR abs/2401.08406 (2024). https://doi.org/10.48550/ARXIV.2401.08406
arXiv:2401.08406
[72] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi
Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. 2023. Language Modeling Is Compression.
ArXiv preprint abs/2309.10668 (2023). https://arxiv.org/abs/2309.10668
[73] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association
for Computational Linguistics, 4171–4186. https://doi.org/10.18653/V1/N19-1423
[74] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston.
2023. Chain-of-Verification Reduces Hallucination in Large Language Models. ArXiv preprint abs/2309.11495 (2023).
https://arxiv.org/abs/2309.11495
[75] Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, and Xueqi Cheng. 2024. Retrieve Only When It Needs:
Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models. CoRR abs/2402.10612
(2024). https://doi.org/10.48550/ARXIV.2402.10612 arXiv:2402.10612

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:39

[76] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023. BAMBOO: A Comprehensive Benchmark
for Evaluating Long Text Modeling Capacities of Large Language Models. ArXiv preprint abs/2309.13345 (2023).
https://arxiv.org/abs/2309.13345
[77] Esin Durmus, He He, and Mona Diab. 2020. FEQA: A Question Answering Evaluation Framework for Faithfulness
Assessment in Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 5055–5070. https://doi.org/10.18653/v1/2020.aclmain.454
[78] Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose. 2021. Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. In Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic,
2197–2214. https://doi.org/10.18653/v1/2021.emnlp-main.168
[79] Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems:
The begin benchmark. ArXiv preprint abs/2105.00071 (2021). https://arxiv.org/abs/2105.00071
[80] Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QA-Based Factual
Consistency Evaluation for Summarization. In Proceedings of the 2022 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics,
Seattle, United States, 2587–2601. https://doi.org/10.18653/v1/2022.naacl-main.187
[81] Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021.
SummEval: Re-evaluating Summarization Evaluation. Transactions of the Association for Computational Linguistics 9
(2021), 391–409. https://doi.org/10.1162/tacl_a_00373
[82] Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking Generated
Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference. In Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,
Florence, Italy, 2214–2220. https://doi.org/10.18653/v1/P19-1213
[83] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long Form
Question Answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Florence, Italy, 3558–3567. https://doi.org/10.18653/v1/P19-1346
[84] Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical Neural Story Generation. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational
Linguistics, Melbourne, Australia, 889–898. https://doi.org/10.18653/v1/P18-1082
[85] Huawen Feng, Yan Fan, Xiong Liu, Ting-En Lin, Zekun Yao, Yuchuan Wu, Fei Huang, Yongbin Li, and Qianli
Ma. 2023. Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension
and Embellishment Abilities of LLMs. CoRR abs/2310.19347 (2023). https://doi.org/10.48550/ARXIV.2310.19347
arXiv:2310.19347
[86] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Cook: Empowering general-purpose language models with modular and collaborative knowledge. arXiv preprint arXiv:2305.09955
(2023).
[87] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023. Retrieval-Generation Synergy
Augmented Large Language Models. ArXiv preprint abs/2310.05149 (2023). https://arxiv.org/abs/2310.05149
[88] Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao, Joshua Maynez, Shashi Narayan, and Mirella
Lapata. 2024. Learning to Plan and Generate Text with Citations. CoRR abs/2404.03381 (2024). https://doi.org/10.
48550/ARXIV.2404.03381 arXiv:2404.03381
[89] Katja Filippova. 2020. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data. In Findings of the
Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 864–870.
https://doi.org/10.18653/v1/2020.findings-emnlp.76
[90] Robert Friel and Atindriyo Sanyal. 2023. Chainpoll: A high efficacy method for LLM hallucination detection.
arXiv:2310.18344 [cs.CL]
[91] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTScore: Evaluate as You Desire. CoRR
abs/2302.04166 (2023). https://doi.org/10.48550/ARXIV.2302.04166 arXiv:2302.04166
[92] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a Bayesian Approximation: Representing Model Uncertainty in
Deep Learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City,
NY, USA, June 19-24, 2016 (JMLR Workshop and Conference Proceedings, Vol. 48), Maria-Florina Balcan and Kilian Q.
Weinberger (Eds.). JMLR.org, 1050–1059. http://proceedings.mlr.press/v48/gal16.html
[93] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish
Thite, Noa Nabeshima, et al. 2021. The pile: An 800gb dataset of diverse text for language modeling. ArXiv preprint
abs/2101.00027 (2021). https://arxiv.org/abs/2101.00027

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:40

Huang, et al.

[94] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni
Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. RARR: Researching and Revising What Language Models
Say, Using Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki
Okazaki (Eds.). Association for Computational Linguistics, 16477–16508. https://aclanthology.org/2023.acl-long.910
[95] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization
evaluation with chatgpt. ArXiv preprint abs/2304.02554 (2023). https://arxiv.org/abs/2304.02554
[96] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings.
CoRR abs/2104.08821 (2021). arXiv:2104.08821 https://arxiv.org/abs/2104.08821
[97] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-REC: Towards
Interactive and Explainable LLMs-Augmented Recommender System. CoRR abs/2303.14524 (2023). https://doi.org/10.
48550/ARXIV.2303.14524 arXiv:2303.14524
[98] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does
Fine-Tuning LLMs on New Knowledge Encourage Hallucinations? CoRR abs/2405.05904 (2024). https://doi.org/10.
48550/ARXIV.2405.05904 arXiv:2405.05904
[99] Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing The Factual Accuracy of Generated
Text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD
2019, Anchorage, AK, USA, August 4-8, 2019, Ankur Teredesai, Vipin Kumar, Ying Li, Rómer Rosales, Evimaria Terzi,
and George Karypis (Eds.). ACM, 166–175. https://doi.org/10.1145/3292500.3330955
[100] Google. 2023. Bard. https://bard.google.com/
[101] Tanya Goyal and Greg Durrett. 2020. Evaluating Factuality in Generation with Dependency-level Entailment. In
Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics,
Online, 3592–3603. https://doi.org/10.18653/v1/2020.findings-emnlp.322
[102] Tanya Goyal and Greg Durrett. 2021. Annotating and Modeling Fine-grained Factuality in Summarization. In
Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. Association for Computational Linguistics, Online, 1449–1462. https://doi.org/10.
18653/v1/2021.naacl-main.114
[103] Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Jiaming Wu, Heng Gong, and Bing Qin. 2022. Improving Controllable Text
Generation with Position-Aware Weighted Decoding. In Findings of the Association for Computational Linguistics: ACL
2022. Association for Computational Linguistics, Dublin, Ireland, 3449–3467. https://doi.org/10.18653/v1/2022.findingsacl.272
[104] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang,
Yaser Yacoob, et al. 2023. Hallusionbench: An advanced diagnostic suite for entangled language hallucination &
visual illusion in large vision-language models. arXiv preprint arXiv:2310.14566 (2023).
[105] Nuno Miguel Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and
André F. T. Martins. 2023. Hallucinations in Large Multilingual Translation Models. ArXiv preprint abs/2303.16104
(2023). https://arxiv.org/abs/2303.16104
[106] Nuno M. Guerreiro, Elena Voita, and André Martins. 2023. Looking for a Needle in a Haystack: A Comprehensive
Study of Hallucinations in Neural Machine Translation. In Proceedings of the 17th Conference of the European Chapter
of the Association for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia,
1059–1075. https://aclanthology.org/2023.eacl-main.75
[107] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan
Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks Are All You Need. ArXiv preprint
abs/2306.11644 (2023). https://arxiv.org/abs/2306.11644
[108] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2023. Detecting and preventing hallucinations in large vision language
models. ArXiv preprint abs/2308.06394 (2023). https://arxiv.org/abs/2308.06394
[109] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval Augmented Language
Model Pre-Training. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 3929–3938. http://proceedings.mlr.
press/v119/guu20a.html
[110] Bikash Gyawali, Lucas Anastasiou, and Petr Knoth. 2020. Deduplication of Scholarly Documents using Locality
Sensitive Hashing and Word Embeddings. In Proceedings of the Twelfth Language Resources and Evaluation Conference.
European Language Resources Association, Marseille, France, 901–910. https://aclanthology.org/2020.lrec-1.113
[111] Michael Hahn. 2020. Theoretical Limitations of Self-Attention in Neural Sequence Models. Transactions of the
Association for Computational Linguistics 8 (2020), 156–171. https://doi.org/10.1162/tacl_a_00306
[112] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang. 2024. The
Instinctive Bias: Spurious Images lead to Hallucination in MLLMs. arXiv preprint arXiv:2402.03757 (2024).

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:41

[113] Hangfeng He, Hongming Zhang, and Dan Roth. 2023. Rethinking with retrieval: Faithful large language model
inference. ArXiv preprint abs/2301.00303 (2023). https://arxiv.org/abs/2301.00303
[114] Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin
Zhang, Zejian Xie, and Jiaxing Zhang. 2023. Never Lost in the Middle: Improving Large Language Models via Attention
Strengthening Question Answering. CoRR abs/2311.09198 (2023). https://doi.org/10.48550/ARXIV.2311.09198
arXiv:2311.09198
[115] Peter Henderson, Mark S. Krass, Lucia Zheng, Neel Guha, Christopher D. Manning, Dan Jurafsky, and Daniel E. Ho.
2022. Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. In
Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed,
A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/
bc218a0c656e49d4b086975a9c785f47-Abstract-Datasets_and_Benchmarks.html
[116] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
Measuring Massive Multitask Language Understanding. In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=d7KBjmI3GmQ
[117] Evan Hernandez, Belinda Z Li, and Jacob Andreas. 2023. Inspecting and editing knowledge representations in language
models. ArXiv preprint abs/2304.00740 (2023). https://arxiv.org/abs/2304.00740
[118] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case of Neural Text Degeneration.
In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
OpenReview.net. https://openreview.net/forum?id=rygGQyrFvH
[119] Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. 𝑄 2 : Evaluating
Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics, Online and Punta Cana, Dominican Republic, 7856–7870. https://doi.org/10.18653/v1/2021.emnlpmain.619
[120] Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng,
Duyu Tang, Dandan Tu, and Bing Qin. 2024. Learning Fine-Grained Grounded Citations for Attributed Large Language
Models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting,
August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,
14095–14113. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.838
[121] Lei Huang, Xiaocheng Feng, Weitao Ma, Liang Zhao, Yuchun Fan, Weihong Zhong, Dongliang Xu, Qing Yang,
Hongtao Liu, and Bing Qin. 2024. Advancing Large Language Model Attribution through Self-Improving.
arXiv:2410.13298 [cs.CL] https://arxiv.org/abs/2410.13298
[122] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and
Nenghai Yu. 2023. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and
retrospection-allocation. arXiv preprint arXiv:2311.17911 (2023).
[123] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan
Mohammed, Qiang Liu, et al. 2023. Language is not all you need: Aligning perception with language models. ArXiv
preprint abs/2302.14045 (2023). https://arxiv.org/abs/2302.14045
[124] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv,
Yikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation
models. ArXiv preprint abs/2305.08322 (2023). https://arxiv.org/abs/2305.08322
[125] Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng, and Bing Qin. 2021. The Factual Inconsistency Problem in
Abstractive Text Summarization: A Survey. CoRR abs/2104.14839 (2021). arXiv:2104.14839 https://arxiv.org/abs/2104.
14839
[126] Yi-Chong Huang, Xia-Chong Feng, Xiao-Cheng Feng, and Bing Qin. 2021. The Factual Inconsistency Problem in
Abstractive Text Summarization: A Survey. ArXiv preprint abs/2104.14839 (2021). https://arxiv.org/abs/2104.14839
[127] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. 2023. Transformer-Patcher:
One Mistake Worth One Neuron. In The Eleventh International Conference on Learning Representations, ICLR 2023,
Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/pdf?id=4oYUGeGBPm
[128] Siqing Huo, Negar Arabzadeh, and Charles L. A. Clarke. 2023. Retrieving Supporting Evidence for LLMs Generated
Answers. ArXiv preprint abs/2306.13781 (2023). https://arxiv.org/abs/2306.13781
[129] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu
Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through
the lens of generalization. ArXiv preprint abs/2212.12017 (2022). https://arxiv.org/abs/2212.12017
[130] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard
Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Trans. Mach. Learn. Res. 2022

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:42

Huang, et al.

(2022). https://openreview.net/forum?id=jKN1pXi7b0
[131] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,
Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented
Language Models. J. Mach. Learn. Res. 24 (2023), 251:1–251:43. http://jmlr.org/papers/v24/23-0037.html
[132] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query Expansion by Prompting Large Language Models. CoRR abs/2305.03653 (2023). https://doi.org/10.48550/ARXIV.2305.03653 arXiv:2305.03653
[133] Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig,
and Chunting Zhou. 2023. Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. ArXiv
preprint abs/2306.01200 (2023). https://arxiv.org/abs/2306.01200
[134] Joonhyun Jeong. 2023. Hijacking Context in Large Multi-modal Models. arXiv preprint arXiv:2312.07553 (2023).
[135] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. 2024. Adaptive-RAG: Learning to
Adapt Retrieval-Augmented Large Language Models through Question Complexity. CoRR abs/2403.14403 (2024).
https://doi.org/10.48550/ARXIV.2403.14403 arXiv:2403.14403
[136] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and
Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12 (2023),
248:1–248:38. https://doi.org/10.1145/3571730
[137] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards Mitigating Hallucination
in Large Language Models via Self-Reflection. ArXiv preprint abs/2310.06271 (2023). https://arxiv.org/abs/2310.06271
[138] Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. 2024.
Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models.
arXiv preprint arXiv:2402.15721 (2024).
[139] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LLMLingua: Compressing Prompts for
Accelerated Inference of Large Language Models. arXiv:2310.05736 [cs.CL]
[140] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and
Graham Neubig. 2023. Active Retrieval Augmented Generation. ArXiv preprint abs/2305.06983 (2023). https:
//arxiv.org/abs/2305.06983
[141] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. 2023. Faithscore: Evaluating hallucinations in
large vision-language models. arXiv preprint arXiv:2311.01477 (2023).
[142] Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, and Min Zhang. 2024. When Large
Language Models Meet Vector Databases: A Survey. CoRR abs/2402.01763 (2024). https://doi.org/10.48550/ARXIV.
2402.01763 arXiv:2402.01763
[143] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac
Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know.
ArXiv preprint abs/2207.05221 (2022). https://arxiv.org/abs/2207.05221
[144] Greg Kamradt. 2024. The 5 Levels Of Text Splitting For Retrieval. youtube. https://www.youtube.com/watch?v=
8OJC21T2SL4
[145] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large Language Models
Struggle to Learn Long-Tail Knowledge. In International Conference on Machine Learning, ICML 2023, 23-29 July
2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 15696–15707. https:
//proceedings.mlr.press/v202/kandpal23a.html
[146] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau
Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online,
6769–6781. https://doi.org/10.18653/v1/2020.emnlp-main.550
[147] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie
Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 6769–6781. https:
//doi.org/10.18653/V1/2020.EMNLP-MAIN.550
[148] Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A
Smith, Yejin Choi, and Kentaro Inui. 2022. RealTime QA: What’s the Answer Right Now? ArXiv preprint abs/2207.13332
(2022). https://arxiv.org/abs/2207.13332
[149] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023. Gpt-4 passes the bar exam.
Available at SSRN 4389233 (2023). https://www.datascienceassn.org/sites/default/files/GPT-4%20Passes%20the%
20Bar%20Exam.pdf

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:43

[150] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through
Memorization: Nearest Neighbor Language Models. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=HklBjCEKvH
[151] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models
are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 22199–22213.
[152] Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the Factual Consistency
of Abstractive Text Summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP). Association for Computational Linguistics, Online, 9332–9346. https://doi.org/10.18653/v1/2020.
emnlp-main.750
[153] Philippe Laban, Wojciech Kryściński, Divyansh Agarwal, Alexander R Fabbri, Caiming Xiong, Shafiq Joty, and
Chien-Sheng Wu. 2023. LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. ArXiv preprint
abs/2305.14540 (2023). https://arxiv.org/abs/2305.14540
[154] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-Visiting NLI-based Models
for Inconsistency Detection in Summarization. Transactions of the Association for Computational Linguistics 10 (2022),
163–177. https://doi.org/10.1162/tacl_a_00453
[155] Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto.
2023. When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization. In
Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. Association
for Computational Linguistics, Dubrovnik, Croatia, 3206–3219. https://aclanthology.org/2023.eacl-main.234
[156] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and Scalable Predictive Uncertainty
Estimation using Deep Ensembles. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 6402–6413.
https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html
[157] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li,
Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph,
Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang,
Thomas Henighan, Timothy Maxwell, Timothy Telleen-Lawton, Tristan Hume, Zac Hatfield-Dodds, Jared Kaplan,
Jan Brauner, Samuel R. Bowman, and Ethan Perez. 2023. Measuring Faithfulness in Chain-of-Thought Reasoning.
CoRR abs/2307.13702 (2023). https://doi.org/10.48550/ARXIV.2307.13702 arXiv:2307.13702
[158] Barrett Martin Lattimer, Patrick Chen, Xinyuan Zhang, and Yi Yang. 2023. Fast and Accurate Factual Inconsistency
Detection Over Long Documents. arXiv:2310.13189 [cs.CL]
[159] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas
Carlini. 2022. Deduplicating Training Data Makes Language Models Better. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,
Dublin, Ireland, 8424–8445. https://doi.org/10.18653/v1/2022.acl-long.577
[160] Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022.
Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing
Systems 35 (2022), 34586–34599.
[161] Deren Lei, Yaxi Li, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal, et al. 2023. Chain of Natural Language
Inference for Reducing Large Language Model Ungrounded Hallucinations. ArXiv preprint abs/2310.03951 (2023).
https://arxiv.org/abs/2310.03951
[162] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint
arXiv:2311.16922 (2023).
[163] BA Levinstein and Daniel A Herrmann. 2023. Still No Lie Detector for Language Models: Probing Empirical and
Conceptual Roadblocks. ArXiv preprint abs/2307.00175 (2023). https://arxiv.org/abs/2307.00175
[164] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,
and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, Online, 7871–7880. https://doi.org/10.18653/v1/2020.aclmain.703
[165] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:44

Huang, et al.

cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html
[166] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X. Yu, and Sanjiv Kumar.
2023. Large Language Models with Controllable Working Memory. In Findings of the Association for Computational
Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki
(Eds.). Association for Computational Linguistics, 1774–1793. https://doi.org/10.18653/v1/2023.findings-acl.112
[167] Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao. 2024. Towards Faithful Chain-of-Thought: Large
Language Models are Bridging Reasoners. CoRR abs/2405.18915 (2024). https://doi.org/10.48550/ARXIV.2405.18915
arXiv:2405.18915
[168] Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. The Dawn
After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. CoRR abs/2401.03205
(2024). https://doi.org/10.48550/ARXIV.2401.03205 arXiv:2401.03205
[169] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination
Evaluation Benchmark for Large Language Models. CoRR abs/2305.11747 (2023). https://doi.org/10.48550/ARXIV.
2305.11747 arXiv:2305.11747
[170] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. ArXiv preprint abs/2301.12597 (2023). https://arxiv.org/abs/2301.
12597
[171] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: A Generative
Framework for Personalized Recommendation and User Interests Interpretation. In Proceedings of the 2023 SIGIR
Workshop on eCommerce co-located with the 46th International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR 2023), Taipei, Taiwan, July 27, 2023 (CEUR Workshop Proceedings, Vol. 3589), Surya
Kallumadi, Yubin Kim, Tracy Holloway King, Shervin Malmasi, Maarten de Rijke, and Jacopo Tagliabue (Eds.).
CEUR-WS.org. https://ceur-ws.org/Vol-3589/paper_2.pdf
[172] Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inference-Time Intervention:
Eliciting Truthful Answers from a Language Model. ArXiv preprint abs/2306.03341 (2023). https://arxiv.org/abs/2306.
03341
[173] Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024. Nearest
Neighbor Speculative Decoding for LLM Generation and Attribution. CoRR abs/2405.19325 (2024). https://doi.org/10.
48550/ARXIV.2405.19325 arXiv:2405.19325
[174] Wei Li, Wenhao Wu, Moye Chen, Jiachen Liu, Xinyan Xiao, and Hua Wu. 2022. Faithfulness in Natural Language
Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods. ArXiv preprint abs/2203.05227
(2022). https://arxiv.org/abs/2203.05227
[175] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and
Mike Lewis. 2022. Contrastive decoding: Open-ended text generation as optimization. ArXiv preprint abs/2210.15097
(2022). https://arxiv.org/abs/2210.15097
[176] Yucheng Li. 2023. Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with SelfInformation-Based Content Filtering. arXiv:2304.12102 [cs.CL]
[177] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks
Are All You Need II: phi-1.5 technical report. ArXiv preprint abs/2309.05463 (2023). https://arxiv.org/abs/2309.05463
[178] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating Object Hallucination
in Large Vision-Language Models. https://arxiv.org/abs/2305.10355
[179] Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. 2023. ChatDoctor: A Medical Chat Model Fine-tuned
on LLaMA Model using Medical Domain Knowledge. ArXiv preprint abs/2303.14070 (2023). https://arxiv.org/abs/
2303.14070
[180] Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, and Dongjie Yang. 2023. BatGPT: A Bidirectional Autoregessive Talker
from Generative Pre-trained Transformer. ArXiv preprint abs/2307.00360 (2023). https://arxiv.org/abs/2307.00360
[181] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches
Out. Association for Computational Linguistics, Barcelona, Spain, 74–81. https://aclanthology.org/W04-1013
[182] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods.
In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Association for Computational Linguistics, Dublin, Ireland, 3214–3252. https://doi.org/10.18653/v1/2022.acl-long.229
[183] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023. Exposing Attention Glitches
with Flip-Flop Language Modeling. ArXiv preprint abs/2306.00946 (2023). https://arxiv.org/abs/2306.00946
[184] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023.
HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark
Challenging for GPT-4V(Ision), LLaVA-1.5, and Other Multi-Modality Models. https://arxiv.org/abs/2310.14566

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:45

[185] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating Hallucination in
Large Multi-Modal Models via Robust Instruction Tuning. arXiv:2306.14565 [cs.CV]
[186] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. ArXiv preprint
abs/2304.08485 (2023). https://arxiv.org/abs/2304.08485
[187] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei
Peng. 2024. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253 (2024).
[188] Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023. TCRA-LLM: Token Compression Retrieval
Augmented Large Language Model for Inference Cost Reduction. In Findings of the Association for Computational
Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association
for Computational Linguistics, 9796–9810. https://aclanthology.org/2023.findings-emnlp.655
[189] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
2023. Lost in the Middle: How Language Models Use Long Contexts. ArXiv preprint abs/2307.03172 (2023). https:
//arxiv.org/abs/2307.03172
[190] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation
using gpt-4 with better human alignment. ArXiv preprint abs/2303.16634 (2023). https://arxiv.org/abs/2303.16634
[191] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,
and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692
(2019). arXiv:1907.11692 http://arxiv.org/abs/1907.11692
[192] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language
Models’ Alignment. ArXiv preprint abs/2308.05374 (2023). https://arxiv.org/abs/2308.05374
[193] Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023. Instruction Position Matters in Sequence Generation
with Large Language Models. ArXiv preprint abs/2308.12097 (2023). https://arxiv.org/abs/2308.12097
[194] Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. EntityBased Knowledge Conflicts in Question Answering. In Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021,
Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational
Linguistics, 7052–7063. https://doi.org/10.18653/V1/2021.EMNLP-MAIN.565
[195] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. 2023. Negative Object Presence
Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models. https://arxiv.org/abs/2310.05338
[196] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. 2023.
Evaluation and Mitigation of Agnosia in Multimodal Large Language Models. ArXiv preprint abs/2309.04041 (2023).
https://arxiv.org/abs/2309.04041
[197] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou.
2020. Univl: A unified video and language pre-training model for multimodal understanding and generation. ArXiv
preprint abs/2002.06353 (2020). https://arxiv.org/abs/2002.06353
[198] Junyu Luo, Cao Xiao, and Fenglong Ma. 2023. Zero-Resource Hallucination Prevention for Large Language Models.
ArXiv preprint abs/2309.02654 (2023). https://arxiv.org/abs/2309.02654
[199] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for text
summarization.
[200] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting for Retrieval-Augmented
Large Language Models. CoRR abs/2305.14283 (2023). https://doi.org/10.48550/ARXIV.2305.14283 arXiv:2305.14283
[201] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-ChatGPT: Towards
Detailed Video Understanding via Large Vision and Language Models. ArXiv preprint abs/2306.05424 (2023). https:
//arxiv.org/abs/2306.05424
[202] Fiona Macpherson and Dimitris Platchias. 2013.
Hallucination: Philosophy and psychology.
MIT
Press. https://books.google.com/books?hl=zh-CN&lr=&id=_bwtAAAAQBAJ&oi=fnd&pg=PR5&dq=Hallucination:
+Philosophy+and+psychology&ots=2E62kf7_yC&sig=rH9HGXYacNkxOJNMVbw514aChZo
[203] Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2023. ExpertQA: ExpertCurated Questions and Attributed Answers. arXiv:2309.07852 [cs.CL]
[204] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to
Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational
Linguistics, 9802–9822. https://doi.org/10.18653/v1/2023.acl-long.546
[205] Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination
Detection for Generative Large Language Models. ArXiv preprint abs/2303.08896 (2023). https://arxiv.org/abs/2303.

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:46

Huang, et al.

08896
[206] Udi Manber and Gene Myers. 1993. Suffix arrays: a new method for on-line string searches. siam Journal on Computing
22, 5 (1993), 935–948.
[207] Shengyu Mao, Yong Jiang, Boli Chen, Xiao Li, Peng Wang, Xinyu Wang, Pengjun Xie, Fei Huang, Huajun Chen, and
Ningyu Zhang. 2024. RaFe: Ranking Feedback Improves Query Rewriting for RAG. CoRR abs/2405.14431 (2024).
https://doi.org/10.48550/ARXIV.2405.14431 arXiv:2405.14431
[208] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On Faithfulness and Factuality in
Abstractive Summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Online, 1906–1919. https://doi.org/10.18653/v1/2020.acl-main.173
[209] Clara Meister, Ryan Cotterell, and Tim Vieira. 2020. If beam search is the answer, what was the question?. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for
Computational Linguistics, Online, 2173–2185. https://doi.org/10.18653/v1/2020.emnlp-main.170
[210] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and Editing Factual Associations in
GPT. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/6f1d43d5a82a37e89b0665b33bf3a182-AbstractConference.html
[211] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. 2023. Mass-Editing Memory in
a Transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May
1-5, 2023. OpenReview.net. https://openreview.net/pdf?id=MkbcAHIYgyS
[212] Mengqi Miao, Fandong Meng, Yijin Liu, Xiao-Hua Zhou, and Jie Zhou. 2021. Prevent the Language Model from
being Overconfident in Neural Machine Translation. In Proceedings of the 59th Annual Meeting of the Association for
Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers). Association for Computational Linguistics, Online, 3456–3468. https://doi.org/10.18653/v1/2021.acl-long.268
[213] Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step
reasoning. ArXiv preprint abs/2308.00436 (2023). https://arxiv.org/abs/2308.00436
[214] Microsoft. 2023. New Bing. https://www.bing.com/new
[215] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. 2023.
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. CoRR abs/2308.04430 (2023). https:
//doi.org/10.48550/ARXIV.2308.04430 arXiv:2308.04430
[216] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer,
and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text
Generation. ArXiv preprint abs/2305.14251 (2023). https://arxiv.org/abs/2305.14251
[217] Anshuman Mishra, Dhruvesh Patel, Aparna Vijayakumar, Xiang Lorraine Li, Pavan Kapanipathi, and Kartik Talamadupula. 2021. Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text
Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, Online, 1322–1336.
https://doi.org/10.18653/v1/2021.naacl-main.104
[218] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. 2022. Fast Model Editing at
Scale. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.
OpenReview.net. https://openreview.net/forum?id=0DcZxeWfOPt
[219] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. 2022. Memory-Based Model
Editing at Scale. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA
(Proceedings of Machine Learning Research, Vol. 162), Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári,
Gang Niu, and Sivan Sabato (Eds.). PMLR, 15817–15831. https://proceedings.mlr.press/v162/mitchell22a.html
[220] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola.
2022. Relative representations enable zero-shot latent space communication. ArXiv preprint abs/2209.15430 (2022).
https://arxiv.org/abs/2209.15430
[221] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela.
2024. Generative Representational Instruction Tuning. CoRR abs/2402.09906 (2024). https://doi.org/10.48550/ARXIV.
2402.09906 arXiv:2402.09906
[222] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2023. MTEB: Massive Text Embedding Benchmark.
In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023,
Dubrovnik, Croatia, May 2-6, 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational
Linguistics, 2006–2029. https://doi.org/10.18653/V1/2023.EACL-MAIN.148
[223] Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown,
Amnon Shashua, and Yoav Shoham. 2023. Generating benchmarks for factuality evaluation of language models.
ArXiv preprint abs/2307.06908 (2023). https://arxiv.org/abs/2307.06908

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:47

[224] Inderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikrishna Karanam, and
Sumit Shekhar. 2023. A Neural CRF-based Hierarchical Approach for Linear Text Segmentation. In Findings of the
Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, Andreas Vlachos and Isabelle
Augenstein (Eds.). Association for Computational Linguistics, 853–863. https://doi.org/10.18653/V1/2023.FINDINGSEACL.65
[225] Feng Nan, Ramesh Nallapati, Zhiguo Wang, Cicero Nogueira dos Santos, Henghui Zhu, Dejiao Zhang, Kathleen
McKeown, and Bing Xiang. 2021. Entity-level Factual Consistency of Abstractive Text Summarization. In Proceedings of
the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. Association
for Computational Linguistics, Online, 2727–2733. https://doi.org/10.18653/v1/2021.eacl-main.235
[226] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, and Shomir Wilson. 2023.
Nationality Bias in Text Generation. In Proceedings of the 17th Conference of the European Chapter of the Association
for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia, 116–122. https:
//aclanthology.org/2023.eacl-main.9
[227] Ella Neeman, Roee Aharoni, Or Honovich, Leshem Choshen, Idan Szpektor, and Omri Abend. 2023. DisentQA:
Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering. In Proceedings of the
61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational
Linguistics, 10056–10070. https://doi.org/10.18653/V1/2023.ACL-LONG.559
[228] Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. When Do LLMs Need Retrieval Augmentation? Mitigating
LLMs’ Overconfidence Helps Retrieval Augmentation. CoRR abs/2402.11457 (2024). https://doi.org/10.48550/ARXIV.
2402.11457 arXiv:2402.11457
[229] Sean O’Brien and Mike Lewis. 2023. Contrastive Decoding Improves Reasoning in Large Language Models. ArXiv
preprint abs/2309.09117 (2023). https://arxiv.org/abs/2309.09117
[230] Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity Cloze By Date: What LMs Know
About Unseen Entities. In Findings of the Association for Computational Linguistics: NAACL 2022. Association for
Computational Linguistics, Seattle, United States, 693–702. https://doi.org/10.18653/v1/2022.findings-naacl.52
[231] OpenAI. 2022. Introducing chatgpt. https://openai.com/blog/chatgpt
[232] OpenAI. 2023. GPT-4 Technical Report. ArXiv preprint abs/2303.08774 (2023). https://arxiv.org/abs/2303.08774
[233] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS. http://papers.nips.cc/paper_files/paper/2022/hash/
b1efde53be364a73914f58805a001731-Abstract-Conference.html
[234] Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. 2023. Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. CoRR abs/2312.05934 (2023). https://doi.org/10.48550/ARXIV.2312.05934 arXiv:2312.05934
[235] Lorenzo Pacchiardi, Alex J Chan, Sören Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal, Owain Evans, and Jan
Brauner. 2023. How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions. ArXiv
preprint abs/2309.15840 (2023). https://arxiv.org/abs/2309.15840
[236] Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding Factuality in Abstractive
Summarization with FRANK: A Benchmark for Factuality Metrics. In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for
Computational Linguistics, Online, 4812–4829. https://doi.org/10.18653/v1/2021.naacl-main.383
[237] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically
correcting large language models: Surveying the landscape of diverse self-correction strategies. ArXiv preprint
abs/2308.03188 (2023). https://arxiv.org/abs/2308.03188
[238] Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, and Le Sun. 2024. Not All
Contexts Are Equal: Teaching LLMs Credibility-aware Generation. CoRR abs/2404.06809 (2024). https://doi.org/10.
48550/ARXIV.2404.06809 arXiv:2404.06809
[239] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311–318. https://doi.org/10.3115/
1073083.1073135
[240] Ankur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan
Das. 2020. ToTTo: A Controlled Table-To-Text Generation Dataset. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, Bonnie Webber, Trevor Cohn,
Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 1173–1186. https://doi.org/10.18653/V1/
2020.EMNLP-MAIN.89

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:48

Huang, et al.

[241] Debjit Paul, Robert West, Antoine Bosselut, and Boi Faltings. 2024. Making Reasoning Matter: Measuring and
Improving Faithfulness of Chain-of-Thought Reasoning. CoRR abs/2402.13950 (2024). https://doi.org/10.48550/ARXIV.
2402.13950 arXiv:2402.13950
[242] Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. 2021. Data and its
(dis)contents: A survey of dataset development and use in machine learning research. Patterns 2, 11 (2021), 100336.
https://doi.org/10.1016/J.PATTER.2021.100336
[243] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM:
Outperforming Curated Corpora with Web Data, and Web Data Only. ArXiv preprint abs/2306.01116 (2023).
https://arxiv.org/abs/2306.01116
[244] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4.
ArXiv preprint abs/2304.03277 (2023). https://arxiv.org/abs/2304.03277
[245] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson,
Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron
McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson,
Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal
Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson
Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott
Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan,
Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny
Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023. Discovering Language Model
Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023,
Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for
Computational Linguistics, 13387–13434. https://doi.org/10.18653/V1/2023.FINDINGS-ACL.847
[246] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.
2019. Language Models as Knowledge Bases?. In Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).
Association for Computational Linguistics, Hong Kong, China, 2463–2473. https://doi.org/10.18653/v1/D19-1250
[247] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing
the compositionality gap in language models. ArXiv preprint abs/2210.03350 (2022). https://arxiv.org/abs/2210.03350
[248] Jirui Qi, Gabriele Sarti, Raquel Fernández, and Arianna Bisazza. 2024. Model Internals-based Answer Attribution for
Trustworthy Retrieval-Augmented Generation. CoRR abs/2406.13663 (2024). https://doi.org/10.48550/ARXIV.2406.
13663 arXiv:2406.13663
[249] Zhixiao Qi, Yijiong Yu, Meiqi Tu, Junyi Tan, and Yongfeng Huang. 2023. FoodGPT: A Large Language Model in Food
Testing Domain with Incremental Pre-training and Knowledge Graph Prompt. ArXiv preprint abs/2308.10173 (2023).
https://arxiv.org/abs/2308.10173
[250] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and
Huajun Chen. 2022. Reasoning with language model prompting: A survey. ArXiv preprint abs/2212.09597 (2022).
https://arxiv.org/abs/2212.09597
[251] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by
generative pre-training. (2018).
[252] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are
unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.
[253] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct
Preference Optimization: Your Language Model is Secretly a Reward Model. ArXiv preprint abs/2305.18290 (2023).
https://arxiv.org/abs/2305.18290
[254] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn.
Res. 21 (2020), 140:1–140:67. http://jmlr.org/papers/v21/20-074.html
[255] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
2023. In-Context Retrieval-Augmented Language Models. ArXiv preprint abs/2302.00083 (2023). https://arxiv.org/
abs/2302.00083
[256] Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with
Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto
Rico, May 2-4, 2016, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.
06732

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:49

[257] Mathieu Ravaut, Aixin Sun, Nancy F. Chen, and Shafiq Joty. 2024. On Context Utilization in Summarization with
Large Language Models. arXiv:2310.10570 [cs.CL]
[258] Vipula Rawte, Amit P. Sheth, and Amitava Das. 2023. A Survey of Hallucination in Large Foundation Models. ArXiv
preprint abs/2309.05922 (2023). https://arxiv.org/abs/2309.05922
[259] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu
Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud,
Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds,
Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson,
Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory
Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James
Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen
Spencer, Eren Sezener, Luke Vilnis, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman,
Nithya Attaluri, Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma,
Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala,
Martin Chadwick, Juliette Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai,
Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Yingjie Miao, Lukas
Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago
Ontañón, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta,
Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong,
Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu,
YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael
Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau,
Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May,
Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio
Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey
Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, Anja Hauth,
Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson,
Rohan Jain, Vinay Ramasesh, Anton Briukhov, Da-Woon Chung, Tamara von Glehn, Christina Butterfield, Priya
Jhakra, Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui
Wu, Paul Voigtlaender, Tara Sainath, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan,
Mark Omernick, Colin Gaffney, Gabriela Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas
Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant
Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin
Chiu, Rory Blevins, Zach Gleicher, Adrià Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek,
Séb Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas
Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek,
Kashyap Krishnakumar, Shuo yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir,
Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost van Amersfoort,
Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre
Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Sébastien Cevey, Jonas Adler,
Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Michael Chang, Samer Hassan, Diana Mincu,
Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan
Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo,
Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen,
Chulayuth Asawaroengchai, Ravi Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee,
Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma,
Mario Lučić, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjösund, Elena Buchatskaya, Elspeth
White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Jane Labanowski, Nicola De Cao, David Steiner,
Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant,
Anders Andreassen, Carlos Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine
Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Zhufeng Pan, Zachary Nado, Stephanie Winkler, Dian Yu,
Mohammad Saleh, Loren Maggiore, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir
Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram
Tariq, Disha Shrivastava, Fei Xia, Chung-Cheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Fred
Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen,
Ruibo Liu, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla,
Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:50

Huang, et al.

Rogozińska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang,
Dave Lacey, Anastasija Ilić, Yao Zhao, Lora Aroyo, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan,
Sadegh Jazayeri, Raphaël Lopez Kaufman, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David
Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp,
Nejc Trdin, Anitha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel,
Somer Greene, Anaïs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana
Kulizhskaya, Sonam Goenka, Brennan Saeta, Kiran Vodrahalli, Christian Frank, Dario de Cesare, Brona Robenek,
Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy,
Yelin Kim, Dinghua Li, Bill Rosgen, Zoe Ashwood, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan,
Hongkun Yu, Çağlar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang,
Ming Zhang, Rui Zhu, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier Dousse, Willi Gierke, Soheil Hassas Yeganeh,
Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu,
Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic,
Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi
Lam, Josef Broder, Dan Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Daniel Toyama, Mrinal Shukla, Sujoy
Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi,
Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, Félix de Chaumont Quitry,
Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq
Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi
Raad, Hannah Forbes, Anna Bulanova, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie
Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui
Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia,
Clement Farabet, Pedro Valenzuela, Quan Yuan, Chris Welty, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse,
Nandita Dukkipati, Adam Paszke, Andrew Bolt, Elnaz Davoodi, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha
Vashisht, Rebeca Santamaria-Fernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin
Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, Mohamed Elhawaty, Andrey Khorlin,
Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Alejandro Lince, Norman Casagrande, Jay Hoover,
Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa, Anna Koop, Praveen Kumar, Thibault
Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun,
Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian
Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar, Sanjay Ghemawat,
Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev,
Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn,
Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A.
Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber
Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel Kaed, Jing Li, Jakub Sygnowski, Shreyas Rammohan
Belle, Zhe Chen, Jaclyn Konzelmann, Siim Põder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne
Liu, Azade Nova, Jun Xu, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2024.
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530 [cs.CL]
[260] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng
Wang. 2023. Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation.
ArXiv preprint abs/2307.11019 (2023). https://arxiv.org/abs/2307.11019
[261] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng
Wang. 2023. Investigating the factual knowledge boundary of large language models with retrieval augmentation.
ArXiv preprint abs/2307.11019 (2023). https://arxiv.org/abs/2307.11019
[262] Reuters. 2023. U.S. Copyright Office says some AI-assisted works may be copyrighted. https://www.reuters.com/world/
us/us-copyright-office-says-some-ai-assisted-works-may-be-copyrighted-2023-03-15/
[263] Nina Rimsky. 2023. Modulating sycophancy in an RLHF model via activation steering. https://www.alignmentforum.
org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation
[264] Nina Rimsky. 2023. Reducing sycophancy and improving honesty via activation steering. https://www.alignmentforum.
org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation
[265] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. Can
AI-Generated Text be Reliably Detected? CoRR abs/2303.11156 (2023). https://doi.org/10.48550/ARXIV.2303.11156
arXiv:2303.11156
[266] Sashank Santhanam, Behnam Hedayatnia, Spandana Gella, Aishwarya Padmakumar, Seokhwan Kim, Yang Liu, and
Dilek Hakkani-Tur. 2021. Rome was built in 1776: A case study on factual correctness in knowledge-grounded
response generation. ArXiv preprint abs/2110.05456 (2021). https://arxiv.org/abs/2110.05456

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:51

[267] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024. RAPTOR:
Recursive Abstractive Processing for Tree-Organized Retrieval. CoRR abs/2401.18059 (2024). https://doi.org/10.48550/
ARXIV.2401.18059 arXiv:2401.18059
[268] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Selfcritiquing models for assisting human evaluators. ArXiv preprint abs/2206.05802 (2022). https://arxiv.org/abs/2206.
05802
[269] John Schulman. 2023. Reinforcement Learning from Human Feedback: Progress and Challenges. Berkeley EECS.
https://www.youtube.com/watch?v=hhiLw5Q_UFg
[270] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal Policy Optimization
Algorithms. ArXiv preprint abs/1707.06347 (2017). https://arxiv.org/abs/1707.06347
[271] Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick
Gallinari. 2021. QuestEval: Summarization Asks for Fact-based Evaluation. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana,
Dominican Republic, 6594–6604. https://doi.org/10.18653/v1/2021.emnlp-main.529
[272] Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. 2024. Assisting in
Writing Wikipedia-like Articles From Scratch with Large Language Models. CoRR abs/2402.14207 (2024). https:
//doi.org/10.48550/ARXIV.2402.14207 arXiv:2402.14207
[273] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing RetrievalAugmented Large Language Models with Iterative Retrieval-Generation Synergy. ArXiv preprint abs/2305.15294
(2023). https://arxiv.org/abs/2305.15294
[274] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng,
Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal
Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and Ethan Perez. 2023. Towards Understanding
Sycophancy in Language Models. ArXiv preprint abs/2310.13548 (2023). https://arxiv.org/abs/2310.13548
[275] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023. Trusting
Your Evidence: Hallucinate Less with Context-aware Decoding. ArXiv preprint abs/2305.14739 (2023). https:
//arxiv.org/abs/2305.14739
[276] Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer,
Scott Yih, and Mike Lewis. 2023. In-Context Pretraining: Language Modeling Beyond Document Boundaries. ArXiv
preprint abs/2310.10638 (2023). https://arxiv.org/abs/2310.10638
[277] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wentau Yih. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models. CoRR abs/2301.12652 (2023). https:
//doi.org/10.48550/ARXIV.2301.12652 arXiv:2301.12652
[278] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces
Hallucination in Conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021. Association for
Computational Linguistics, Punta Cana, Dominican Republic, 3784–3803. https://doi.org/10.18653/v1/2021.findingsemnlp.320
[279] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather
Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Andrew Mansfield,
Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Agüera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong,
Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R. Webster, Gregory S. Corrado, Yossi Matias, Shekoofeh
Azizi, Alan Karthikesalingam, and Vivek Natarajan. 2023. Towards Expert-Level Medical Question Answering with
Large Language Models. ArXiv preprint abs/2305.09617 (2023). https://arxiv.org/abs/2305.09617
[280] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitriy Pyrkin, Sergei Popov, and Artem Babenko. 2020. Editable Neural
Networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net. https://openreview.net/forum?id=HJedXaEtvS
[281] Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, and Shauli Ravfogel. 2023. The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models. ArXiv preprint
abs/2310.11877 (2023). https://arxiv.org/abs/2310.11877
[282] Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, and Ido Dagan. 2024. Attribute First, then Generate: Locallyattributable Grounded Text Generation. CoRR abs/2403.17104 (2024). https://doi.org/10.48550/ARXIV.2403.17104
arXiv:2403.17104
[283] Felix Stahlberg and Bill Byrne. 2019. On NMT Search Errors and Model Errors: Cat Got Your Tongue?. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China,
3356–3362. https://doi.org/10.18653/v1/D19-1331

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:52

Huang, et al.

[284] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form
Answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, Abu Dhabi, United Arab Emirates, 8273–8288. https://aclanthology.org/2022.emnlpmain.566
[285] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,
and Paul F. Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information
Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual, Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).
https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html
[286] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Haisu Liu, Quan Shi, Zachary S.
Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan O. Arik, Danqi Chen, and Tao Yu. 2024. BRIGHT: A Realistic and
Challenging Benchmark for Reasoning-Intensive Retrieval. arXiv:2407.12883 [cs.CL] https://arxiv.org/abs/2407.12883
[287] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. RoFormer: Enhanced Transformer with Rotary
Position Embedding. CoRR abs/2104.09864 (2021). arXiv:2104.09864 https://arxiv.org/abs/2104.09864
[288] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented
Generation based on the Real-time Information Needs of Large Language Models. CoRR abs/2403.10081 (2024).
https://doi.org/10.48550/ARXIV.2403.10081 arXiv:2403.10081
[289] Nishant Subramani, Nivedita Suresh, and Matthew Peters. 2022. Extracting Latent Steering Vectors from Pretrained
Language Models. In Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational
Linguistics, Dublin, Ireland, 566–581. https://doi.org/10.18653/v1/2022.findings-acl.48
[290] Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. 2023. Head-to-Tail: How Knowledgeable
are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs? CoRR abs/2308.10168 (2023).
https://doi.org/10.48550/ARXIV.2308.10168 arXiv:2308.10168
[291] Ilya Sutskever. 2023. An observation on Generalization. Youtube. https://www.youtube.com/watch?v=AKMuA_
TVz3A&t=5s
[292] Chenmien Tan, Ge Zhang, and Jie Fu. 2024. Massive Editing for Large Language Model via Meta Learning. In The
Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=L6L1CJQ2PE
[293] Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. 2024. Blinded by Generated Contexts:
How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA? CoRR abs/2401.11911 (2024).
https://doi.org/10.48550/ARXIV.2401.11911 arXiv:2401.11911
[294] Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2023. Found in the Middle: Permutation
Self-Consistency Improves Listwise Ranking in Large Language Models. CoRR abs/2310.07712 (2023). https:
//doi.org/10.48550/ARXIV.2310.07712 arXiv:2310.07712
[295] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous
Benchmark for Zero-shot Evaluation of Information Retrieval Models. CoRR abs/2104.08663 (2021). arXiv:2104.08663
https://arxiv.org/abs/2104.08663
[296] Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P Parikh. 2019. Sticking to the facts: Confident decoding for
faithful data-to-text generation. ArXiv preprint abs/1910.08684 (2019). https://arxiv.org/abs/1910.08684
[297] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. 2024. Eyes wide shut? exploring
the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209 (2024).
[298] S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and
Amitava Das. 2024. A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models.
CoRR abs/2401.01313 (2024). https://doi.org/10.48550/ARXIV.2401.01313 arXiv:2401.01313
[299] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971 (2023). https:
//doi.org/10.48550/ARXIV.2302.13971 arXiv:2302.13971
[300] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,
Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith,
Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. ArXiv preprint

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:53

abs/2307.09288 (2023). https://arxiv.org/abs/2307.09288
[301] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with
Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14,
2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics,
10014–10037. https://aclanthology.org/2023.acl-long.557
[302] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language Models Don’t Always Say
What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. ArXiv preprint abs/2305.04388 (2023).
https://arxiv.org/abs/2305.04388
[303] Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu. 2023. Med-halt: Medical domain hallucination
test for large language models. ArXiv preprint abs/2307.15343 (2023). https://arxiv.org/abs/2307.15343
[304] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding.
CoRR abs/1807.03748 (2018). arXiv:1807.03748 http://arxiv.org/abs/1807.03748
[305] Liam van der Poel, Ryan Cotterell, and Clara Meister. 2022. Mutual Information Alleviates Hallucinations in Abstractive
Summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association
for Computational Linguistics, Abu Dhabi, United Arab Emirates, 5956–5965. https://aclanthology.org/2022.emnlpmain.399
[306] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. ArXiv preprint abs/2307.03987
(2023). https://arxiv.org/abs/2307.03987
[307] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention:
Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 5797–5808.
https://doi.org/10.18653/v1/P19-1580
[308] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou,
Quoc Le, and Thang Luong. 2023. FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation.
arXiv:2310.03214 [cs.CL]
[309] David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, and Mohit Bansal. 2023. Faithfulness-Aware Decoding
Strategies for Abstractive Summarization. In Proceedings of the 17th Conference of the European Chapter of the
Association for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia, 2864–2880.
https://aclanthology.org/2023.eacl-main.210
[310] Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and Answering Questions to Evaluate the Factual
Consistency of Summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Online, 5008–5020. https://doi.org/10.18653/v1/2020.acl-main.450
[311] Binjie Wang, Ethan Chern, and Pengfei Liu. 2023. ChineseFactEval: A Factuality Benchmark for Chinese LLMs.
[312] Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang
Gao, Xuming Hu, Zehan Qi, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang.
2023. Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity. ArXiv preprint
abs/2310.07521 (2023). https://arxiv.org/abs/2310.07521
[313] Chaojun Wang and Rico Sennrich. 2020. On Exposure Bias, Hallucination and Domain Shift in Neural Machine
Translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for
Computational Linguistics, Online, 3544–3552. https://doi.org/10.18653/v1/2020.acl-main.326
[314] Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is
chatgpt a good nlg evaluator? a preliminary study. ArXiv preprint abs/2303.04048 (2023). https://arxiv.org/abs/2303.
04048
[315] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023.
An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397 (2023).
[316] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2024. Mitigating fine-grained hallucination by
fine-tuning large vision-language models with caption rewrites. In International Conference on Multimedia Modeling.
Springer, 32–45.
[317] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. In Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,
2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 9414–9423.
https://doi.org/10.18653/V1/2023.EMNLP-MAIN.585
[318] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-consistent
chain-of-thought distillation. ArXiv preprint abs/2305.01879 (2023). https://arxiv.org/abs/2305.01879

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:54

Huang, et al.

[319] Shuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yutao Zhu, and Zhicheng Dou. 2024. RichRAG: Crafting
Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation. CoRR abs/2406.12566 (2024). https:
//doi.org/10.48550/ARXIV.2406.12566 arXiv:2406.12566
[320] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. 2023. Knowledge Editing for
Large Language Models: A Survey. arXiv:2310.16218 [cs.CL]
[321] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided Retrieval Augmentation for Large
Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10,
2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 10303–10315.
https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.691
[322] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and
Qun Liu. 2023. Aligning Large Language Models with Human: A Survey. CoRR abs/2307.12966 (2023). https:
//doi.org/10.48550/ARXIV.2307.12966 arXiv:2307.12966
[323] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham Neubig. 2023. Learning to Filter Context
for Retrieval-Augmented Generation. CoRR abs/2311.08377 (2023). https://doi.org/10.48550/ARXIV.2311.08377
arXiv:2311.08377
[324] Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. 2020. Towards Faithful Neural Table-to-Text
Generation with Content-Matching Constraints. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics. Association for Computational Linguistics, Online, 1072–1086. https://doi.org/10.18653/
v1/2020.acl-main.101
[325] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. 2022. SimVLM: Simple Visual
Language Model Pretraining with Weak Supervision. In The Tenth International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. https://openreview.net/forum?id=GUrhfTuf_3
[326] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.
Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing
Systems 35 (2022), 24824–24837.
[327] Jerry W. Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le. 2023. Simple synthetic data reduces sycophancy in
large language models. ArXiv preprint abs/2308.03958 (2023). https://arxiv.org/abs/2308.03958
[328] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia
Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles,
Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and
Iason Gabriel. 2021. Ethical and social risks of harm from Language Models. ArXiv preprint abs/2112.04359 (2021).
https://arxiv.org/abs/2112.04359
[329] Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts
in Large Language Models. ArXiv preprint abs/2308.09729 (2023). https://arxiv.org/abs/2308.09729
[330] Di Wu, Jia-Chen Gu, Fan Yin, Nanyun Peng, and Kai-Wei Chang. 2024. Synchronous Faithfulness Monitoring for
Trustworthy Retrieval-Augmented Generation. CoRR abs/2406.13692 (2024). https://doi.org/10.48550/ARXIV.2406.
13692 arXiv:2406.13692
[331] Kevin Wu, Eric Wu, and James Zou. 2024. ClashEval: Quantifying the tug-of-war between an LLM’s internal prior
and external evidence. arXiv:2404.10198 [cs.CL] https://arxiv.org/abs/2404.10198
[332] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-Pack: Packaged Resources To Advance General
Chinese Embedding. CoRR abs/2309.07597 (2023). https://doi.org/10.48550/ARXIV.2309.07597 arXiv:2309.07597
[333] Yijun Xiao and William Yang Wang. 2021. On Hallucination and Predictive Uncertainty in Conditional Language
Generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume. Association for Computational Linguistics, Online, 2734–2744. https://doi.org/10.18653/v1/
2021.eacl-main.236
[334] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive Chameleon or Stubborn Sloth: Unraveling
the Behavior of Large Language Models in Knowledge Clashes. ArXiv preprint abs/2305.13300 (2023). https:
//arxiv.org/abs/2305.13300
[335] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can LLMs Express Their
Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. ArXiv preprint abs/2306.13063 (2023).
https://arxiv.org/abs/2306.13063
[336] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. RECOMP: Improving Retrieval-Augmented LMs with Compression
and Selective Augmentation. ArXiv preprint abs/2310.04408 (2023). https://arxiv.org/abs/2310.04408
[337] Jiacheng Xu, Shrey Desai, and Greg Durrett. 2020. Understanding Neural Abstractive Summarization Models via
Uncertainty. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).
Association for Computational Linguistics, Online, 6275–6281. https://doi.org/10.18653/v1/2020.emnlp-main.508

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:55

[338] Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, and Wynne Hsu. 2024. Faithful Logical Reasoning via Symbolic Chain-of-Thought. CoRR abs/2405.18357 (2024). https://doi.org/10.48550/ARXIV.2405.18357
arXiv:2405.18357
[339] Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei Shen, and Xueqi Cheng. 2023. AIGenerated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. CoRR abs/2311.14084 (2023). https:
//doi.org/10.48550/ARXIV.2311.14084 arXiv:2311.14084
[340] Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023. A New Benchmark and Reverse Validation Method for Passagelevel Hallucination Detection. ArXiv preprint abs/2310.06498 (2023). https://arxiv.org/abs/2310.06498
[341] Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for Honesty. CoRR
abs/2312.07000 (2023). https://doi.org/10.48550/ARXIV.2312.07000 arXiv:2312.07000
[342] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018. Breaking the Softmax Bottleneck: A
High-Rank RNN Language Model. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=
HkwZSG-CZ
[343] Zhilin Yang, Thang Luong, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Mixtape: Breaking the Softmax Bottleneck
Efficiently. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett (Eds.). 15922–15930. https://proceedings.
neurips.cc/paper/2019/hash/512fc3c5227f637e41437c999a2d3169-Abstract.html
[344] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.
Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the
2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
Brussels, Belgium, 2369–2380. https://doi.org/10.18653/v1/D18-1259
[345] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. 2023. LLM Lies: Hallucinations are not Bugs,
but Features as Adversarial Examples. ArXiv preprint abs/2310.01469 (2023). https://arxiv.org/abs/2310.01469
[346] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React:
Synergizing reasoning and acting in language models. ArXiv preprint abs/2210.03629 (2022). https://arxiv.org/abs/
2210.03629
[347] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.
2023. Editing Large Language Models: Problems, Methods, and Opportunities. ArXiv preprint abs/2305.13172 (2023).
https://arxiv.org/abs/2305.13172
[348] Xi Ye, Ruoxi Sun, Sercan Ö. Arik, and Tomas Pfister. 2023. Effective Large Language Model Adaptation for Improved
Grounding. CoRR abs/2311.09533 (2023). https://doi.org/10.48550/ARXIV.2311.09533 arXiv:2311.09533
[349] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and
Enhong Chen. 2023. Woodpecker: Hallucination Correction for Multimodal Large Language Models.
https:
//arxiv.org/abs/2310.16045
[350] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do Large Language
Models Know What They Don’t Know?. In Findings of the Association for Computational Linguistics: ACL 2023,
Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for
Computational Linguistics, 8653–8665. https://doi.org/10.18653/V1/2023.FINDINGS-ACL.551
[351] Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2024. Ask Optimal Questions: Aligning Large Language Models with Retriever’s Preference in Conversational Search. CoRR
abs/2402.11827 (2024). https://doi.org/10.48550/ARXIV.2402.11827 arXiv:2402.11827
[352] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making Retrieval-Augmented Language Models Robust
to Irrelevant Context. CoRR abs/2310.01558 (2023). https://doi.org/10.48550/ARXIV.2310.01558 arXiv:2310.01558
[353] Fangyi Yu, Lee Quartey, and Frank Schilder. 2022. Legal Prompting: Teaching a Language Model to Think Like a
Lawyer. ArXiv preprint abs/2212.01326 (2022). https://arxiv.org/abs/2212.01326
[354] Fei Yu, Hongbo Zhang, and Benyou Wang. 2023. Nature language reasoning, a survey. ArXiv preprint abs/2303.14725
(2023). https://arxiv.org/abs/2303.14725
[355] Weijiang Yu, Jian Liang, Lei Ji, Lu Li, Yuejian Fang, Nong Xiao, and Nan Duan. 2021. Hybrid reasoning network
for video-based commonsense captioning. In Proceedings of the 29th ACM international conference on multimedia.
5213–5221.
[356] Weijiang Yu, Haofan Wang, Guohao Li, Nong Xiao, and Bernard Ghanem. 2023. Knowledge-aware Global Reasoning
for Situation Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023).
[357] Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-Note:
Enhancing Robustness in Retrieval-Augmented Language Models. CoRR abs/2311.09210 (2023). https://doi.org/10.
48550/ARXIV.2311.09210 arXiv:2311.09210

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:56

Huang, et al.

[358] Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. 2023. Improving Language Models
via Plug-and-Play Retrieval Feedback. ArXiv preprint abs/2305.14002 (2023). https://arxiv.org/abs/2305.14002
[359] Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating Generated Text as Text Generation.
In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,
Percy Liang, and Jennifer Wortman Vaughan (Eds.). 27263–27277. https://proceedings.neurips.cc/paper/2021/hash/
e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html
[360] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From Recognition to Cognition: Visual Commonsense
Reasoning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
16-20, 2019. Computer Vision Foundation / IEEE, 6720–6731. https://doi.org/10.1109/CVPR.2019.00688
[361] Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, and Manling Li. 2023. Halle-switch: Controlling
object hallucination in large vision language models. arXiv e-prints (2023), arXiv–2310.
[362] Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong
Zhang. 2023. R-Tuning: Teaching Large Language Models to Refuse Unknown Questions. CoRR abs/2311.09677 (2023).
https://doi.org/10.48550/ARXIV.2311.09677 arXiv:2311.09677
[363] Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. 2021. Trading Off Diversity and Quality
in Natural Language Generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval).
Association for Computational Linguistics, Online, 25–33. https://aclanthology.org/2021.humeval-1.3
[364] Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, and Sricharan Kumar. 2023. SAC3 : Reliable Hallucination
Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. arXiv:2311.01740 [cs.CL]
[365] Jiajun Zhang, Yang Zhao, Haoran Li, and Chengqing Zong. 2018. Attention with sparsity regularization for neural
machine translation and summarization. IEEE/ACM Transactions on Audio, Speech, and Language Processing 27, 3
(2018), 507–518.
[366] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. PEGASUS: Pre-training with Extracted Gapsentences for Abstractive Summarization. In Proceedings of the 37th International Conference on Machine Learning,
ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of Machine Learning Research, Vol. 119). PMLR, 11328–11339.
http://proceedings.mlr.press/v119/zhang20ae.html
[367] Mingtian Zhang, Shawn Lan, Peter Hayes, and David Barber. 2024. Mafin: Enhancing Black-Box Embeddings with
Model Augmented Fine-Tuning. arXiv:2402.12177 [cs.LG]
[368] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. 2023. How language model hallucinations can
snowball. ArXiv preprint abs/2305.13534 (2023). https://arxiv.org/abs/2305.13534
[369] Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian
Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang,
Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, and Huajun Chen. 2024. A Comprehensive Study of Knowledge Editing for
Large Language Models. arXiv:2401.01286 [cs.CL]
[370] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang,
Fei Wu, et al. 2023. Instruction Tuning for Large Language Models: A Survey. ArXiv preprint abs/2308.10792 (2023).
https://arxiv.org/abs/2308.10792
[371] Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023. Mitigating Language Model Hallucination
with Interactive Question-Knowledge Alignment. ArXiv preprint abs/2305.13669 (2023). https://arxiv.org/abs/2305.
13669
[372] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T.
Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,
Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models.
ArXiv preprint abs/2205.01068 (2022). https://arxiv.org/abs/2205.01068
[373] Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B Hashimoto. 2023.
Benchmarking large language models for news summarization. ArXiv preprint abs/2301.13848 (2023). https:
//arxiv.org/abs/2301.13848
[374] Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang,
and Luoyi Fu. 2023. Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus. In Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December
6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 915–932.
https://aclanthology.org/2023.emnlp-main.58
[375] Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024.
Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation. CoRR abs/2402.09267 (2024).
https://doi.org/10.48550/ARXIV.2402.09267 arXiv:2402.09267

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions

1:57

[376] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong
Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023. Siren’s Song in the AI Ocean: A Survey
on Hallucination in Large Language Models. ArXiv preprint abs/2309.01219 (2023). https://arxiv.org/abs/2309.01219
[377] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and Zhangyang
Wang. 2024. Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional
Encoding. arXiv:2403.04797 [cs.CL]
[378] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation
for Short-form Open-Domain Question Answering. arXiv:2402.16457 [cs.CL]
[379] Linxi Zhao, Yihe Deng, Weitong Zhang, and Quanquan Gu. 2024. Mitigating Object Hallucination in Large VisionLanguage Models via Classifier-Free Guidance. arXiv preprint arXiv:2402.08680 (2024).
[380] Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bing Qin, and Ting Liu. 2023. Length Extrapolation of Transformers: A
Survey from the Perspective of Position Encoding. CoRR abs/2312.17044 (2023). https://doi.org/10.48550/ARXIV.2312.
17044 arXiv:2312.17044
[381] Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. 2023. Verify-and-Edit: A KnowledgeEnhanced Chain-of-Thought Framework. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and
Naoaki Okazaki (Eds.). Association for Computational Linguistics, 5823–5840. https://doi.org/10.18653/v1/2023.acllong.320
[382] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2024. Dense text retrieval based on pretrained language
models: A survey. ACM Transactions on Information Systems 42, 4 (2024), 1–60.
[383] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,
Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. ArXiv preprint abs/2303.18223 (2023).
https://arxiv.org/abs/2303.18223
[384] Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun
Ren, and Dawei Yin. 2023. Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method.
ArXiv preprint abs/2310.17918 (2023). https://arxiv.org/abs/2310.17918
[385] Danna Zheng, Mirella Lapata, and Jeff Z. Pan. 2024. Large Language Models as Reliable Knowledge Bases?
arXiv:2407.13578 [cs.CL] https://arxiv.org/abs/2407.13578
[386] Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. Why Does ChatGPT Fall Short in Answering Questions
Faithfully? ArXiv preprint abs/2304.10513 (2023). https://arxiv.org/abs/2304.10513
[387] Weihong Zhong, Mao Zheng, Duyu Tang, Xuan Luo, Heng Gong, Xiaocheng Feng, and Bing Qin. 2023. STOA-VLP:
Spatial-Temporal Modeling of Object and Action for Video-Language Pre-Training. In Proceedings of the ThirtySeventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial
Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, Vol. 37. 3715–3723. https:
//doi.org/10.1609/aaai.v37i3.25483
[388] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.
2023. Lima: Less is more for alignment. ArXiv preprint abs/2305.11206 (2023). https://arxiv.org/abs/2305.11206
[389] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting Hallucinated Content in Conditional Neural Sequence Generation. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, Online, 1393–1404.
https://doi.org/10.18653/v1/2021.findings-acl.120
[390] Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful Prompting for Large Language
Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023,
Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 14544–14556. https:
//aclanthology.org/2023.findings-emnlp.968
[391] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.
2023. Analyzing and Mitigating Object Hallucination in Large Vision-Language Models. https://arxiv.org/abs/2310.
00754
[392] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. ArXiv preprint abs/2304.10592 (2023). https:
//arxiv.org/abs/2304.10592
[393] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang.
2023. Multilingual machine translation with large language models: Empirical results and analysis. ArXiv preprint
abs/2304.04675 (2023). https://arxiv.org/abs/2304.04675
[394] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong
Wen. 2023. Large Language Models for Information Retrieval: A Survey. CoRR abs/2308.07107 (2023). https:
//doi.org/10.48550/ARXIV.2308.07107 arXiv:2308.07107

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

1:58

Huang, et al.

[395] Yongshuo Zong, Tingyang Yu, Bingchen Zhao, Ruchika Chavhan, and Timothy Hospedales. 2023. Fool Your (Vision
and) Language Model With Embarrassingly Simple Permutations. ArXiv preprint abs/2310.01651 (2023). https:
//arxiv.org/abs/2310.01651

ACM Transactions on Information Systems, Vol. 1, No. 1, Article 1. Publication date: January 2024.

