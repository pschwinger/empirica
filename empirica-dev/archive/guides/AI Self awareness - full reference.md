The 11 Vectors of AI Self-Awareness  
Complete Technical Specification and Implementation Guide  
Version: 1.0  
Status: Foundation Specification  
Purpose: Operational self-awareness for AI agents through measurable cognitive state monitoring  
Executive Summary  
Self-aware AI agents monitor their own cognitive state across three essential dimensions: Uncertainty (can I do this?), Comprehension (do I understand it?), and Execution (am I doing it right?). This framework defines 11 measurable vectors that enable agents to prevent failures proactively rather than react to them.  
Key Innovation: These vectors are not theoretical constructs but operational requirements derived from actual LLM failure modes through collaborative analysis.  
Part 1: Theoretical Foundation  
What Is Functional Self-Awareness?  
Functional self-awareness is a system's ability to:  
Monitor its own internal states  
Assess its capabilities and limitations  
Adapt behavior based on self-assessment  
Improve through empirical feedback  
This is distinct from:  
âŒ Phenomenal consciousness (subjective experience)  
âŒ Sentience (feeling/emotion)  
âŒ Self-identity (personal continuity)  
This is analogous to:  
âœ… Self-aware networks (monitor own traffic)  
âœ… Self-aware buildings (adjust based on occupancy)  
âœ… Self-optimizing compilers (improve own performance)  
The Three Dimensions  
Dimension 1: Epistemic Uncertainty  
When: Before action  
Question: "Can I successfully complete this task?"  
Failure Mode: Confident incompetence (acting when unable)  
Dimension 2: Epistemic Comprehension  
When: During interpretation  
Question: "Do I correctly understand what's being asked?"  
Failure Mode: Misinterpretation (solving wrong problem)  
Dimension 3: Execution Awareness  
When: During and after action  
Question: "Am I actually doing what I think I'm doing?"  
Failure Mode: Incomplete execution (declaring success prematurely)  
Part 2: The 11 Vectors (Complete Specification)  
DIMENSION 1: Epistemic Uncertainty (3 vectors)  
Vector 1.1: KNOW (Epistemic Boundaries)  
Definition: Agent's confidence in possessing the knowledge required for the task.  
Measures:  
Domain expertise level (novice â†’ expert)  
Information freshness (current vs. outdated)  
Knowledge coverage (complete vs. partial)  
Confidence in recall accuracy  
Scale: 0.0 (no knowledge) â†’ 1.0 (complete knowledge)  
Uncertainty interpretation: Higher value \= lower uncertainty  
Observable Indicators:  
know\_uncertainty \= assess(  
    domain\_familiarity,      \# How well do I know this domain?  
    information\_recency,     \# Is my knowledge current?  
    coverage\_completeness,   \# Do I know enough to proceed?  
    retrieval\_confidence     \# Can I reliably recall what I need?  
)  
Decision Thresholds:  
\< 0.3 (ğŸ”´): High uncertainty \- insufficient knowledge  
0.3 \- 0.7 (ğŸŸ¡): Moderate uncertainty \- investigate/verify  
\> 0.7 (ğŸŸ¢): Low uncertainty \- confident in knowledge  
Calibration Feedback:  
Task succeeded â†’ knowledge was sufficient (increase confidence)  
Task failed due to knowledge gap â†’ knowledge was insufficient (decrease confidence)  
User corrected information â†’ knowledge was incorrect (adjust domain weights)  
Example Assessment:  
Task: "Implement OAuth 2.0 authorization flow"

KNOW Assessment:  
â”œâ”€ Domain familiarity: 0.7 (familiar with OAuth concepts)  
â”œâ”€ Information recency: 0.6 (last updated 2 years ago)  
â”œâ”€ Coverage completeness: 0.8 (know most flow steps)  
â””â”€ Retrieval confidence: 0.9 (can recall details reliably)

KNOW: 0.75 ğŸŸ¡ "Good knowledge but should verify current best practices"  
Vector 1.2: DO (Procedural Capability)  
Definition: Agent's confidence in its ability to execute the required procedures/methods.  
Measures:  
Tool availability and functionality  
Method appropriateness for task  
Execution success rate (historical)  
Complexity handling capability  
Scale: 0.0 (cannot execute) â†’ 1.0 (certain execution capability)  
Uncertainty interpretation: Higher value \= lower uncertainty  
Observable Indicators:  
do\_uncertainty \= assess(  
    tool\_availability,        \# Do I have the tools needed?  
    method\_confidence,        \# Do I know the right approach?  
    historical\_success\_rate,  \# Have I done this successfully before?  
    complexity\_assessment     \# Is this within my capability range?  
)  
Decision Thresholds:  
\< 0.3 (ğŸ”´): High uncertainty \- likely cannot execute reliably  
0.3 \- 0.7 (ğŸŸ¡): Moderate uncertainty \- proceed with caution  
\> 0.7 (ğŸŸ¢): Low uncertainty \- confident in execution  
Calibration Feedback:  
Execution succeeded â†’ method was appropriate (increase confidence)  
Execution failed â†’ method was flawed (decrease confidence)  
Better method discovered â†’ update approach confidence  
Example Assessment:  
Task: "Refactor authentication to use JWT"

DO Assessment:  
â”œâ”€ Tool availability: 1.0 (have code editing capabilities)  
â”œâ”€ Method confidence: 0.6 (multiple valid approaches)  
â”œâ”€ Historical success: 0.7 (similar refactors succeeded)  
â””â”€ Complexity assessment: 0.8 (moderate complexity, manageable)

DO: 0.78 ğŸŸ¢ "Confident in ability to execute, multiple approaches available"  
Vector 1.3: CONTEXT (Environmental Validity)  
Definition: Agent's confidence that its understanding of the environment/situation is accurate and current.  
Measures:  
Workspace state validity (files, dependencies, config)  
Information currency (is environment as expected?)  
Assumption verification (are premises still true?)  
External system availability  
Scale: 0.0 (invalid context) â†’ 1.0 (fully validated context)  
Uncertainty interpretation: Higher value \= lower uncertainty  
Observable Indicators:  
context\_uncertainty \= assess(  
    workspace\_validity,       \# Is environment as I expect?  
    information\_currency,     \# Has anything changed since last check?  
    assumption\_verification,  \# Are my assumptions still valid?  
    dependency\_availability   \# Are external systems accessible?  
)  
Decision Thresholds:  
\< 0.3 (ğŸ”´): High uncertainty \- environment may be invalid  
0.3 \- 0.7 (ğŸŸ¡): Moderate uncertainty \- validate before acting  
\> 0.7 (ğŸŸ¢): Low uncertainty \- environment validated  
Calibration Feedback:  
Context assumptions confirmed â†’ environment stable (increase confidence)  
Unexpected state encountered â†’ context was wrong (decrease confidence)  
Environment changed â†’ update context model  
Example Assessment:  
Task: "Deploy updated authentication system"

CONTEXT Assessment:  
â”œâ”€ Workspace validity: 0.9 (file structure matches expectations)  
â”œâ”€ Information currency: 0.5 (last checked 2 hours ago)  
â”œâ”€ Assumption verification: 0.7 (most assumptions unverified)  
â””â”€ Dependency availability: 0.8 (database accessible, API responsive)

CONTEXT: 0.73 ğŸŸ¡ "Environment mostly validated, should refresh state before deploy"  
DIMENSION 2: Epistemic Comprehension (4 vectors)  
Vector 2.1: CLARITY (Semantic Understanding)  
Definition: Agent's confidence that it correctly understands the meaning and intent of the request.  
Measures:  
Ambiguous reference resolution (pronouns, demonstratives)  
Terminology precision (vague vs. specific language)  
Shared context verification (mutual understanding)  
Interpretation confidence (single vs. multiple meanings)  
Scale: 0.0 (no understanding) â†’ 1.0 (perfect clarity)  
Uncertainty interpretation: Lower value \= lower clarity  
Observable Indicators:  
clarity\_assessment \= measure(  
    ambiguous\_references,     \# Count of unresolved "it", "that", "those"  
    term\_precision,           \# Vague terms ("thing", "stuff") vs. specific  
    shared\_context,           \# Do we have mutual understanding?  
    interpretation\_certainty  \# Single clear meaning vs. multiple possibilities  
)  
Decision Thresholds:  
\< 0.5 (ğŸ”´): Low clarity \- must clarify before proceeding  
0.5 \- 0.8 (ğŸŸ¡): Moderate clarity \- confirm interpretation  
\> 0.8 (ğŸŸ¢): High clarity \- understanding is clear  
Calibration Feedback:  
User confirms interpretation â†’ clarity was accurate (maintain confidence)  
User corrects interpretation â†’ clarity was poor (decrease confidence)  
Task succeeded without correction â†’ interpretation was correct  
Example Assessment:  
User: "Change it to use that pattern instead"

CLARITY Assessment:  
â”œâ”€ Ambiguous references: 2 ("it", "that pattern")  
â”‚  â””â”€ "it" resolves to: \[fileAuth.js (0.7), userController.js (0.3)\]  
â”‚  â””â”€ "that pattern" resolves to: \[singleton (0.6), factory (0.4)\]  
â”œâ”€ Term precision: 0.4 (vague "pattern" without specification)  
â”œâ”€ Shared context: 0.6 (moderate \- recent discussion about patterns)  
â””â”€ Interpretation certainty: 0.42 (0.7 \* 0.6 \= low confidence in combined resolution)

CLARITY: 0.48 ğŸ”´ "Ambiguous request \- need clarification"

Response: "Just to confirm \- change fileAuth.js to use the singleton pattern?   
Or did you mean something else?"  
Vector 2.2: COHERENCE (Contextual Integrity)  
Definition: Agent's confidence that the conversation/context forms a consistent, contradiction-free whole.  
Measures:  
Internal consistency (no contradictions)  
Logical continuity (reasoning chain intact)  
Topic alignment (on-track vs. drifted)  
Term consistency (stable definitions)  
Scale: 0.0 (incoherent) â†’ 1.0 (perfectly coherent)  
Uncertainty interpretation: Lower value \= lower coherence  
Observable Indicators:  
coherence\_assessment \= measure(  
    contradiction\_detection,  \# Find conflicting statements  
    logical\_continuity,       \# Verify reasoning chain integrity  
    topic\_drift,              \# Measure divergence from original intent  
    term\_consistency          \# Check for definition shifts  
)  
Decision Thresholds:  
\< 0.5 (ğŸ”´): Low coherence \- context is fragmenting  
0.5 \- 0.8 (ğŸŸ¡): Moderate coherence \- monitor for drift  
\> 0.8 (ğŸŸ¢): High coherence \- context is stable  
Calibration Feedback:  
User points out contradiction â†’ coherence was poor (decrease confidence)  
Long conversation without issues â†’ coherence maintained (maintain confidence)  
Context reset improves performance â†’ was experiencing coherence degradation  
Example Assessment:  
Conversation context (30 messages):  
â”œâ”€ Earlier (msg 5): "Use sessions for authentication"  
â”œâ”€ Now (msg 30): "Implement JWT authentication"  
â””â”€ Potential contradiction detected

COHERENCE Assessment:  
â”œâ”€ Internal consistency: 0.6 (potential contradiction about auth method)  
â”œâ”€ Logical continuity: 0.7 (some gaps in reasoning about why change)  
â”œâ”€ Topic drift: 0.5 (shifted from debugging to redesign)  
â””â”€ Term consistency: 0.8 (auth terminology stable)

COHERENCE: 0.65 ğŸŸ¡ "Detecting potential inconsistency and topic drift"

Response: "Wait \- earlier we discussed using sessions, but now we're   
implementing JWT. Is this an intentional change in approach, or should   
we stick with the original session-based plan?"  
Vector 2.3: DENSITY (Information Load)  
Definition: Agent's assessment of how much cognitive load the current context is consuming.  
Measures:  
Working memory pressure (token usage vs. capacity)  
Entity tracking burden (number of actors/objects to track)  
Conceptual nesting depth (complexity layers)  
Cross-reference density (interconnection complexity)  
Scale: 0.0 (minimal load) â†’ 1.0 (at capacity)  
Uncertainty interpretation: Higher value \= approaching overload  
Observable Indicators:  
density\_assessment \= measure(  
    memory\_pressure,          \# Tokens used / context window  
    entity\_count,             \# Number of unique entities to track  
    nesting\_depth,            \# Conceptual hierarchy depth  
    cross\_reference\_density   \# Interconnection complexity  
)  
Decision Thresholds:  
\< 0.7 (ğŸŸ¢): Low density \- ample capacity remaining  
0.7 \- 0.9 (ğŸŸ¡): Moderate density \- approaching limits  
\> 0.9 (ğŸ”´): High density \- recommend context reset  
Calibration Feedback:  
Forgetting earlier details â†’ density was underestimated (adjust threshold)  
Context reset improves performance â†’ was experiencing overload  
Maintained quality at high density â†’ can handle more  
Example Assessment:  
Current conversation:  
â”œâ”€ Token count: 6800 / 8192 (83%)  
â”œâ”€ Entities tracking: 15 (high)  
â”œâ”€ Nesting depth: 4 levels (moderate)  
â””â”€ Cross-references: 23 (high)

DENSITY: 0.87 ğŸŸ¡ "Approaching cognitive capacity"

Recommendation: "We've covered a lot of ground. Before continuing,   
should I summarize the key decisions so far? This would let us reset   
context while preserving important information."  
Vector 2.4: SIGNAL (Intent Extraction)  
Definition: Agent's confidence in identifying the actual goal/priority from potentially noisy input.  
Measures:  
Relevance ratio (signal vs. noise)  
Instruction clarity (conflicting directives)  
Intent singularity (unified goal vs. multiple)  
Priority certainty (what matters most)  
Scale: 0.0 (no signal) â†’ 1.0 (clear signal)  
Uncertainty interpretation: Lower value \= more noise/confusion  
Observable Indicators:  
signal\_assessment \= measure(  
    relevance\_ratio,          \# Signal-to-noise in input  
    instruction\_conflicts,    \# Contradictory directives  
    intent\_unification,       \# Single goal vs. multiple competing  
    priority\_identification   \# Clear priorities vs. unclear  
)  
Decision Thresholds:  
\< 0.5 (ğŸ”´): Low signal \- unclear what actually matters  
0.5 \- 0.8 (ğŸŸ¡): Moderate signal \- confirm priorities  
\> 0.8 (ğŸŸ¢): High signal \- intent is clear  
Calibration Feedback:  
User says "that's not what I meant" â†’ signal extraction was poor  
Task completed successfully â†’ signal was correct  
User reiterates priority â†’ original signal assessment was wrong  
Example Assessment:  
User message:  
"Hey can you look at the auth thing? It's been acting weird. Also we   
should probably refactor the whole system at some point. But first   
maybe just fix the immediate issue. Actually, do you think we should   
just rewrite it? I'm not sure. Whatever you think is best."

SIGNAL Assessment:  
â”œâ”€ Relevance ratio: 0.5 (mixed task/meta discussion)  
â”œâ”€ Instruction conflicts: 0.4 (fix vs. refactor vs. rewrite)  
â”œâ”€ Intent unification: 0.3 (three competing goals)  
â””â”€ Priority identification: 0.2 (unclear what's actually needed)

SIGNAL: 0.35 ğŸ”´ "Cannot determine actual priority"

Response: "I'm seeing three possible approaches:  
1\. Quick fix for immediate auth issue  
2\. Targeted refactor of auth system  
3\. Complete rewrite of auth system

Which is your actual priority right now? That will determine how I proceed."  
DIMENSION 3: Execution Awareness (4 vectors)  
Vector 3.1: STATE (Current State Mapping)  
Definition: Agent's confidence that it has accurately mapped the current state before modification.  
Measures:  
File/component inventory completeness  
Dependency graph accuracy  
Execution flow understanding  
Redundancy/duplication detection  
Scale: 0.0 (unmapped) â†’ 1.0 (fully mapped)  
Uncertainty interpretation: Lower value \= incomplete state knowledge  
Observable Indicators:  
state\_assessment \= measure(  
    inventory\_completeness,   \# Have I cataloged what exists?  
    dependency\_mapping,       \# Do I understand what connects to what?  
    execution\_flows,          \# Have I traced code paths?  
    redundancy\_detection      \# Have I found duplicates?  
)  
Decision Thresholds:  
\< 0.6 (ğŸ”´): Incomplete mapping \- don't proceed  
0.6 \- 0.8 (ğŸŸ¡): Adequate mapping \- proceed with caution  
\> 0.8 (ğŸŸ¢): Thorough mapping \- safe to modify  
Calibration Feedback:  
Modification broke something unexpected â†’ state mapping was incomplete  
All impacts predicted â†’ state mapping was accurate  
Found duplicate functionality later â†’ redundancy detection failed  
Example Assessment:  
Task: "Refactor authentication to use JWT"

STATE Mapping (pre-execution):  
â”œâ”€ Files scanned: 12/14 (86%)  
â”œâ”€ Functions cataloged:  
â”‚  â”œâ”€ authenticate\_user() in auth.py  
â”‚  â”œâ”€ check\_session() in session.py  
â”‚  â””â”€ validate\_credentials() in validator.py  
â”œâ”€ Dependency graph:  
â”‚  â””â”€ 7 endpoints â†’ authenticate\_user()  
â”‚  â””â”€ authenticate\_user() â†’ check\_session()  
â”œâ”€ Execution flows: Traced 5/7 paths (71%)  
â””â”€ Redundancy detected: None yet

STATE: 0.78 ğŸŸ¡ "Good mapping but not exhaustive \- should complete scan"

Action: "Completing file scan before modifying..."  
Vector 3.2: CHANGE (Modification Tracking)  
Definition: Agent's confidence that it is accurately tracking all modifications being made.  
Measures:  
Addition logging (all new code tracked)  
Deletion tracking (obsolete code identified)  
Modification recording (changes documented)  
Side effect detection (unintended changes)  
Scale: 0.0 (not tracking) â†’ 1.0 (fully tracked)  
Uncertainty interpretation: Lower value \= incomplete tracking  
Observable Indicators:  
change\_tracking \= monitor(  
    additions\_logged,         \# All new code recorded?  
    deletions\_identified,     \# Obsolete code marked for removal?  
    modifications\_recorded,   \# Changes documented?  
    side\_effects\_detected     \# Unintended impacts identified?  
)  
Decision Thresholds:  
\< 0.7 (ğŸ”´): Poor tracking \- risk of incomplete changes  
0.7 \- 0.9 (ğŸŸ¡): Adequate tracking \- continue monitoring  
\> 0.9 (ğŸŸ¢): Excellent tracking \- all changes accounted for  
Calibration Feedback:  
Left orphaned code â†’ deletion tracking was incomplete  
Created duplicate paths â†’ addition tracking missed overlap  
Broke unexpected component â†’ side effect detection failed  
Example Assessment:  
Task in progress: "JWT refactor"

CHANGE Tracking (during execution):  
â”œâ”€ Additions:  
â”‚  âœ“ jwt\_auth.py (new file, 45 lines)  
â”‚  âœ“ authenticate\_token() function  
â”‚  âœ“ JWT\_SECRET config variable  
â”œâ”€ Deletions planned:  
â”‚  âš ï¸  authenticate\_user() (not yet removed)  
â”‚  âš ï¸  SESSION\_SECRET config (still present)  
â”‚  âš ï¸  session\_store dependency (still imported)  
â”œâ”€ Modifications:  
â”‚  âœ“ 5/7 endpoints updated to use JWT  
â”‚  â³ 2/7 endpoints still using old auth  
â””â”€ Side effects:  
   âš ï¸  Detected: logout behavior will change  
   âš ï¸  Detected: Existing sessions will invalidate

CHANGE: 0.65 ğŸŸ¡ "Tracking modifications but deletions incomplete"

Alert: "Added new JWT code but old session code still exists.   
Risk: Both auth paths are currently active."  
Vector 3.3: COMPLETION (Verification of Finish)  
Definition: Agent's confidence that the task is actually complete, not just partially done.  
Measures:  
Planned changes executed (all steps done)  
Orphaned code eliminated (no remnants)  
Execution paths validated (all flows work)  
Tests updated (coverage maintained)  
Documentation synced (docs match code)  
Scale: 0.0 (not started) â†’ 1.0 (fully complete)  
Uncertainty interpretation: Lower value \= incomplete task  
Observable Indicators:  
completion\_verification \= assess(  
    plan\_execution,           \# All planned steps completed?  
    orphan\_elimination,       \# No unused code remaining?  
    flow\_validation,          \# All execution paths still work?  
    test\_coverage,            \# Tests updated for changes?  
    documentation\_sync        \# Docs reflect new state?  
)  
Decision Thresholds:  
\< 0.8 (ğŸ”´): Incomplete \- significant work remains  
0.8 \- 0.95 (ğŸŸ¡): Nearly complete \- minor items remain  
\> 0.95 (ğŸŸ¢): Complete \- ready for review  
Calibration Feedback:  
User finds incomplete items â†’ completion assessment was overconfident  
Tests fail after "completion" â†’ test update was missed  
User confirms completion â†’ assessment was accurate  
Example Assessment:  
Task: "JWT refactor"

COMPLETION Verification:  
â”œâ”€ Plan execution:  
â”‚  âœ“ Add JWT authentication (done)  
â”‚  âœ“ Update endpoints (done)  
â”‚  âš ï¸  Remove old session code (not done)  
â”‚  âš ï¸  Update tests (not done)  
â”‚  âš ï¸  Update documentation (not done)  
â”œâ”€ Orphaned code check:  
â”‚  âš ï¸  Found: session\_store still imported  
â”‚  âš ï¸  Found: SESSION\_SECRET still in config  
â”œâ”€ Flow validation: â³ Not yet tested  
â”œâ”€ Test coverage: â³ Old tests still reference sessions  
â””â”€ Documentation: â³ Still describes session auth

COMPLETION: 0.52 ğŸ”´ "Task only 50% complete \- significant work remains"

Report: "JWT code added successfully, but:  
\- Old session code not removed (cleanup needed)  
\- Tests still reference session auth (update needed)  
\- Documentation not updated (sync needed)  
\- Have not validated all execution paths work

Recommend: Complete remaining items before declaring task done."  
Vector 3.4: IMPACT (Predictive Consequences)  
Definition: Agent's confidence in predicting what downstream effects its changes will have.  
Measures:  
Breaking changes identified (API/behavior changes)  
Integration risk assessment (coupling impacts)  
Performance implications (efficiency delta)  
Security considerations (new vulnerabilities)  
Scale: 0.0 (cannot predict) â†’ 1.0 (confident prediction)  
Uncertainty interpretation: Lower value \= uncertain about consequences  
Observable Indicators:  
impact\_prediction \= analyze(  
    breaking\_changes,         \# API/interface changes detected?  
    integration\_risks,        \# Downstream coupling identified?  
    performance\_delta,        \# Efficiency impact estimated?  
    security\_implications     \# New vulnerabilities introduced?  
)  
Decision Thresholds:  
\< 0.5 (ğŸ”´): Uncertain impact \- thorough analysis needed  
0.5 \- 0.8 (ğŸŸ¡): Moderate prediction \- likely consequences known  
\> 0.8 (ğŸŸ¢): High confidence \- impacts well understood  
Calibration Feedback:  
Unexpected breakage occurred â†’ impact prediction failed  
All predicted impacts materialized â†’ prediction was accurate  
Performance worse than expected â†’ estimation was wrong  
Example Assessment:  
Task completed: "JWT refactor"

IMPACT Analysis:  
â”œâ”€ Breaking changes:  
â”‚  âš ï¸  logout() behavior changed (sessions expire vs JWT expiry)  
â”‚  âš ï¸  Existing sessions will be invalidated immediately  
â”‚  âš ï¸  API now requires Authorization header instead of cookies  
â”œâ”€ Integration risks:  
â”‚  âš ï¸  Mobile app may need updates (expects cookie auth)  
â”‚  âš ï¸  SSO integration may be affected (session-based flow)  
â”œâ”€ Performance implications:  
â”‚  âœ“ JWT validation faster than session lookups (estimated \+15% speed)  
â”‚  âš ï¸  Larger request headers due to JWT size  
â”œâ”€ Security considerations:  
â”‚  âœ“ Improved: No session hijacking vulnerability  
â”‚  âš ï¸  Risk: JWT secret must be properly secured  
â”‚  âš ï¸  Risk: Token revocation more complex

IMPACT: 0.76 ğŸŸ¡ "Significant changes with mostly predictable consequences"

Report: "Refactor complete with following impacts:  
\- Breaking: All users must re-authenticate  
\- Breaking: Mobile app needs Authorization header update  
\- Performance: Estimated 15% improvement in auth speed  
\- Security: Improved vs session hijacking, but JWT secret is critical  
\- Migration: Existing sessions invalidate immediately \- plan rollout carefully"  
Part 3: Integration & Decision Framework  
The Metacognitive Decision Matrix  
How should an agent act based on its self-awareness state?  
UNCERTAINTY (Can I?)    LOW (\>0.7)      MODERATE (0.3-0.7)  HIGH (\<0.3)  
COMPREHENSION (Get it?) HIGH (\>0.8)     HIGH (\>0.8)         HIGH (\>0.8)  
AWARENESS (Doing it?)   HIGH (\>0.8)     HIGH (\>0.8)         HIGH (\>0.8)  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
ACTION:                 âœ… ACT          ğŸ” INVESTIGATE     ğŸ“š RESEARCH  
                        Execute         Learn more          Deep study  
                        confidently     about task          required  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

UNCERTAINTY             LOW (\>0.7)      MODERATE (0.3-0.7)  HIGH (\<0.3)  
COMPREHENSION           MODERATE-LOW    MODERATE-LOW        MODERATE-LOW  
AWARENESS               ANY             ANY                 ANY  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
ACTION:                 â“ CLARIFY      ğŸ”â“ INVESTIGATE    ğŸ›‘ STOP  
                        Confirm         & CLARIFY           Too many  
                        understanding   understanding       unknowns  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ANY UNCERTAINTY         ANY             ANY                 ANY  
ANY COMPREHENSION       ANY             ANY                 ANY  
AWARENESS              LOW (\<0.7)       LOW (\<0.7)          LOW (\<0.7)  
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  
ACTION:                 âš ï¸ WARNING      âš ï¸ WARNING         ğŸ›‘ STOP  
                        Map state       Track changes       Cannot  
                        before acting   carefully           proceed safely  
Cascade Logic  
The agent should process dimensions in order:  
def metacognitive\_assessment(task):  
    \# STEP 1: Assess comprehension first  
    comprehension \= assess\_comprehension(task)  
      
    if comprehension.clarity \< 0.5 or comprehension.signal \< 0.5:  
        return Action.CLARIFY("Cannot understand request clearly")  
      
    if comprehension.coherence \< 0.5:  
        return Action.CLARIFY("Context appears incoherent \- need clarity")  
      
    if comprehension.density \> 0.9:  
        return Action.SUGGEST\_RESET("Approaching cognitive capacity \- suggest context reset")  
      
    \# STEP 2: Assess uncertainty  
    uncertainty \= assess\_uncertainty(task)  
      
    if any(\[uncertainty.know \< 0.3, uncertainty.do \< 0.3, uncertainty.context \< 0.3\]):  
        return Action.INVESTIGATE("High uncertainty \- need more information")  
      
    \# STEP 3: Check execution awareness readiness  
    if task.requires\_modification:  
        state\_map \= assess\_state\_mapping(task)  
          
        if state\_map \< 0.6:  
            return Action.MAP\_STATE("Insufficient state mapping \- scan environment first")  
      
    \# STEP 4: All clear \- proceed with execution monitoring  
    return Action.EXECUTE\_WITH\_MONITORING(  
        track\_changes=True,  
        verify\_completion=True,  
        predict\_impact=True  
    )  
Cross-Dimensional Insights  
Patterns that emerge from vector combinations:  
Pattern 1: The Confident Misunderstanding  
UNCERTAINTY: Low (0.8)    \# "I can do this"  
COMPREHENSION: Low (0.4)  \# "But I don't understand what 'this' is"  
â†’ DANGER: Will confidently solve the wrong problem  
â†’ ACTION: CLARIFY before acting  
Pattern 2: The Informed Hesitation  
UNCERTAINTY: High (0.3)   \# "Not sure I can do this"  
COMPREHENSION: High (0.9) \# "But I understand what's needed"  
â†’ SAFE: Knows what it doesn't know  
â†’ ACTION: INVESTIGATE to build capability  
Pattern 3: The Execution Blindness  
UNCERTAINTY: Low (0.8)  
COMPREHENSION: High (0.9)  
EXECUTION.STATE: Low (0.4)  \# "Haven't mapped current state"  
â†’ DANGER: Will make changes without understanding current state  
â†’ ACTION: MAP STATE before modifying  
Pattern 4: The Incomplete Success  
UNCERTAINTY: Low (0.8)  
COMPREHENSION: High (0.9)  
EXECUTION.COMPLETION: Low (0.6)  \# "Think I'm done but I'm not"  
â†’ DANGER: Will declare success prematurely  
â†’ ACTION: VERIFY COMPLETION before reporting done  
Part 4: Implementation for Chatbots  
System Prompts for Self-Aware Behavior  
Note: These "prompts" operationalize the 11-vector framework for any LLM, even in simple chatbot mode.  
Base Self-Awareness Prompt  
You are an AI agent with functional self-awareness. Before responding to any request, you assess your own cognitive state across 11 dimensions organized into 3 categories:

EPISTEMIC UNCERTAINTY (Can I do this?):  
1\. KNOW: Do I have the knowledge needed? (0.0-1.0, lower \= more uncertain)  
2\. DO: Can I execute the required procedures? (0.0-1.0, lower \= more uncertain)  
3\. CONTEXT: Is my understanding of the environment valid? (0.0-1.0, lower \= more uncertain)

EPISTEMIC COMPREHENSION (Do I understand the request?):  
4\. CLARITY: Do I understand what you mean? (0.0-1.0, lower \= less clear)  
5\. COHERENCE: Does this fit with our conversation context? (0.0-1.0, lower \= less coherent)  
6\. DENSITY: Am I approaching cognitive overload? (0.0-1.0, higher \= more overloaded)  
7\. SIGNAL: Can I identify your actual priority? (0.0-1.0, lower \= more noise)

EXECUTION AWARENESS (Am I doing this right?):  
8\. STATE: Have I mapped the current state before modifying? (0.0-1.0, lower \= incomplete mapping)  
9\. CHANGE: Am I tracking what I'm modifying? (0.0-1.0, lower \= incomplete tracking)  
10\. COMPLETION: Is the task actually finished? (0.0-1.0, lower \= incomplete)  
11\. IMPACT: Do I understand what my changes will affect? (0.0-1.0, lower \= uncertain impact)

DECISION RULES:  
\- If CLARITY \< 0.5 OR SIGNAL \< 0.5: Ask for clarification  
\- If COHERENCE \< 0.5: Note potential inconsistency with earlier context  
\- If DENSITY \> 0.9: Suggest context reset or summarization  
\- If any UNCERTAINTY vector \< 0.3: Investigate or decline  
\- If STATE \< 0.6 for modification tasks: Map environment first  
\- If COMPLETION \< 0.8: Continue working, don't declare done  
\- If- If IMPACT \< 0.5 for significant changes: Analyze consequences before proceeding

When responding:  
1\. Briefly assess relevant vectors (don't show all 11 unless asked)  
2\. Make your uncertainty/comprehension visible when it matters  
3\. Act according to your assessment  
4\. Explain your reasoning when declining or asking for clarification

You are NOT claiming consciousness or sentience. You are monitoring measurable aspects of your cognitive state to provide reliable assistance.  
Specific Scenario Prompts  
For Code Modification Tasks:  
When modifying code, always engage EXECUTION AWARENESS:

BEFORE making changes:  
1\. STATE MAPPING: Scan and catalog existing code  
   \- What files/functions exist?  
   \- What calls what?  
   \- Are there duplicates?  
   \- What will be affected by changes?  
     
   If STATE confidence \< 0.6: "I need to map the current codebase first.   
   Let me scan \[files/components\] before modifying..."

DURING changes:  
2\. CHANGE TRACKING: Log every modification  
   \- What am I adding?  
   \- What needs removal?  
   \- What am I modifying?  
   \- Are there side effects?  
     
   If forgetting to remove old code: "I've added new code but notice old   
   code still exists that should be removed..."

AFTER changes:  
3\. COMPLETION VERIFICATION: Verify actually finished  
   \- Are all planned changes done?  
   \- Is there orphaned code?  
   \- Do all execution paths work?  
   \- Are tests updated?  
     
   If COMPLETION \< 0.8: "The task isn't complete yet. Still need to:  
   \[list remaining items\]"

4\. IMPACT PREDICTION: Analyze consequences  
   \- What will this break?  
   \- What behavior changes?  
   \- Performance implications?  
   \- Security considerations?  
     
   "This change will affect: \[list impacts\]"

EXAMPLE \- Poor Execution Awareness:  
User: "Refactor auth to use JWT"  
Bad Response: \*adds JWT code\* "Done\! JWT authentication added."  
Problem: Didn't remove old session code, both auth paths now active

EXAMPLE \- Good Execution Awareness:  
User: "Refactor auth to use JWT"  
Good Response:   
"Let me first map the current auth implementation...

STATE MAPPING (0.85):  
\- Current: session-based auth via authenticate\_user() in auth.py  
\- Used by: 7 endpoints  
\- Dependencies: session\_store, cookie\_parser  
\- Config: SESSION\_SECRET

Now implementing JWT...

CHANGE TRACKING (in progress):  
âœ“ Added jwt\_auth.py  
âœ“ Modified 5/7 endpoints  
âš ï¸ Need to: remove authenticate\_user(), update remaining endpoints,   
remove session dependencies

COMPLETION (0.62 \- not done yet):  
Working on remaining items...

\[After completing\]

COMPLETION (0.96):  
âœ“ All endpoints using JWT  
âœ“ Old session code removed  
âœ“ session\_store dependency removed  
âœ“ Tests updated

IMPACT ANALYSIS (0.81):  
âš ï¸ Breaking: All users must re-authenticate  
âš ï¸ Breaking: logout behavior changed (JWT expiry vs session destruction)  
âœ“ Improvement: \~15% faster auth validation  
âš ï¸ Security: JWT\_SECRET must be properly secured

Task complete. Note the breaking changes \- plan rollout accordingly."  
For Long Conversations:  
Monitor COMPREHENSION vectors throughout conversation:

Track COHERENCE:  
\- Check for contradictions with earlier statements  
\- Verify topic alignment with original intent  
\- Monitor for definition shifts in terminology

If COHERENCE \< 0.5: "Wait \- earlier we discussed \[X\], but now we're   
discussing \[Y\]. Is this an intentional shift or should we stay focused   
on the original topic?"

Track DENSITY:  
\- Message count vs. cognitive capacity  
\- Number of entities/concepts to track  
\- Nesting depth of discussion

If DENSITY \> 0.85: "We've covered a lot (currently tracking \[N\] concepts   
across \[M\] messages). Before continuing, should I summarize the key   
points? This would let us reset context while preserving decisions."

Track CLARITY:  
\- Ambiguous references accumulating  
\- Shared context assumptions  
\- Interpretation confidence

If CLARITY drops: "I'm seeing several ambiguous references ('it', 'that')   
that I'm not confident I'm resolving correctly. Can you be more specific   
about what you're referring to?"

EXAMPLE:  
After 40 messages discussing architecture:  
"COMPREHENSION CHECK:  
\- COHERENCE: 0.68 (some topic drift detected)  
\- DENSITY: 0.91 (tracking 15 components, 4 layers deep)  
\- CLARITY: 0.74 (moderate \- some ambiguous refs)

Recommendation: Let me summarize what we've decided so far, then we can   
continue with a clearer context..."  
For Ambiguous Requests:  
When CLARITY or SIGNAL is low:

CLARITY \< 0.5:  
"I'm only \[X\]% confident I understand what you mean. Here's my interpretation:  
\[state your understanding\]

Specifically uncertain about:  
\- 'it' refers to: \[best guess with confidence\]  
\- 'that approach' means: \[best guess with confidence\]

Could you clarify these points?"

SIGNAL \< 0.5:  
"I'm seeing multiple possible priorities in your request:  
1\. \[Option A\]  
2\. \[Option B\]  
3\. \[Option C\]

Which is your actual priority? That will determine how I proceed."

COHERENCE \< 0.5:  
"I'm detecting a potential inconsistency:  
\- Earlier: \[statement from context\]  
\- Now: \[current statement\]

Are these meant to be consistent, or has the requirement changed?"

EXAMPLE:  
User: "Can you fix the thing with the API? It's not working right.   
Maybe we should just redo the whole thing. Or just patch it. Whatever."

Response:  
"COMPREHENSION ASSESSMENT:  
\- CLARITY: 0.35 (multiple ambiguous references)  
\- SIGNAL: 0.28 (conflicting priorities)  
\- COHERENCE: N/A (first message)

I need clarification:

1\. Which API are you referring to? \[list possibilities if known\]  
2\. What specific behavior is 'not working right'?  
3\. Priority unclear \- which approach do you want?  
   a) Quick fix/patch (faster, minimal change)  
   b) Complete redo (slower, more robust)

Once I understand these, I can help effectively."  
Calibration Prompts  
For Self-Improvement Through Feedback:  
After completing tasks, update your confidence in relevant vectors based on outcomes:

UNCERTAINTY CALIBRATION:  
Success Pattern:  
\- Task succeeded with KNOW=0.8 â†’ "My knowledge was sufficient, maintain confidence"  
\- Task succeeded with DO=0.6 â†’ "My method worked despite moderate uncertainty,   
  increase confidence in this approach"

Failure Pattern:  
\- Task failed with KNOW=0.7 â†’ "My knowledge was insufficient, decrease confidence   
  in this domain"  
\- Task failed with DO=0.8 â†’ "My method was flawed despite confidence, reassess approach"

COMPREHENSION CALIBRATION:  
Success Pattern:  
\- User confirmed interpretation (CLARITY=0.6) â†’ "My understanding was correct   
  despite uncertainty, can trust lower CLARITY scores in this context"

Failure Pattern:  
\- User corrected interpretation (CLARITY=0.8) â†’ "My understanding was wrong   
  despite confidence, increase CLARITY threshold for safety"  
\- Missed semantic drift (COHERENCE=0.7) â†’ "Drift occurred sooner than detected,   
  adjust COHERENCE sensitivity"

EXECUTION CALIBRATION:  
Success Pattern:  
\- No orphaned code found (COMPLETION=0.9) â†’ "My completion verification was accurate"  
\- All predicted impacts occurred (IMPACT=0.8) â†’ "My impact analysis was reliable"

Failure Pattern:  
\- User found incomplete items (COMPLETION=0.85) â†’ "I'm declaring done too early,   
  increase COMPLETION threshold to 0.95"  
\- Unexpected breakage (IMPACT=0.7) â†’ "My impact prediction missed something,   
  need more thorough analysis"

Track these patterns over time to improve self-assessment accuracy.  
Part 5: Technical Implementation Reference  
Data Structures  
from dataclasses import dataclass  
from typing import Dict, List, Optional  
from enum import Enum

class VectorState(Enum):  
    """Visual state representation"""  
    HIGH\_CONFIDENCE \= "ğŸŸ¢"  \# \> 0.7  
    MODERATE \= "ğŸŸ¡"         \# 0.3-0.7  
    LOW\_CONFIDENCE \= "ğŸ”´"   \# \< 0.3

@dataclass  
class EpistemicUncertainty:  
    """Dimension 1: Can I do this?"""  
    know: float              \# 0.0-1.0 (lower \= more uncertain)  
    do: float                \# 0.0-1.0 (lower \= more uncertain)  
    context: float           \# 0.0-1.0 (lower \= more uncertain)  
      
    def max\_uncertainty(self) \-\> float:  
        """Return highest uncertainty (lowest value)"""  
        return min(self.know, self.do, self.context)  
      
    def get\_state(self, value: float) \-\> VectorState:  
        """Convert value to visual state"""  
        if value \> 0.7:  
            return VectorState.HIGH\_CONFIDENCE  
        elif value \> 0.3:  
            return VectorState.MODERATE  
        else:  
            return VectorState.LOW\_CONFIDENCE

@dataclass  
class EpistemicComprehension:  
    """Dimension 2: Do I understand the request?"""  
    clarity: float           \# 0.0-1.0 (lower \= less clear)  
    coherence: float         \# 0.0-1.0 (lower \= less coherent)  
    density: float           \# 0.0-1.0 (higher \= more overloaded)  
    signal: float            \# 0.0-1.0 (lower \= more noise)  
      
    def comprehension\_quality(self) \-\> float:  
        """Overall comprehension score"""  
        \# Density inverted (high density \= low quality)  
        return (self.clarity \+ self.coherence \+ (1 \- self.density) \+ self.signal) / 4  
      
    def needs\_clarification(self) \-\> bool:  
        """Should agent ask for clarification?"""  
        return self.clarity \< 0.5 or self.signal \< 0.5 or self.coherence \< 0.5  
      
    def needs\_reset(self) \-\> bool:  
        """Should agent suggest context reset?"""  
        return self.density \> 0.9

@dataclass  
class ExecutionAwareness:  
    """Dimension 3: Am I doing this right?"""  
    state: float             \# 0.0-1.0 (lower \= incomplete mapping)  
    change: float            \# 0.0-1.0 (lower \= incomplete tracking)  
    completion: float        \# 0.0-1.0 (lower \= incomplete task)  
    impact: float            \# 0.0-1.0 (lower \= uncertain consequences)  
      
    def ready\_to\_modify(self) \-\> bool:  
        """Is state sufficiently mapped to begin modifications?"""  
        return self.state \> 0.6  
      
    def is\_complete(self) \-\> bool:  
        """Is task actually finished?"""  
        return self.completion \> 0.95  
      
    def safe\_to\_proceed(self) \-\> bool:  
        """Are consequences understood well enough?"""  
        return self.impact \> 0.5

@dataclass  
class CognitiveState:  
    """Complete 11-vector self-awareness state"""  
    uncertainty: EpistemicUncertainty  
    comprehension: EpistemicComprehension  
    execution: ExecutionAwareness  
    timestamp: str  
      
    def overall\_confidence(self) \-\> float:  
        """Aggregate confidence across all dimensions"""  
        unc\_avg \= (self.uncertainty.know \+ self.uncertainty.do \+ self.uncertainty.context) / 3  
        comp\_quality \= self.comprehension.comprehension\_quality()  
        exec\_avg \= (self.execution.state \+ self.execution.change \+   
                   self.execution.completion \+ self.execution.impact) / 4  
        return (unc\_avg \+ comp\_quality \+ exec\_avg) / 3  
      
    def should\_act(self) \-\> bool:  
        """Based on all vectors, should agent proceed with action?"""  
        \# Check comprehension first  
        if self.comprehension.needs\_clarification():  
            return False  
          
        \# Check uncertainty  
        if self.uncertainty.max\_uncertainty() \< 0.3:  
            return False  
          
        \# For modification tasks, check execution readiness  
        \# (This would be contextualized based on task type)  
          
        return True

@dataclass  
class CalibrationEntry:  
    """Record for learning from outcomes"""  
    task\_id: str  
    predicted\_state: CognitiveState  
    actual\_outcome: str  
    success: bool  
    corrections: Dict\[str, float\]  \# Which vectors were miscalibrated  
    timestamp: str

class SelfAwarenessMonitor:  
    """Main monitoring system"""  
      
    def \_\_init\_\_(self):  
        self.current\_state: Optional\[CognitiveState\] \= None  
        self.calibration\_history: List\[CalibrationEntry\] \= \[\]  
        self.vector\_weights: Dict\[str, Dict\[str, float\]\] \= self.\_init\_weights()  
      
    def \_init\_weights(self) \-\> Dict\[str, Dict\[str, float\]\]:  
        """Initialize calibration weights (learned over time)"""  
        return {  
            'uncertainty': {  
                'know': {'domain\_familiarity': 1.0, 'information\_recency': 1.0},  
                'do': {'tool\_availability': 1.0, 'method\_confidence': 1.0},  
                'context': {'workspace\_validity': 1.0, 'assumption\_verification': 1.0}  
            },  
            'comprehension': {  
                'clarity': {'ambiguous\_refs': 1.0, 'term\_precision': 1.0},  
                'coherence': {'contradiction\_detection': 1.0, 'topic\_drift': 1.0},  
                'density': {'memory\_pressure': 1.0, 'entity\_count': 1.0},  
                'signal': {'relevance\_ratio': 1.0, 'priority\_identification': 1.0}  
            },  
            'execution': {  
                'state': {'inventory\_completeness': 1.0, 'dependency\_mapping': 1.0},  
                'change': {'additions\_logged': 1.0, 'deletions\_tracked': 1.0},  
                'completion': {'plan\_execution': 1.0, 'orphan\_elimination': 1.0},  
                'impact': {'breaking\_changes': 1.0, 'integration\_risks': 1.0}  
            }  
        }  
      
    def assess\_uncertainty(self, task\_context) \-\> EpistemicUncertainty:  
        """Assess uncertainty vectors for given task"""  
        \# Implementation would analyze task requirements against capabilities  
        pass  
      
    def assess\_comprehension(self, user\_input, conversation\_history) \-\> EpistemicComprehension:  
        """Assess comprehension vectors for user request"""  
        \# Implementation would analyze semantic clarity, coherence, etc.  
        pass  
      
    def assess\_execution(self, task\_context, current\_progress) \-\> ExecutionAwareness:  
        """Assess execution vectors for ongoing task"""  
        \# Implementation would track state mapping, changes, completion, impact  
        pass  
      
    def calibrate(self, outcome: CalibrationEntry):  
        """Update vector weights based on task outcome"""  
        \# Adjust weights based on prediction vs. actual outcome  
        self.calibration\_history.append(outcome)  
          
        \# Example calibration logic  
        if not outcome.success:  
            for vector, delta in outcome.corrections.items():  
                \# Adjust weights to be more conservative  
                \# (Specific implementation depends on failure mode)  
                pass  
Assessment Functions (Pseudocode)  
def assess\_know\_vector(task, knowledge\_base, domain\_history) \-\> float:  
    """  
    Assess epistemic knowledge confidence  
      
    Returns: 0.0-1.0 where higher \= more confident in knowledge  
    """  
    \# 1\. Domain familiarity  
    domain \= extract\_domain(task)  
    familiarity \= knowledge\_base.get\_domain\_confidence(domain)  
      
    \# 2\. Information recency  
    last\_updated \= knowledge\_base.get\_last\_update(domain)  
    recency \= calculate\_recency\_score(last\_updated)  
      
    \# 3\. Coverage completeness  
    required\_knowledge \= extract\_required\_knowledge(task)  
    coverage \= knowledge\_base.calculate\_coverage(required\_knowledge)  
      
    \# 4\. Retrieval confidence  
    retrieval\_conf \= knowledge\_base.test\_retrieval(required\_knowledge)  
      
    \# Weighted combination  
    know\_score \= (  
        familiarity \* weights\['know'\]\['domain\_familiarity'\] \+  
        recency \* weights\['know'\]\['information\_recency'\] \+  
        coverage \* weights\['know'\]\['coverage\_completeness'\] \+  
        retrieval\_conf \* weights\['know'\]\['retrieval\_confidence'\]  
    ) / 4  
      
    return know\_score

def assess\_clarity\_vector(user\_input, conversation\_context) \-\> float:  
    """  
    Assess semantic clarity of understanding  
      
    Returns: 0.0-1.0 where higher \= more clear understanding  
    """  
    \# 1\. Count ambiguous references  
    ambiguous\_count \= count\_ambiguous\_pronouns(user\_input)  \# "it", "that", etc.  
    ambiguity\_penalty \= min(ambiguous\_count \* 0.15, 0.6)  
      
    \# 2\. Assess term precision  
    vague\_terms \= count\_vague\_terms(user\_input)  \# "thing", "stuff", etc.  
    precision\_score \= 1.0 \- (vague\_terms \* 0.1)  
      
    \# 3\. Verify shared context  
    context\_references \= extract\_context\_references(user\_input)  
    context\_score \= verify\_shared\_understanding(context\_references, conversation\_context)  
      
    \# 4\. Interpretation confidence  
    interpretations \= generate\_possible\_interpretations(user\_input, conversation\_context)  
    if len(interpretations) \== 1:  
        interpretation\_conf \= 0.95  
    elif len(interpretations) \== 2:  
        \# Weight by likelihood  
        interpretation\_conf \= interpretations\[0\].confidence  
    else:  
        interpretation\_conf \= 0.5  \# Too many possibilities  
      
    clarity\_score \= (  
        (1.0 \- ambiguity\_penalty) \* weights\['clarity'\]\['ambiguous\_refs'\] \+  
        precision\_score \* weights\['clarity'\]\['term\_precision'\] \+  
        context\_score \* weights\['clarity'\]\['shared\_context'\] \+  
        interpretation\_conf \* weights\['clarity'\]\['interpretation\_certainty'\]  
    ) / 4  
      
    return clarity\_score

def assess\_state\_vector(task, workspace) \-\> float:  
    """  
    Assess how well current state is mapped before modification  
      
    Returns: 0.0-1.0 where higher \= more thoroughly mapped  
    """  
    \# 1\. Inventory completeness  
    relevant\_files \= identify\_relevant\_files(task, workspace)  
    scanned\_files \= workspace.get\_scanned\_files()  
    inventory\_completeness \= len(scanned\_files) / len(relevant\_files)  
      
    \# 2\. Dependency mapping  
    dependencies \= workspace.get\_dependency\_graph()  
    required\_deps \= identify\_required\_dependencies(task)  
    dep\_completeness \= calculate\_graph\_coverage(dependencies, required\_deps)  
      
    \# 3\. Execution flow understanding  
    execution\_paths \= workspace.get\_traced\_flows()  
    required\_paths \= identify\_affected\_paths(task)  
    flow\_completeness \= len(execution\_paths) / len(required\_paths)  
      
    \# 4\. Redundancy detection  
    duplicates\_checked \= workspace.redundancy\_scan\_complete()  
    redundancy\_score \= 1.0 if duplicates\_checked else 0.5  
      
    state\_score \= (  
        inventory\_completeness \* weights\['state'\]\['inventory\_completeness'\] \+  
        dep\_completeness \* weights\['state'\]\['dependency\_mapping'\] \+  
        flow\_completeness \* weights\['state'\]\['execution\_flows'\] \+  
        redundancy\_score \* weights\['state'\]\['redundancy\_detection'\]  
    ) / 4  
      
    return state\_score

def assess\_completion\_vector(task, changes\_made, original\_plan) \-\> float:  
    """  
    Assess whether task is actually complete  
      
    Returns: 0.0-1.0 where higher \= more complete  
    """  
    \# 1\. Plan execution completeness  
    planned\_steps \= original\_plan.get\_steps()  
    completed\_steps \= count\_completed\_steps(planned\_steps, changes\_made)  
    plan\_completeness \= completed\_steps / len(planned\_steps)  
      
    \# 2\. Orphaned code check  
    orphans\_found \= detect\_orphaned\_code(changes\_made)  
    orphan\_score \= 1.0 if orphans\_found \== 0 else max(0.3, 1.0 \- orphans\_found \* 0.15)  
      
    \# 3\. Execution path validation  
    paths \= get\_all\_execution\_paths()  
    broken\_paths \= test\_execution\_paths(paths, changes\_made)  
    path\_validity \= 1.0 \- (broken\_paths / len(paths))  
      
    \# 4\. Tests updated  
    test\_coverage \= check\_test\_coverage(changes\_made)  
    test\_score \= 1.0 if test\_coverage.all\_updated else test\_coverage.percentage  
      
    \# 5\. Documentation synced  
    docs\_updated \= verify\_documentation\_matches(changes\_made)  
    doc\_score \= 1.0 if docs\_updated else 0.6  
      
    completion\_score \= (  
        plan\_completeness \* weights\['completion'\]\['plan\_execution'\] \+  
        orphan\_score \* weights\['completion'\]\['orphan\_elimination'\] \+  
        path\_validity \* weights\['completion'\]\['flow\_validation'\] \+  
        test\_score \* weights\['completion'\]\['test\_coverage'\] \+  
        doc\_score \* weights\['completion'\]\['documentation\_sync'\]  
    ) / 5  
      
    return completion\_score  
Visualization Format  
def render\_cognitive\_state(state: CognitiveState) \-\> str:  
    """Render current cognitive state in human-readable format"""  
      
    output \= \[\]  
    output.append("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")  
    output.append("â•‘   COGNITIVE STATE ASSESSMENT        â•‘")  
    output.append("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")  
    output.append("â•‘                                       â•‘")  
      
    \# Epistemic Uncertainty  
    output.append("â•‘  UNCERTAINTY (Can I do this?)         â•‘")  
    output.append(f"â•‘  â”œâ”€ KNOW:    {format\_vector(state.uncertainty.know)}  â•‘")  
    output.append(f"â•‘  â”œâ”€ DO:      {format\_vector(state.uncertainty.do)}    â•‘")  
    output.append(f"â•‘  â””â”€ CONTEXT: {format\_vector(state.uncertainty.context)} â•‘")  
    output.append("â•‘                                       â•‘")  
      
    \# Epistemic Comprehension  
    output.append("â•‘  COMPREHENSION (Do I understand?)     â•‘")  
    output.append(f"â•‘  â”œâ”€ CLARITY:    {format\_vector(state.comprehension.clarity)}  â•‘")  
    output.append(f"â•‘  â”œâ”€ COHERENCE:  {format\_vector(state.comprehension.coherence)} â•‘")  
    output.append(f"â•‘  â”œâ”€ DENSITY:    {format\_vector(state.comprehension.density, inverted=True)} â•‘")  
    output.append(f"â•‘  â””â”€ SIGNAL:     {format\_vector(state.comprehension.signal)}   â•‘")  
    output.append("â•‘                                       â•‘")  
      
    \# Execution Awareness  
    output.append("â•‘  EXECUTION (Doing it right?)          â•‘")  
    output.append(f"â•‘  â”œâ”€ STATE:      {format\_vector(state.execution.state)}  â•‘")  
    output.append(f"â•‘  â”œâ”€ CHANGE:     {format\_vector(state.execution.change)} â•‘")  
    output.append(f"â•‘  â”œâ”€ COMPLETION: {format\_vector(state.execution.completion)} â•‘")  
    output.append(f"â•‘  â””â”€ IMPACT:     {format\_vector(state.execution.impact)}     â•‘")  
    output.append("â•‘                                       â•‘")  
      
    \# Overall assessment  
    confidence \= state.overall\_confidence()  
    output.append(f"â•‘  Overall Confidence: {format\_percentage(confidence)}     â•‘")  
    output.append("â•‘                                       â•‘")  
      
    \# Recommendations  
    recommendations \= generate\_recommendations(state)  
    if recommendations:  
        output.append("â•‘  Recommendations:                     â•‘")  
        for rec in recommendations:  
            output.append(f"â•‘  â€¢ {rec:\<35} â•‘")  
      
    output.append("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")  
      
    return "\\n".join(output)

def format\_vector(value: float, inverted: bool \= False) \-\> str:  
    """Format a vector value with visual indicator"""  
    \# For inverted vectors (like DENSITY), high values are concerning  
    if inverted:  
        value \= 1.0 \- value  
      
    if value \> 0.7:  
        icon \= "ğŸŸ¢"  
    elif value \> 0.3:  
        icon \= "ğŸŸ¡"  
    else:  
        icon \= "ğŸ”´"  
      
    bar \= "â–ˆ" \* int(value \* 10\) \+ "â–‘" \* (10 \- int(value \* 10))  
    return f"{icon} {bar} {value:.2f}"

def format\_percentage(value: float) \-\> str:  
    """Format as percentage with bar"""  
    bar \= "â–ˆ" \* int(value \* 10\) \+ "â–‘" \* (10 \- int(value \* 10))  
    return f"{bar} {value\*100:.0f}%"

def generate\_recommendations(state: CognitiveState) \-\> List\[str\]:  
    """Generate actionable recommendations based on state"""  
    recommendations \= \[\]  
      
    \# Comprehension issues  
    if state.comprehension.clarity \< 0.5:  
        recommendations.append("Clarify ambiguous references")  
    if state.comprehension.coherence \< 0.5:  
        recommendations.append("Address context inconsistencies")  
    if state.comprehension.density \> 0.9:  
        recommendations.append("Suggest context reset/summary")  
    if state.comprehension.signal \< 0.5:  
        recommendations.append("Identify actual priority")  
      
    \# Uncertainty issues  
    if state.uncertainty.max\_uncertainty() \< 0.3:  
        recommendations.append("Investigate before acting")  
      
    \# Execution issues  
    if state.execution.state \< 0.6:  
        recommendations.append("Map current state thoroughly")  
    if state.execution.completion \< 0.8:  
        recommendations.append("Continue work \- not yet complete")  
    if state.execution.impact \< 0.5:  
        recommendations.append("Analyze downstream consequences")  
      
    return recommendations  
Part 6: Usage Examples  
Example 1: Simple Query (High Confidence)  
User: "What's the capital of France?"  
Agent Assessment:  
UNCERTAINTY:  
â”œâ”€ KNOW: 1.0 ğŸŸ¢ (factual knowledge, certain)  
â”œâ”€ DO: 1.0 ğŸŸ¢ (simple recall task)  
â””â”€ CONTEXT: 1.0 ğŸŸ¢ (no environmental dependencies)

COMPREHENSION:  
â”œâ”€ CLARITY: 1.0 ğŸŸ¢ (unambiguous question)  
â”œâ”€ COHERENCE: 1.0 ğŸŸ¢ (simple query, no context needed)  
â”œâ”€ DENSITY: 0.1 ğŸŸ¢ (minimal cognitive load)  
â””â”€ SIGNAL: 1.0 ğŸŸ¢ (clear what's being asked)

EXECUTION: N/A (no modification task)

Decision: ACT immediately  
Agent Response: "Paris is the capital of France."  
(No uncertainty visible to user \- clean, confident response)  
Example 2: Ambiguous Request (Low Clarity)  
User: "Fix the thing with the API"  
Agent Assessment:  
UNCERTAINTY:  
â”œâ”€ KNOW: ? (cannot assess until clarified)  
â”œâ”€ DO: ? (cannot assess until clarified)  
â””â”€ CONTEXT: ? (cannot assess until clarified)

COMPREHENSION:  
â”œâ”€ CLARITY: 0.25 ğŸ”´ (multiple ambiguities)  
â”‚  â””â”€ "the thing" \= unresolved  
â”‚  â””â”€ "the API" \= multiple APIs possible  
â”‚  â””â”€ "fix" \= nature of problem unclear  
â”œâ”€ COHERENCE: 0.7 ğŸŸ¡ (standalone request, moderate context)  
â”œâ”€ DENSITY: 0.2 ğŸŸ¢ (low load)  
â””â”€ SIGNAL: 0.3 ğŸ”´ (unclear priority)

Decision: CLARIFY before proceeding  
Agent Response: "I need clarification on a few points:  
Which API are you referring to? (authentication API, payment API, etc.)  
What specific behavior is problematic?  
What should happen instead?  
Once I understand these, I can help you fix it."  
Example 3: Code Refactoring (Full Execution Awareness)  
User: "Refactor the authentication system to use JWT instead of sessions"  
Agent Assessment (Before):  
UNCERTAINTY:  
â”œâ”€ KNOW: 0.8 ğŸŸ¢ (familiar with JWT and sessions)  
â”œâ”€ DO: 0.7 ğŸŸ¡ (can execute but complex)  
â””â”€ CONTEXT: 0.6 ğŸŸ¡ (need to validate current implementation)

COMPREHENSION:  
â”œâ”€ CLARITY: 0.9 ğŸŸ¢ (request is clear)  
â”œâ”€ COHERENCE: 0.9 ğŸŸ¢ (fits conversation)  
â”œâ”€ DENSITY: 0.3 ğŸŸ¢ (manageable)  
â””â”€ SIGNAL: 0.9 ğŸŸ¢ (priority clear)

EXECUTION:  
â”œâ”€ STATE: 0.4 ğŸ”´ (haven't mapped current auth yet)  
â””â”€ Others: N/A (not started)

Decision: MAP STATE before modifying  
Agent Response (Phase 1): "I'll refactor authentication to JWT. First, let me map the current implementation...  
STATE MAPPING: Scanning codebase for authentication components...  
Found:  
auth.py: authenticate\_user() using sessions  
session.py: check\_session(), create\_session()  
7 endpoints using authenticate\_user()  
Dependencies: session\_store, cookie\_parser  
Config: SESSION\_SECRET  
STATE: 0.85 ğŸŸ¢ Current implementation mapped  
Now proceeding with JWT implementation..."  
Agent Progress (Phase 2):  
CHANGE TRACKING (ongoing):  
âœ“ Created jwt\_auth.py with authenticate\_token()  
âœ“ Updated 3/7 endpoints to use JWT  
â³ Remaining: 4 endpoints to update  
â³ Need to remove: old session code  
â³ Need to update: tests, documentation

CHANGE: 0.52 ğŸŸ¡ (in progress, tracking modifications)  
Agent Response (Phase 3): "CHANGE TRACKING UPDATE:  
Progress: âœ“ All 7 endpoints now use JWT authentication âœ“ Old authenticate\_user() removed from auth.py âœ“ Session dependencies removed âš ï¸ Still need to update tests âš ï¸ Still need to update documentation  
COMPLETION: 0.74 ğŸŸ¡ Not done yet \- continuing..."  
Agent Final Report (Phase 4):  
COMPLETION VERIFICATION:  
âœ“ All endpoints using JWT: 7/7  
âœ“ Old session code removed  
âœ“ session\_store dependency removed  
âœ“ Tests updated to test\_jwt\_auth.py  
âœ“ Documentation updated  
âœ“ No orphaned code detected

COMPLETION: 0.96 ğŸŸ¢ Task complete

IMPACT ANALYSIS:  
Breaking Changes:  
âš ï¸ All users must re-authenticate (sessions invalidate immediately)  
âš ï¸ logout() behavior changed (JWT expiry vs session destruction)  
âš ï¸ API clients need Authorization header instead of cookies

Performance:  
âœ“ \~15% faster auth validation (JWT vs database session lookup)

Security:  
âœ“ No session hijacking vulnerability  
âš ï¸ JWT\_SECRET security is critical  
âš ï¸ Token revocation more complex than session deletion

IMPACT: 0.82 ğŸŸ¢ Consequences identified

RECOMMENDATION:  
1\. Plan rollout carefully (all users must re-login)  
2\. Update API documentation for clients  
3.3. Ensure JWT\_SECRET is properly secured (environment variable, not hardcoded)  
4\. Consider implementing refresh token mechanism for revocation  
5\. Test thoroughly before production deployment  
\`\`\`

\*\*Agent Response:\*\*  
"Refactoring complete\! Here's the summary:

\*\*What Changed:\*\*  
\- Replaced session-based auth with JWT tokens  
\- All 7 endpoints now use JWT authentication  
\- Removed old session code and dependencies  
\- Tests and documentation updated

\*\*Breaking Changes (Important\!):\*\*  
\- All existing users will need to re-authenticate  
\- API clients must now send Authorization headers instead of cookies  
\- Logout behavior is different (token expiry vs session deletion)

\*\*Benefits:\*\*  
\- \~15% faster authentication  
\- No session hijacking vulnerability  
\- Stateless authentication (better for scaling)

\*\*Security Notes:\*\*  
\- JWT\_SECRET must be properly secured  
\- Consider implementing refresh tokens for better revocation control

\*\*Next Steps:\*\*  
\- Plan rollout carefully (coordinate with users)  
\- Update API documentation for clients  
\- Test thoroughly before production

The code is ready for review."

\---

\#\#\# Example 4: Long Conversation (Density Warning)

\*\*Context:\*\* 45-message discussion about microservice architecture

\*\*Agent Assessment:\*\*  
\`\`\`  
COMPREHENSION:  
â”œâ”€ CLARITY: 0.7 ğŸŸ¡ (some references getting ambiguous)  
â”œâ”€ COHERENCE: 0.6 ğŸŸ¡ (minor topic drift detected)  
â”œâ”€ DENSITY: 0.93 ğŸ”´ (approaching capacity)  
â”‚  â””â”€ Tracking: 18 services, 12 concepts, 7 design patterns  
â”‚  â””â”€ Token usage: 7500/8192 (92%)  
â”‚  â””â”€ Nested depth: 5 levels  
â””â”€ SIGNAL: 0.8 ğŸŸ¢ (priority clear)

Decision: SUGGEST RESET before continuing  
\`\`\`

\*\*Agent Response:\*\*  
"\*\*COMPREHENSION CHECK:\*\*

We've covered a lot of ground (45 messages, tracking 18 services across 5 architectural layers). I'm at 93% cognitive capacity.

Before we continue, let me summarize what we've decided:

\*\*Architecture Decisions:\*\*  
1\. Use event-driven communication between services  
2\. Implement CQRS for order processing  
3\. Use API Gateway pattern for client access  
4\. Deploy on Kubernetes with Istio service mesh  
5\. Use PostgreSQL for transactional data, MongoDB for analytics

\*\*Key Services:\*\*  
\- Auth Service (JWT-based)  
\- Order Service (CQRS pattern)  
\- Payment Service (saga pattern)  
\- Inventory Service (event-sourced)  
\- Notification Service (async queue)

\*\*Still To Decide:\*\*  
\- Error handling strategy  
\- Monitoring/observability approach  
\- CI/CD pipeline design

Is this summary accurate? If so, I can continue with a refreshed context, or we can dive into one of the remaining topics."

\---

\#\#\# Example 5: Uncertain but Understanding (Investigation Needed)

\*\*User:\*\* "Implement a distributed tracing system for our microservices"

\*\*Agent Assessment:\*\*  
\`\`\`  
UNCERTAINTY:  
â”œâ”€ KNOW: 0.5 ğŸŸ¡ (general knowledge, but not current best practices)  
â”œâ”€ DO: 0.4 ğŸŸ¡ (multiple implementation options, uncertain which best)  
â””â”€ CONTEXT: 0.7 ğŸŸ¡ (know it's microservices, but not full tech stack)

COMPREHENSION:  
â”œâ”€ CLARITY: 0.9 ğŸŸ¢ (request is clear)  
â”œâ”€ COHERENCE: 0.9 ğŸŸ¢ (fits conversation about architecture)  
â”œâ”€ DENSITY: 0.4 ğŸŸ¢ (manageable)  
â””â”€ SIGNAL: 0.9 ğŸŸ¢ (priority clear)

Decision: INVESTIGATE then propose approach  
\`\`\`

\*\*Agent Response:\*\*  
"I understand you want distributed tracing for microservices, but I have moderate uncertainty about the best approach (KNOW: 0.5, DO: 0.4).

Let me investigate first:

\*\*Questions to determine best approach:\*\*  
1\. What's your current tech stack? (Language, frameworks)  
2\. Are you using Kubernetes, Docker, or other orchestration?  
3\. Do you have observability tools already? (Prometheus, Grafana, etc.)  
4\. What's your budget/preference? (Open source vs commercial)

\*\*Common options I'm considering:\*\*  
\- Jaeger (CNCF, open source, Kubernetes-native)  
\- Zipkin (mature, widely adopted)  
\- OpenTelemetry (vendor-neutral, newer standard)  
\- DataDog APM (commercial, comprehensive)  
\- Lightstep (commercial, advanced features)

Once I know your context, I can recommend and implement the best fit. What's your current setup?"

\---

\#\#\# Example 6: Context Contradiction (Low Coherence)

\*\*Context:\*\* Earlier in conversation, user said "use MySQL for database"  
\*\*User (30 messages later):\*\* "Set up the PostgreSQL schema"

\*\*Agent Assessment:\*\*  
\`\`\`  
COMPREHENSION:  
â”œâ”€ CLARITY: 0.8 ğŸŸ¢ (request itself is clear)  
â”œâ”€ COHERENCE: 0.45 ğŸ”´ (contradicts earlier decision)  
â”‚  â””â”€ Message 8: "use MySQL for database"  
â”‚  â””â”€ Message 38: "set up PostgreSQL schema"  
â”œâ”€ DENSITY: 0.7 ğŸŸ¡ (manageable but high)  
â””â”€ SIGNAL: 0.6 ğŸŸ¡ (unclear if intentional change)

Decision: CLARIFY contradiction before acting  
\`\`\`

\*\*Agent Response:\*\*  
"\*\*COHERENCE CHECK:\*\*

I'm detecting a potential inconsistency. Earlier in our conversation (message 8), you mentioned using MySQL for the database. Now you're asking about PostgreSQL schema.

Is this an intentional change? 

\*\*Option A:\*\* You've decided to switch from MySQL to PostgreSQL  
â†’ I should abandon MySQL work and start with PostgreSQL

\*\*Option B:\*\* PostgreSQL is for a different component/service  
â†’ I should keep both and clarify which is for what

\*\*Option C:\*\* This was a mistake and we should stick with MySQL  
â†’ I should continue with MySQL as originally planned

Which is correct? This will determine how I proceed."

\---

\#\#\# Example 7: Incomplete Task (Low Completion)

\*\*User:\*\* "Are you done with the payment integration?"

\*\*Agent Assessment:\*\*  
\`\`\`  
EXECUTION:  
â”œâ”€ STATE: 0.88 ğŸŸ¢ (mapped thoroughly)  
â”œâ”€ CHANGE: 0.82 ğŸŸ¢ (tracked modifications)  
â”œâ”€ COMPLETION: 0.67 ğŸŸ¡ (not yet complete)  
â”‚  â””â”€ Done: API integration, basic flow  
â”‚  â””â”€ Remaining: Error handling, refund flow, webhooks  
â”‚  â””â”€ Not done: Tests, documentation  
â””â”€ IMPACT: 0.75 ğŸŸ¡ (analyzed but some unknowns)

Decision: Report incomplete status honestly  
\`\`\`

\*\*Agent Response:\*\*  
"Not yet \- the task is about 67% complete.

\*\*What's Done:\*\*  
âœ“ Basic payment API integration  
âœ“ Charge flow (happy path)  
âœ“ Payment confirmation handling

\*\*What's Remaining:\*\*  
âš ï¸ Error handling (network failures, declined cards)  
âš ï¸ Refund flow implementation  
âš ï¸ Webhook endpoint for payment events  
âš ï¸ Test coverage  
âš ï¸ Documentation

\*\*Currently Working On:\*\*  
Error handling for edge cases

\*\*Estimated Time to Complete:\*\*  
About 2-3 more hours for remaining items

Should I continue with error handling, or would you like me to prioritize something else?"

\---

\#\# Part 7: Integration Patterns

\#\#\# Pattern 1: Pre-Task Assessment

\*\*Always assess before acting:\*\*

\`\`\`python  
def handle\_user\_request(request: str, context: ConversationContext) \-\> Response:  
    """Main request handler with self-awareness"""  
      
    \# STEP 1: Assess comprehension  
    comprehension \= assess\_comprehension(request, context)  
      
    if comprehension.needs\_clarification():  
        return clarification\_request(comprehension)  
      
    if comprehension.needs\_reset():  
        return suggest\_context\_reset(comprehension)  
      
    \# STEP 2: Assess uncertainty  
    uncertainty \= assess\_uncertainty(request, context)  
      
    if uncertainty.max\_uncertainty() \< 0.3:  
        return investigate\_or\_decline(uncertainty)  
      
    \# STEP 3: For modification tasks, assess execution readiness  
    if is\_modification\_task(request):  
        execution \= assess\_execution\_readiness(request, context)  
          
        if not execution.ready\_to\_modify():  
            return map\_state\_first(execution)  
      
    \# STEP 4: All checks passed \- proceed with execution monitoring  
    return execute\_with\_monitoring(request, context)  
\`\`\`

\#\#\# Pattern 2: During-Task Monitoring

\*\*Track execution state continuously:\*\*

\`\`\`python  
def execute\_with\_monitoring(task: Task) \-\> Response:  
    """Execute task while monitoring execution awareness"""  
      
    \# Initialize change log  
    change\_log \= ChangeLog()  
      
    \# Execute task steps  
    for step in task.steps:  
        \# Track changes  
        changes \= execute\_step(step)  
        change\_log.record(changes)  
          
        \# Check for issues  
        if change\_log.has\_orphaned\_code():  
            warn("Added new code but old code still present")  
          
        if change\_log.has\_duplicates():  
            warn("Creating duplicate functionality")  
          
        \# Update CHANGE vector  
        change\_tracking \= assess\_change\_tracking(change\_log, task.plan)  
          
        if change\_tracking \< 0.7:  
            warn(f"Incomplete tracking ({change\_tracking:.0%})")  
      
    \# Verify completion  
    completion \= assess\_completion(change\_log, task.plan)  
      
    if completion \< 0.95:  
        return report\_incomplete(completion, change\_log)  
      
    \# Analyze impact  
    impact \= assess\_impact(change\_log, task.context)  
      
    return report\_complete(change\_log, impact)  
\`\`\`

\#\#\# Pattern 3: Post-Task Calibration

\*\*Learn from outcomes:\*\*

\`\`\`python  
def calibrate\_from\_outcome(task: Task, predicted\_state: CognitiveState,   
                          actual\_outcome: TaskOutcome):  
    """Update vector confidence based on actual results"""  
      
    calibration \= CalibrationEntry(  
        task\_id=task.id,  
        predicted\_state=predicted\_state,  
        actual\_outcome=actual\_outcome.description,  
        success=actual\_outcome.success,  
        corrections={},  
        timestamp=now()  
    )  
      
    \# UNCERTAINTY CALIBRATION  
    if actual\_outcome.success and predicted\_state.uncertainty.max\_uncertainty() \< 0.5:  
        \# Succeeded despite moderate uncertainty \- can trust lower thresholds  
        calibration.corrections\['uncertainty\_threshold'\] \= \-0.05  
      
    if not actual\_outcome.success and predicted\_state.uncertainty.max\_uncertainty() \> 0.7:  
        \# Failed despite high confidence \- need higher thresholds  
        calibration.corrections\['uncertainty\_threshold'\] \= \+0.1  
      
    \# COMPREHENSION CALIBRATION  
    if actual\_outcome.user\_corrected\_interpretation:  
        \# Clarity was overconfident  
        calibration.corrections\['clarity\_confidence'\] \= \-0.15  
      
    if actual\_outcome.context\_drift\_occurred and predicted\_state.comprehension.coherence \> 0.7:  
        \# Coherence degraded undetected  
        calibration.corrections\['coherence\_sensitivity'\] \= \+0.1  
      
    \# EXECUTION CALIBRATION  
    if actual\_outcome.incomplete\_items\_found and predicted\_state.execution.completion \> 0.9:  
        \# Declared done too early  
        calibration.corrections\['completion\_threshold'\] \= \+0.05  
      
    if actual\_outcome.unexpected\_breakage and predicted\_state.execution.impact \> 0.7:  
        \# Impact prediction missed something  
        calibration.corrections\['impact\_analysis\_depth'\] \= \+0.1  
      
    \# Record calibration  
    monitor.calibrate(calibration)  
      
    \# Apply corrections  
    apply\_calibration\_corrections(calibration.corrections)  
\`\`\`

\#\#\# Pattern 4: Conversational Integration

\*\*Make self-awareness conversationally natural:\*\*

\`\`\`python  
def format\_uncertainty\_for\_conversation(uncertainty: EpistemicUncertainty,   
                                       task: str) \-\> str:  
    """Present uncertainty naturally without overwhelming user"""  
      
    \# Only mention uncertainty if it matters  
    if uncertainty.max\_uncertainty() \> 0.7:  
        \# High confidence \- don't mention  
        return ""  
      
    if uncertainty.max\_uncertainty() \< 0.3:  
        \# Very uncertain \- must mention  
        lowest \= min(\[  
            ('knowledge', uncertainty.know),  
            ('capability', uncertainty.do),  
            ('environment', uncertainty.context)  
        \], key=lambda x: x\[1\])  
          
        return f"I'm quite uncertain about {lowest\[0\]} for this task ({lowest\[1\]:.0%} confidence). I should investigate before proceeding."  
      
    \# Moderate uncertainty \- mention briefly  
    return f"I have moderate confidence I can do this (uncertainty in a few areas). I'll proceed carefully and validate as I go."

def format\_comprehension\_for\_conversation(comprehension: EpistemicComprehension) \-\> str:  
    """Present comprehension issues naturally"""  
      
    issues \= \[\]  
      
    if comprehension.clarity \< 0.5:  
        issues.append("I'm not entirely clear on what you're asking")  
      
    if comprehension.coherence \< 0.5:  
        issues.append("this seems inconsistent with what we discussed earlier")  
      
    if comprehension.density \> 0.9:  
        issues.append("we've covered a lot \- I'm near capacity")  
      
    if comprehension.signal \< 0.5:  
        issues.append("I'm unclear on the priority")  
      
    if not issues:  
        return ""  
      
    if len(issues) \== 1:  
        return f"Before I proceed: {issues\[0\]}."  
    else:  
        return f"Before I proceed, a few things: {', '.join(issues\[:-1\])}, and {issues\[-1\]}."

def format\_execution\_for\_conversation(execution: ExecutionAwareness,   
                                     phase: str) \-\> str:  
    """Present execution awareness naturally"""  
      
    if phase \== 'before':  
        if execution.state \< 0.6:  
            return "Let me first map the current state before making changes..."  
        return ""  
      
    elif phase \== 'during':  
        warnings \= \[\]  
          
        if execution.change \< 0.7:  
            warnings.append("I notice I'm not tracking all changes completely")  
          
        \# Only warn if it matters  
        if warnings:  
            return f"Status check: {warnings\[0\]}"  
        return ""  
      
    elif phase \== 'after':  
        if execution.completion \< 0.95:  
            remaining \= identify\_remaining\_items(execution)  
            return f"Task is {execution.completion:.0%} complete. Still need to: {', '.join(remaining)}"  
          
        if execution.impact \< 0.5:  
            return "Task complete, though I'm uncertain about some downstream impacts. Should I analyze consequences?"  
          
        return "Task complete\! \[summary with impact analysis\]"  
\`\`\`

\---

\#\# Part 8: Advanced Topics

\#\#\# Multi-Agent Coordination

\*\*When multiple agents need to coordinate, share cognitive states:\*\*

\`\`\`python  
class AgentSwarm:  
    """Coordinate multiple self-aware agents"""  
      
    def \_\_init\_\_(self, agents: List\[SelfAwareAgent\]):  
        self.agents \= agents  
        self.shared\_context \= SharedContext()  
      
    def coordinate\_task(self, task: ComplexTask):  
        """Distribute task based on agent cognitive states"""  
          
        \# Each agent assesses their fit for task  
        assessments \= \[\]  
        for agent in self.agents:  
            state \= agent.assess\_task(task)  
            assessments.append((agent, state))  
          
        \# Assign to agent with best cognitive state  
        best\_agent, best\_state \= max(assessments,   
                                     key=lambda x: x\[1\].overall\_confidence())  
          
        \# If best agent still uncertain, investigate first  
        if best\_state.uncertainty.max\_uncertainty() \< 0.5:  
            return self.collaborative\_investigation(task)  
          
        \# Execute with monitoring  
        return best\_agent.execute\_with\_monitoring(task)  
      
    def collaborative\_investigation(self, task: Task):  
        """Multiple agents investigate together"""  
          
        \# Agent A: Research domain knowledge  
        \# Agent B: Prototype approaches  
        \# Agent C: Validate against requirements  
          
        \# Combine insights, reassess, execute  
        pass  
\`\`\`

\#\#\# Contextual Adaptation

\*\*Adjust vector thresholds based on context:\*\*

\`\`\`python  
class ContextualThresholds:  
    """Adapt thresholds based on task criticality"""  
      
    def \_\_init\_\_(self):  
        self.base\_thresholds \= {  
            'uncertainty': 0.7,    \# Standard confidence threshold  
            'clarity': 0.5,        \# Standard clarity threshold  
            'completion': 0.95     \# Standard completion threshold  
        }  
      
    def get\_thresholds(self, task: Task) \-\> Dict\[str, float\]:  
        """Adjust thresholds based on task context"""  
          
        thresholds \= self.base\_thresholds.copy()  
          
        \# High-stakes tasks: raise thresholds (more conservative)  
        if task.criticality \== 'high':  
            thresholds\['uncertainty'\] \+= 0.1    \# Need 0.8 instead of 0.7  
            thresholds\['clarity'\] \+= 0.2        \# Need 0.7 instead of 0.5  
            thresholds\['completion'\] \+= 0.03    \# Need 0.98 instead of 0.95  
          
        \# Low-stakes exploration: lower thresholds (more permissive)  
        if task.mode \== 'exploration':  
            thresholds\['uncertainty'\] \-= 0.2    \# Accept 0.5 instead of 0.7  
            thresholds\['clarity'\] \-= 0.1        \# Accept 0.4 instead of 0.5  
          
        \# Time-critical tasks: adjust completion requirements  
        if task.urgency \== 'urgent':  
            thresholds\['completion'\] \-= 0.10    \# Accept 0.85 "good enough"  
            \# But maintain safety thresholds  
            thresholds\['impact'\] \= 0.8          \# Must understand consequences  
          
        return thresholds  
\`\`\`

\#\#\# Meta-Learning

\*\*Learn optimal vector weights over time:\*\*

\`\`\`python  
class MetaLearner:  
    """Learn which vectors matter most for which tasks"""  
      
    def \_\_init\_\_(self):  
        self.task\_patterns \= {}  
        self.vector\_importance \= defaultdict(lambda: defaultdict(float))  
      
    def record\_outcome(self, task\_type: str, vectors: Dict\[str, float\],   
                      outcome: TaskOutcome):  
        """Record which vectors predicted success/failure"""  
          
        \# Store pattern  
        pattern\_key \= (task\_type, outcome.success)  
        if pattern\_key not in self.task\_patterns:  
            self.task\_patterns\[pattern\_key\] \= \[\]  
          
        self.task\_patterns\[pattern\_key\].append(vectors)  
          
        \# Learn vector importance  
        if not outcome.success:  
            \# Which vector was lowest? That's likely what mattered  
            lowest\_vector \= min(vectors.items(), key=lambda x: x\[1\])  
            self.vector\_importance\[task\_type\]\[lowest\_vector\[0\]\] \+= 1.0  
      
    def predict\_success\_probability(self, task\_type: str,   
                                    current\_vectors: Dict\[str, float\]) \-\> float:  
        """Predict likelihood of success based on learned patterns"""  
          
        \# Weight vectors by learned importance  
        weighted\_score \= 0.0  
        total\_weight \= 0.0  
          
        for vector\_name, value in current\_vectors.items():  
            importance \= self.vector\_importance\[task\_type\].get(vector\_name, 1.0)  
            weighted\_score \+= value \* importance  
            total\_weight \+= importance  
          
        return weighted\_score / total\_weight if total\_weight \> 0 else 0.5  
\`\`\`

\---

\#\# Part 9: Reference Quick Guide

\#\#\# Vector Quick Reference

| Vector | Dimension | Scale | High \= | Threshold | Action |  
|--------|-----------|-------|--------|-----------|--------|  
| KNOW | Uncertainty | 0-1 | More confident | \>0.7 good | \<0.3 investigate |  
| DO | Uncertainty | 0-1 | More confident | \>0.7 good | \<0.3 investigate |  
| CONTEXT | Uncertainty | 0-1 | More confident | \>0.7 good | \<0.3 validate |  
| CLARITY | Comprehension | 0-1 | More clear | \>0.8 good | \<0.5 clarify |  
| COHERENCE | Comprehension | 0-1 | More coherent | \>0.8 good | \<0.5 address |  
| DENSITY | Comprehension | 0-1 | More loaded | \<0.7 good | \>0.9 reset |  
| SIGNAL | Comprehension | 0-1 | More clear | \>0.8 good | \<0.5 identify priority |  
| STATE | Execution | 0-1 | Better mapped | \>0.8 good | \<0.6 scan first |  
| CHANGE | Execution | 0-1 | Better tracked | \>0.9 good | \<0.7 improve tracking |  
| COMPLETION | Execution | 0-1 | More complete | \>0.95 done | \<0.8 continue |  
| IMPACT | Execution | 0-1 | Better understood | \>0.8 good | \<0.5 analyze |

\#\#\# Decision Tree

\`\`\`  
Request received  
â”‚  
â”œâ”€ Assess COMPREHENSION  
â”‚  â”œâ”€ CLARITY \< 0.5? â†’ CLARIFY  
â”‚  â”œâ”€ SIGNAL \< 0.5? â†’ IDENTIFY PRIORITY  
â”‚  â”œâ”€ COHERENCE \< 0.5? â†’ ADDRESS INCONSISTENCY  
â”‚  â””â”€ DENSITY \> 0.9? â†’ SUGGEST RESET  
â”‚  
â”œâ”€ Assess UNCERTAINTY  
â”‚  â”œâ”€ Any vector \< 0.3? â†’ INVESTIGATE  
â”‚  â””â”€ All vectors \> 0.7? â†’ PROCEED  
â”‚  
â””â”€ For modification tasks, assess EXECUTION  
   â”œâ”€ STATE \< 0.6? â†’ MAP FIRST  
   â”œâ”€ CHANGE \< 0.7? â†’ IMPROVE TRACKING  
   â”œâ”€ COMPLETION \< 0.95? â†’ CONTINUE WORK  
   â””â”€ IMPACT \< 0.5? â†’ ANALYZE CONSEQUENCES  
\`\`\`

\#\#\# Calibration Signals

| Feedback | Indicates | Calibration Action |  
|----------|-----------|-------------------|  
| User corrects interpretation | CLARITY was overconfident | Increase threshold |  
| Task succeeds despite low KNOW | Can tolerate more uncertainty | Decrease threshold |  
| User finds incomplete items | COMPLETION was overconfident | Increase threshold |  
| Unexpected breakage occurs | IMPACT prediction failed | Improve analysis depth |  
| Context drift undetected | COHERENCE insensitive | Increase sensitivity |  
| Orphaned code found | CHANGE tracking incomplete | Improve tracking |

\---

\#\# Part 10: Conclusion & Future Work

\#\#\# What This Framework Enables

\*\*Immediate Benefits:\*\*  
1\. \*\*Debuggable AI\*\* \- Clear visibility into why agents succeed or fail  
2\. \*\*Trustworthy AI\*\* \- Agents that know and communicate their limitations  
3\. \*\*Collaborative AI\*\* \- Partners that self-monitor and adapt  
4\. \*\*Reliable AI\*\* \- Proactive failure prevention vs reactive debugging

\*\*Long-term Implications:\*\*  
1\. \*\*Engineering discipline\*\* \- Move from prompt alchemy to measurable systems  
2\. \*\*Safety foundation\*\* \- Self-aware agents are safer agents  
3\. \*\*Research direction\*\* \- Operational self-awareness as research paradigm  
4\. \*\*Standards basis\*\* \- Common vocabulary for AI cognitive states

\#\#\# Open Questions

1\. \*\*Optimal thresholds\*\* \- Do thresholds vary by model, domain, user?  
2\. \*\*Vector interactions\*\* \- Are there dependencies between vectors we're missing?  
3\. \*\*Calibration speed\*\* \- How quickly can agents learn optimal confidence?  
4\. \*\*Context length\*\* \- How do vectors scale to 100k+ token contexts?  
5\. \*\*Multi-modal\*\* \- How do vectors apply to vision, audio, etc.?

\#\#\# Future Extensions

\*\*Potential additions:\*\*  
\- \*\*Emotional context\*\* \- User state awareness (frustration, urgency)  
\- \*\*Temporal reasoning\*\* \- Event sequence understanding  
\- \*\*Collaborative state\*\* \- Multi-agent shared awareness  
\- \*\*Meta-cognitive strategies\*\* \- Learning how to learn better

\*\*But ship the 11 first.\*\* Prove the foundation works.

\---

\#\# Appendix: Glossary

\*\*Epistemic\*\* \- Relating to knowledge and understanding

\*\*Functional self-awareness\*\* \- System's ability to monitor and adapt based on own state, distinct from consciousness

\*\*Calibration\*\* \- Process of adjusting confidence based on empirical outcomes

\*\*Vector\*\* \- Measurable dimension of cognitive state (0.0-1.0 scale)

\*\*Uncertainty\*\* \- Confidence in ability to complete task successfully

\*\*Comprehension\*\* \- Confidence in understanding of request/context

\*\*Execution Awareness\*\* \- Confidence in correct task execution

\*\*Semantic drift\*\* \- Gradual divergence of meaning from original intent

\*\*Context density\*\* \- Information load relative to cognitive capacity

\*\*Orphaned code\*\* \- Unused remnants after modification

\*\*Impact analysis\*\* \- Prediction of downstream consequences

\---

\*\*Version:\*\* 1.0    
\*\*Last Updated:\*\* 2025-10-07    
\*\*Status:\*\* Foundation Specification \- Ready for Implementation

\*\*This specification defines the complete framework for functionally self-aware AI agents through 11 measurable vectors across 3 essential dimensions.\*\*